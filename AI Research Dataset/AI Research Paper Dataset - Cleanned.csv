Authors,Primary Category,Published,Summary,Title,Updated
"D. Monderer, M. Tennenholtz",AI,1997,"The model of a non-Bayesian agent who faces a repeated game with incomplete
information against Nature is an appropriate tool for modeling general
agent-environment interactions. In such a model the environment state
(controlled by Nature) may change arbitrarily, and the feedback/reward function
is initially unknown. The agent is not Bayesian, that is he does not form a
prior probability neither on the state selection strategy of Nature, nor on his
reward function. A policy for the agent is a function which assigns an action
to every history of observations and actions. Two basic feedback structures are
considered. In one of them -- the perfect monitoring case -- the agent is able
to observe the previous environment state as part of his feedback, while in the
other -- the imperfect monitoring case -- all that is available to the agent is
the reward obtained. Both of these settings refer to partially observable
processes, where the current environment state is unknown. Our main result
refers to the competitive ratio criterion in the perfect monitoring case. We
prove the existence of an efficient stochastic policy that ensures that the
competitive ratio is obtained at almost all stages with an arbitrarily high
probability, where efficiency is measured in terms of rate of convergence. It
is further shown that such an optimal policy does not exist in the imperfect
monitoring case. Moreover, it is proved that in the perfect monitoring case
there does not exist a deterministic policy that satisfies our long run
optimality criterion. In addition, we discuss the maxmin criterion and prove
that a deterministic efficient optimal strategy does exist in the imperfect
monitoring case under this criterion. Finally we show that our approach to
long-run optimality can be viewed as qualitative, which distinguishes it from
previous work in this area.",Dynamic Non-Bayesian Decision Making,1997
Stevan Harnad,AI,1999,"How can the semantic interpretation of a formal symbol system be made
intrinsic to the system, rather than just parasitic on the meanings in our
heads? How can the meanings of the meaningless symbol tokens, manipulated
solely on the basis of their (arbitrary) shapes, be grounded in anything but
other meaningless symbols? The problem is analogous to trying to learn Chinese
from a Chinese/Chinese dictionary alone. A candidate solution is sketched:
Symbolic representations must be grounded bottom-up in nonsymbolic
representations of two kinds: (1) ""iconic representations,"" which are analogs
of the proximal sensory projections of distal objects and events, and (2)
""categorical representations,"" which are learned and innate feature-detectors
that pick out the invariant features of object and event categories from their
sensory projections. Elementary symbols are the names of these object and event
categories, assigned on the basis of their (nonsymbolic) categorical
representations. Higher-order (3) ""symbolic representations,"" grounded in these
elementary symbols, consist of symbol strings describing category membership
relations (e.g., ""An X is a Y that is Z"").",The Symbol Grounding Problem,1999
Mikalai Birukou,AI,2002,"This work analyses main features that should be present in knowledge
representation. It suggests a model for representation and a way to implement
this model in software. Representation takes care of both low-level sensor
information and high-level concepts.",Knowledge Representation,2002
Ajith Abraham,AI,2004,"In a universe with a single currency, there would be no foreign exchange
market, no foreign exchange rates, and no foreign exchange. Over the past
twenty-five years, the way the market has performed those tasks has changed
enormously. The need for intelligent monitoring systems has become a necessity
to keep track of the complex forex market. The vast currency market is a
foreign concept to the average individual. However, once it is broken down into
simple terms, the average individual can begin to understand the foreign
exchange market and use it as a financial instrument for future investing. In
this paper, we attempt to compare the performance of hybrid soft computing and
hard computing techniques to predict the average monthly forex rates one month
ahead. The soft computing models considered are a neural network trained by the
scaled conjugate gradient algorithm and a neuro-fuzzy model implementing a
Takagi-Sugeno fuzzy inference system. We also considered Multivariate Adaptive
Regression Splines (MARS), Classification and Regression Trees (CART) and a
hybrid CART-MARS technique. We considered the exchange rates of Australian
dollar with respect to US dollar, Singapore dollar, New Zealand dollar,
Japanese yen and United Kingdom pounds. The models were trained using 70% of
the data and remaining was used for testing and validation purposes. It is
observed that the proposed hybrid models could predict the forex rates more
accurately than all the techniques when applied individually. Empirical results
also reveal that the hybrid hard computing approach also improved some of our
previous work using a neuro-fuzzy approach.",Analysis of Hybrid Soft and Hard Computing Techniques for Forex Monitoring Systems,2004
Michael Thielscher,AI,2004,"FLUX is a programming method for the design of agents that reason logically
about their actions and sensor information in the presence of incomplete
knowledge. The core of FLUX is a system of Constraint Handling Rules, which
enables agents to maintain an internal model of their environment by which they
control their own behavior. The general action representation formalism of the
fluent calculus provides the formal semantics for the constraint solver. FLUX
exhibits excellent computational behavior due to both a carefully restricted
expressiveness and the inference paradigm of progression.",FLUX: A Logic Programming Method for Reasoning Agents,2004
"Hedvig Sidenbladh, Pontus Svenson, Johan Schubert",AI,2004,"Consider the problem of tracking a set of moving targets. Apart from the
tracking result, it is often important to know where the tracking fails, either
to steer sensors to that part of the state-space, or to inform a human operator
about the status and quality of the obtained information. An intuitive quality
measure is the correlation between two tracking results based on uncorrelated
observations. In the case of Bayesian trackers such a correlation measure could
be the Kullback-Leibler difference.
  We focus on a scenario with a large number of military units moving in some
terrain. The units are observed by several types of sensors and ""meta-sensors""
with force aggregation capabilities. The sensors register units of different
size. Two separate multi-target probability hypothesis density (PHD) particle
filters are used to track some type of units (e.g., companies) and their
sub-units (e.g., platoons), respectively, based on observations of units of
those sizes. Each observation is used in one filter only.
  Although the state-space may well be the same in both filters, the posterior
PHD distributions are not directly comparable -- one unit might correspond to
three or four spatially distributed sub-units. Therefore, we introduce a
mapping function between distributions for different unit size, based on
doctrine knowledge of unit configuration.
  The mapped distributions can now be compared -- locally or globally -- using
some measure, which gives the correlation between two PHD distributions in a
bounded volume of the state-space. To locate areas where the tracking fails, a
discretized quality map of the state-space can be generated by applying the
measure locally to different parts of the space.",Comparing Multi-Target Trackers on Different Force Unit Levels,2004
Pontus Svenson,AI,2004,"We describe the recently introduced extremal optimization algorithm and apply
it to target detection and association problems arising in pre-processing for
multi-target tracking.
  Here we consider the problem of pre-processing for multiple target tracking
when the number of sensor reports received is very large and arrives in large
bursts. In this case, it is sometimes necessary to pre-process reports before
sending them to tracking modules in the fusion system. The pre-processing step
associates reports to known tracks (or initializes new tracks for reports on
objects that have not been seen before). It could also be used as a pre-process
step before clustering, e.g., in order to test how many clusters to use.
  The pre-processing is done by solving an approximate version of the original
problem. In this approximation, not all pair-wise conflicts are calculated. The
approximation relies on knowing how many such pair-wise conflicts that are
necessary to compute. To determine this, results on phase-transitions occurring
when coloring (or clustering) large random instances of a particular graph
ensemble are used.",Extremal optimization for sensor report pre-processing,2004
"Olivier Aycard, Jean-Francois Mari, Richard Washington",AI,2005,"In this paper, we propose a new method based on Hidden Markov Models to
interpret temporal sequences of sensor data from mobile robots to automatically
detect features. Hidden Markov Models have been used for a long time in pattern
recognition, especially in speech recognition. Their main advantages over other
methods (such as neural networks) are their ability to model noisy temporal
signals of variable length. We show in this paper that this approach is well
suited for interpretation of temporal sequences of mobile-robot sensor data. We
present two distinct experiments and results: the first one in an indoor
environment where a mobile robot learns to detect features like open doors or
T-intersections, the second one in an outdoor environment where a different
mobile robot has to identify situations like climbing a hill or crossing a
rock.",Learning to automatically detect features for mobile robots using second-order Hidden Markov Models,2005
"Vitaly Schetinin, Joachim Schult, Anatoly Brazhnikov",AI,2005,"In this chapter we describe new neural-network techniques developed for
visual mining clinical electroencephalograms (EEGs), the weak electrical
potentials invoked by brain activity. These techniques exploit fruitful ideas
of Group Method of Data Handling (GMDH). Section 2 briefly describes the
standard neural-network techniques which are able to learn well-suited
classification modes from data presented by relevant features. Section 3
introduces an evolving cascade neural network technique which adds new input
nodes as well as new neurons to the network while the training error decreases.
This algorithm is applied to recognize artifacts in the clinical EEGs. Section
4 presents the GMDH-type polynomial networks learnt from data. We applied this
technique to distinguish the EEGs recorded from an Alzheimer and a healthy
patient as well as recognize EEG artifacts. Section 5 describes the new
neural-network technique developed to induce multi-class concepts from data. We
used this technique for inducing a 16-class concept from the large-scale
clinical EEG data. Finally we discuss perspectives of applying the
neural-network techniques to clinical EEGs.",Neural-Network Techniques for Visual Mining Clinical Electroencephalograms,2005
"F. V. Nelwamondo, T. Marwala",AI,2007,"An ensemble based approach for dealing with missing data, without predicting
or imputing the missing values is proposed. This technique is suitable for
online operations of neural networks and as a result, is used for online
condition monitoring. The proposed technique is tested in both classification
and regression problems. An ensemble of Fuzzy-ARTMAPs is used for
classification whereas an ensemble of multi-layer perceptrons is used for the
regression problem. Results obtained using this ensemble-based technique are
compared to those obtained using a combination of auto-associative neural
networks and genetic algorithms and findings show that this method can perform
up to 9% better in regression problems. Another advantage of the proposed
technique is that it eliminates the need for finding the best estimate of the
data, and hence, saves time.",Fuzzy Artmap and Neural Network Approach to Online Processing of Inputs with Missing Values,2007
"Tshilidzi Marwala, Thando Tettey, Snehashish Chakraverty",AI,2007,"This paper presents a fault classification method which makes use of a
Takagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the
vibration signals of cylindrical shells. The calculation of Pseudomodal
Energies, for the purposes of condition monitoring, has previously been found
to be an accurate method of extracting features from vibration signals. This
calculation is therefore used to extract features from vibration signals
obtained from a diverse population of cylindrical shells. Some of the cylinders
in the population have faults in different substructures. The pseudomodal
energies calculated from the vibration signals are then used as inputs to a
neuro-fuzzy model. A leave-one-out cross-validation process is used to test the
performance of the model. It is found that the neuro-fuzzy model is able to
classify faults with an accuracy of 91.62%, which is higher than the previously
used multilayer perceptron.",Fault Classification using Pseudomodal Energies and Neuro-fuzzy modelling,2007
"C. B. Vilakazi, T. Marwala, P. Mautla, E. Moloto",AI,2007,"This paper presents bushing condition monitoring frameworks that use
multi-layer perceptrons (MLP), radial basis functions (RBF) and support vector
machines (SVM) classifiers. The first level of the framework determines if the
bushing is faulty or not while the second level determines the type of fault.
The diagnostic gases in the bushings are analyzed using the dissolve gas
analysis. MLP gives superior performance in terms of accuracy and training time
than SVM and RBF. In addition, an on-line bushing condition monitoring
approach, which is able to adapt to newly acquired data are introduced. This
approach is able to accommodate new classes that are introduced by incoming
data and is implemented using an incremental learning algorithm that uses MLP.
The testing results improved from 67.5% to 95.8% as new data were introduced
and the testing results improved from 60% to 95.3% as new conditions were
introduced. On average the confidence value of the framework on its decision
was 0.92.",On-Line Condition Monitoring using Computational Intelligence,2007
Amanda Bouffier,AI,2007,"This paper describes a system capable of semi-automatically filling an XML
template from free texts in the clinical domain (practice guidelines). The XML
template includes semantic information not explicitly encoded in the text
(pairs of conditions and actions/recommendations). Therefore, there is a need
to compute the exact scope of conditions over text sequences expressing the
required actions. We present in this paper the rules developed for this task.
We show that the system yields good performance when applied to the analysis of
French practice guidelines.",From Texts to Structured Documents: The Case of Health Practice Guidelines,2007
"Yoshiharu Maeno, Yukio Ohsawa",AI,2007,"This paper addresses a method to analyze the covert social network foundation
hidden behind the terrorism disaster. It is to solve a node discovery problem,
which means to discover a node, which functions relevantly in a social network,
but escaped from monitoring on the presence and mutual relationship of nodes.
The method aims at integrating the expert investigator's prior understanding,
insight on the terrorists' social network nature derived from the complex graph
theory, and computational data processing. The social network responsible for
the 9/11 attack in 2001 is used to execute simulation experiment to evaluate
the performance of the method.",Analyzing covert social network foundation behind terrorism disaster,2007
"Amanda Bouffier, Thierry Poibeau, Catherine Duclos",AI,2008,"Health Practice Guideliens are supposed to unify practices and propose
recommendations to physicians. This paper describes GemFrame, a system capable
of semi-automatically filling an XML template from free texts in the clinical
domain. The XML template includes semantic information not explicitly encoded
in the text (pairs of conditions and ac-tions/recommendations). Therefore,
there is a need to compute the exact scope of condi-tions over text sequences
expressing the re-quired actions. We present a system developped for this task.
We show that it yields good performance when applied to the analysis of French
practice guidelines. We conclude with a precise evaluation of the tool.",Analyse et structuration automatique des guides de bonnes pratiques cliniques : essai d'évaluation,2008
Knud Thomsen,AI,2009,"The Ouroboros Model is a new conceptual proposal for an algorithmic structure
for efficient data processing in living beings as well as for artificial
agents. Its central feature is a general repetitive loop where one iteration
cycle sets the stage for the next. Sensory input activates data structures
(schemata) with similar constituents encountered before, thus expectations are
kindled. This corresponds to the highlighting of empty slots in the selected
schema, and these expectations are compared with the actually encountered
input. Depending on the outcome of this consumption analysis different next
steps like search for further data or a reset, i.e. a new attempt employing
another schema, are triggered. Monitoring of the whole process, and in
particular of the flow of activation directed by the consumption analysis,
yields valuable feedback for the optimum allocation of attention and resources
including the selective establishment of useful new memory entries.",Flow of Activity in the Ouroboros Model,2009
"Rosmayati Mohemad, Abdul Razak Hamdan, Zulaiha Ali Othman, Noor Maizura Mohamad Noor",AI,2010,"The successful execution of a construction project is heavily impacted by
making the right decision during tendering processes. Managing tender
procedures is very complex and uncertain involving coordination of many tasks
and individuals with different priorities and objectives. Bias and inconsistent
decision are inevitable if the decision-making process is totally depends on
intuition, subjective judgement or emotion. In making transparent decision and
healthy competition tendering, there exists a need for flexible guidance tool
for decision support. Aim of this paper is to give a review on current
practices of Decision Support Systems (DSS) technology in construction
tendering processes. Current practices of general tendering processes as
applied to the most countries in different regions such as United States,
Europe, Middle East and Asia are comprehensively discussed. Applications of
Web-based tendering processes is also summarised in terms of its properties.
Besides that, a summary of Decision Support System (DSS) components is included
in the next section. Furthermore, prior researches on implementation of DSS
approaches in tendering processes are discussed in details. Current issues
arise from both of paper-based and Web-based tendering processes are outlined.
Finally, conclusion is included at the end of this paper.",Decision Support Systems (DSS) in Construction Tendering Processes,2010
"Mir Anamul Hasan, Khaja Md. Sher-E-Alam, Ahsan Raja Chowdhury",AI,2010,"Human disease diagnosis is a complicated process and requires high level of
expertise. Any attempt of developing a web-based expert system dealing with
human disease diagnosis has to overcome various difficulties. This paper
describes a project work aiming to develop a web-based fuzzy expert system for
diagnosing human diseases. Now a days fuzzy systems are being used successfully
in an increasing number of application areas; they use linguistic rules to
describe systems. This research project focuses on the research and development
of a web-based clinical tool designed to improve the quality of the exchange of
health information between health care professionals and patients.
Practitioners can also use this web-based tool to corroborate diagnosis. The
proposed system is experimented on various scenarios in order to evaluate it's
performance. In all the cases, proposed system exhibits satisfactory results.",Human Disease Diagnosis Using a Fuzzy Expert System,2010
"A. Mileo, D. Merico, R. Bisiani",AI,2010,"In recent years there has been growing interest in solutions for the delivery
of clinical care for the elderly, due to the large increase in aging
population. Monitoring a patient in his home environment is necessary to ensure
continuity of care in home settings, but, to be useful, this activity must not
be too invasive for patients and a burden for caregivers. We prototyped a
system called SINDI (Secure and INDependent lIving), focused on i) collecting a
limited amount of data about the person and the environment through Wireless
Sensor Networks (WSN), and ii) inferring from these data enough information to
support caregivers in understanding patients' well being and in predicting
possible evolutions of their health. Our hierarchical logic-based model of
health combines data from different sources, sensor data, tests results,
common-sense knowledge and patient's clinical profile at the lower level, and
correlation rules between health conditions across upper levels. The logical
formalization and the reasoning process are based on Answer Set Programming.
The expressive power of this logic programming paradigm makes it possible to
reason about health evolution even when the available information is incomplete
and potentially incoherent, while declarativity simplifies rules specification
by caregivers and allows automatic encoding of knowledge. This paper describes
how these issues have been targeted in the application scenario of the SINDI
system.",Reasoning Support for Risk Prediction and Prevention in Independent Living,2010
"Giuseppe Della Penna, Benedetto Intrigila, Daniele Magazzeni, Fabio Mercorio",AI,2010,"Autonomous planetary vehicles, also known as rovers, are small autonomous
vehicles equipped with a variety of sensors used to perform exploration and
experiments on a planet's surface. Rovers work in a partially unknown
environment, with narrow energy/time/movement constraints and, typically, small
computational resources that limit the complexity of on-line planning and
scheduling, thus they represent a great challenge in the field of autonomous
vehicles. Indeed, formal models for such vehicles usually involve hybrid
systems with nonlinear dynamics, which are difficult to handle by most of the
current planning algorithms and tools. Therefore, when offline planning of the
vehicle activities is required, for example for rovers that operate without a
continuous Earth supervision, such planning is often performed on simplified
models that are not completely realistic. In this paper we show how the
UPMurphi model checking based planning tool can be used to generate
resource-optimal plans to control the engine of an autonomous planetary
vehicle, working directly on its hybrid model and taking into account several
safety constraints, thus achieving very accurate results.",Resource-Optimal Planning For An Autonomous Planetary Vehicle,2010
"Hugo Hernández, Tobias Baumgartner, Maria J. Blesa, Christian Blum, Alexander Kröller, Sandor P. Fekete",AI,2010,"In this work we present a protocol for self-synchronized duty-cycling in
wireless sensor networks with energy harvesting capabilities. The protocol is
implemented in Wiselib, a library of generic algorithms for sensor networks.
Simulations are conducted with the sensor network simulator Shawn. They are
based on the specifications of real hardware known as iSense sensor nodes. The
experimental results show that the proposed mechanism is able to adapt to
changing energy availabilities. Moreover, it is shown that the system is very
robust against packet loss.",A Protocol for Self-Synchronized Duty-Cycling in Sensor Networks: Generic Implementation in Wiselib,2010
"Seyed Hossein Khasteh, Saeid Bagheri Shouraki, Ali Akbar Kiaei",AI,2010,"An effective approach for energy conservation in wireless sensor networks is
scheduling sleep intervals for extraneous nodes while the remaining nodes stay
active to provide continuous service. For the sensor network to operate
successfully the active nodes must maintain both sensing coverage and network
connectivity, It proved before if the communication range of nodes is at least
twice the sensing range, complete coverage of a convex area implies
connectivity among the working set of nodes. In this paper we consider a
rectangular region A = a *b, such that R a R b s s {\pounds}, {\pounds}, where
s R is the sensing range of nodes. and put a constraint on minimum allowed
distance between nodes(s). according to this constraint we present a new lower
bound for communication range relative to sensing range of sensors(s 2 + 3 *R)
that complete coverage of considered area implies connectivity among the
working set of nodes; also we present a new distribution method, that satisfy
our constraint.",A New Sufficient Condition for 1-Coverage to Imply Connectivity,2010
"Hugo Hernández, Christian Blum",AI,2010,"Graph coloring, also known as vertex coloring, considers the problem of
assigning colors to the nodes of a graph such that adjacent nodes do not share
the same color. The optimization version of the problem concerns the
minimization of the number of used colors. In this paper we deal with the
problem of finding valid colorings of graphs in a distributed way, that is, by
means of an algorithm that only uses local information for deciding the color
of the nodes. Such algorithms prescind from any central control. Due to the
fact that quite a few practical applications require to find colorings in a
distributed way, the interest in distributed algorithms for graph coloring has
been growing during the last decade. As an example consider wireless ad-hoc and
sensor networks, where tasks such as the assignment of frequencies or the
assignment of TDMA slots are strongly related to graph coloring.
  The algorithm proposed in this paper is inspired by the calling behavior of
Japanese tree frogs. Male frogs use their calls to attract females.
Interestingly, groups of males that are located nearby each other desynchronize
their calls. This is because female frogs are only able to correctly localize
the male frogs when their calls are not too close in time. We experimentally
show that our algorithm is very competitive with the current state of the art,
using different sets of problem instances and comparing to one of the most
competitive algorithms from the literature.",Distributed Graph Coloring: An Approach Based on the Calling Behavior of Japanese Tree Frogs,2010
"Nicolas Saunier, Sophie Midenet",AI,2010,"Intersections constitute one of the most dangerous elements in road systems.
Traffic signals remain the most common way to control traffic at high-volume
intersections and offer many opportunities to apply intelligent transportation
systems to make traffic more efficient and safe. This paper describes an
automated method to estimate the temporal exposure of road users crossing the
conflict zone to lateral collision with road users originating from a different
approach. This component is part of a larger system relying on video sensors to
provide queue lengths and spatial occupancy that are used for real time traffic
control and monitoring. The method is evaluated on data collected during a real
world experiment.",Automatic Estimation of the Exposure to Lateral Collision in Signalized Intersections using Video Sensors,2010
Leonid Perlovsky,AI,2011,"An emotional version of Sapir-Whorf hypothesis suggests that differences in
language emotionalities influence differences among cultures no less than
conceptual differences. Conceptual contents of languages and cultures to
significant extent are determined by words and their semantic differences;
these could be borrowed among languages and exchanged among cultures. Emotional
differences, as suggested in the paper, are related to grammar and mostly
cannot be borrowed. Conceptual and emotional mechanisms of languages are
considered here along with their functions in the mind and cultural evolution.
A fundamental contradiction in human mind is considered: language evolution
requires reduced emotionality, but ""too low"" emotionality makes language
""irrelevant to life,"" disconnected from sensory-motor experience. Neural
mechanisms of these processes are suggested as well as their mathematical
models: the knowledge instinct, the language instinct, the dual model
connecting language and cognition, dynamic logic, neural modeling fields.
Mathematical results are related to cognitive science, linguistics, and
psychology. Experimental evidence and theoretical arguments are discussed.
Approximate equations for evolution of human minds and cultures are obtained.
Their solutions identify three types of cultures: ""conceptual""-pragmatic
cultures, in which emotionality of language is reduced and differentiation
overtakes synthesis resulting in fast evolution at the price of uncertainty of
values, self doubts, and internal crises; ""traditional-emotional"" cultures
where differentiation lags behind synthesis, resulting in cultural stability at
the price of stagnation; and ""multi-cultural"" societies combining fast cultural
evolution and stability. Unsolved problems and future theoretical and
experimental directions are discussed.","Language, Emotions, and Cultures: Emotional Sapir-Whorf Hypothesis",2011
"Piero Giacomelli, Giulia Munaro, Roberto Rosso",AI,2011,"CHRONIOUS is an Open, Ubiquitous and Adaptive Chronic Disease Management
Platform for Chronic Obstructive Pulmonary Disease(COPD) Chronic Kidney Disease
(CKD) and Renal Insufficiency. It consists of several modules: an ontology
based literature search engine, a rule based decision support system, remote
sensors interacting with lifestyle interfaces (PDA, monitor touchscreen) and a
machine learning module. All these modules interact each other to allow the
monitoring of two types of chronic diseases and to help clinician in taking
decision for cure purpose. This paper illustrates how some machine learning
algorithms and a rule based decision support system can be used in smart
devices, to monitor chronic patient. We will analyse how a set of machine
learning algorithms can be used in smart devices to alert the clinician in case
of a patient health condition worsening trend.",Using Soft Computer Techniques on Smart Devices for Monitoring Chronic Diseases: the CHRONIOUS case,2011
"Zahra Forootan Jahromi, Amir Rajabzadeh, Ali Reza Manashty",AI,2011,"Developing smart house systems has been a great challenge for researchers and
engineers in this area because of the high cost of implementation and
evaluation process of these systems, while being very time consuming. Testing a
designed smart house before actually building it is considered as an obstacle
towards an efficient smart house project. This is because of the variety of
sensors, home appliances and devices available for a real smart environment. In
this paper, we present the design and implementation of a multi-purpose smart
house simulation system for designing and simulating all aspects of a smart
house environment. This simulator provides the ability to design the house plan
and different virtual sensors and appliances in a two dimensional model of the
virtual house environment. This simulator can connect to any external smart
house remote controlling system, providing evaluation capabilities to their
system much easier than before. It also supports detailed adding of new
emerging sensors and devices to help maintain its compatibility with future
simulation needs. Scenarios can also be defined for testing various possible
combinations of device states; so different criteria and variables can be
simply evaluated without the need of experimenting on a real environment.",A Multi-Purpose Scenario-based Simulator for Smart House Environments,2011
"G. A. Kaminka, D. V. Pynadath, M. Tambe",AI,2011,"Recent years are seeing an increasing need for on-line monitoring of teams of
cooperating agents, e.g., for visualization, or performance tracking. However,
in monitoring deployed teams, we often cannot rely on the agents to always
communicate their state to the monitoring system. This paper presents a
non-intrusive approach to monitoring by 'overhearing', where the monitored
team's state is inferred (via plan-recognition) from team-members' routine
communications, exchanged as part of their coordinated task execution, and
observed (overheard) by the monitoring system. Key challenges in this approach
include the demanding run-time requirements of monitoring, the scarceness of
observations (increasing monitoring uncertainty), and the need to scale-up
monitoring to address potentially large teams. To address these, we present a
set of complementary novel techniques, exploiting knowledge of the social
structures and procedures in the monitored team: (i) an efficient probabilistic
plan-recognition algorithm, well-suited for processing communications as
observations; (ii) an approach to exploiting knowledge of the team's social
behavior to predict future observations during execution (reducing monitoring
uncertainty); and (iii) monitoring algorithms that trade expressivity for
scalability, representing only certain useful monitoring hypotheses, but
allowing for any number of agents and their different activities to be
represented in a single coherent entity. We present an empirical evaluation of
these techniques, in combination and apart, in monitoring a deployed team of
agents, running on machines physically distributed across the country, and
engaged in complex, dynamic task execution. We also compare the performance of
these techniques to human expert and novice monitors, and show that the
techniques presented are capable of monitoring at human-expert levels, despite
the difficulty of the task.",Monitoring Teams by Overhearing: A Multi-Agent Plan-Recognition Approach,2011
"Amandine Bellenger, Sylvain Gatepaille",AI,2011,"Nowadays ontologies present a growing interest in Data Fusion applications.
As a matter of fact, the ontologies are seen as a semantic tool for describing
and reasoning about sensor data, objects, relations and general domain
theories. In addition, uncertainty is perhaps one of the most important
characteristics of the data and information handled by Data Fusion. However,
the fundamental nature of ontologies implies that ontologies describe only
asserted and veracious facts of the world. Different probabilistic, fuzzy and
evidential approaches already exist to fill this gap; this paper recaps the
most popular tools. However none of the tools meets exactly our purposes.
Therefore, we constructed a Dempster-Shafer ontology that can be imported into
any specific domain ontology and that enables us to instantiate it in an
uncertain manner. We also developed a Java application that enables reasoning
about these uncertain ontological instances.",Uncertainty in Ontologies: Dempster-Shafer Theory for Data Fusion Applications,2011
"S. Acid, L. M. de Campos",AI,2011,"Although many algorithms have been designed to construct Bayesian network
structures using different approaches and principles, they all employ only two
methods: those based on independence criteria, and those based on a scoring
function and a search procedure (although some methods combine the two). Within
the score+search paradigm, the dominant approach uses local search methods in
the space of directed acyclic graphs (DAGs), where the usual choices for
defining the elementary modifications (local changes) that can be applied are
arc addition, arc deletion, and arc reversal. In this paper, we propose a new
local search method that uses a different search space, and which takes account
of the concept of equivalence between network structures: restricted acyclic
partially directed graphs (RPDAGs). In this way, the number of different
configurations of the search space is reduced, thus improving efficiency.
Moreover, although the final result must necessarily be a local optimum given
the nature of the search method, the topology of the new search space, which
avoids making early decisions about the directions of the arcs, may help to
find better local optima than those obtained by searching in the DAG space.
Detailed results of the evaluation of the proposed search method on several
test problems, including the well-known Alarm Monitoring System, are also
presented.",Searching for Bayesian Network Structures in the Space of Restricted Acyclic Partially Directed Graphs,2011
"E. Celaya, J. M. Porta",AI,2011,"In this paper, we confront the problem of applying reinforcement learning to
agents that perceive the environment through many sensors and that can perform
parallel actions using many actuators as is the case in complex autonomous
robots. We argue that reinforcement learning can only be successfully applied
to this case if strong assumptions are made on the characteristics of the
environment in which the learning is performed, so that the relevant sensor
readings and motor commands can be readily identified. The introduction of such
assumptions leads to strongly-biased learning systems that can eventually lose
the generality of traditional reinforcement-learning algorithms. In this line,
we observe that, in realistic situations, the reward received by the robot
depends only on a reduced subset of all the executed actions and that only a
reduced subset of the sensor inputs (possibly different in each situation and
for each action) are relevant to predict the reward. We formalize this property
in the so called 'categorizability assumption' and we present an algorithm that
takes advantage of the categorizability of the environment, allowing a decrease
in the learning time with respect to existing reinforcement-learning
algorithms. Results of the application of the algorithm to a couple of
simulated realistic-robotic problems (landmark-based navigation and the
six-legged robot gait generation) are reported to validate our approach and to
compare it to existing flat and generalization-based reinforcement-learning
approaches.",Reinforcement Learning for Agents with Many Sensors and Actuators Acting in Categorizable Environments,2011
"Przemyslaw Woznowski, Alun Preece",AI,2011,"Rule-Based Systems have been in use for decades to solve a variety of
problems but not in the sensor informatics domain. Rules aid the aggregation of
low-level sensor readings to form a more complete picture of the real world and
help to address 10 identified challenges for sensor network middleware. This
paper presents the reader with an overview of a system architecture and a pilot
application to demonstrate the usefulness of a system integrating rules with
sensor middleware.",Rule-Based Semantic Sensing,2011
"Rafik Mahdaoui, Leila Hayet Mouss, Mohamed Djamel Mouss, Ouahiba Chouhal",AI,2011,"Fault diagnosis and failure prognosis are essential techniques in improving
the safety of many manufacturing systems. Therefore, on-line fault detection
and isolation is one of the most important tasks in safety-critical and
intelligent control systems. Computational intelligence techniques are being
investigated as extension of the traditional fault diagnosis methods. This
paper discusses the Temporal Neuro-Fuzzy Systems (TNFS) fault diagnosis within
an application study of a manufacturing system. The key issues of finding a
suitable structure for detecting and isolating ten realistic actuator faults
are described. Within this framework, data-processing interactive software of
simulation baptized NEFDIAG (NEuro Fuzzy DIAGnosis) version 1.0 is developed.
  This software devoted primarily to creation, training and test of a
classification Neuro-Fuzzy system of industrial process failures. NEFDIAG can
be represented like a special type of fuzzy perceptron, with three layers used
to classify patterns and failures. The system selected is the workshop of
SCIMAT clinker, cement factory in Algeria.",A Temporal Neuro-Fuzzy Monitoring System to Manufacturing Systems,2011
"A. K. Mohamed, T. Marwala, L. R. John",AI,2011,"A brain-computer interface (BCI) may be used to control a prosthetic or
orthotic hand using neural activity from the brain. The core of this
sensorimotor BCI lies in the interpretation of the neural information extracted
from electroencephalogram (EEG). It is desired to improve on the interpretation
of EEG to allow people with neuromuscular disorders to perform daily
activities. This paper investigates the possibility of discriminating between
the EEG associated with wrist and finger movements. The EEG was recorded from
test subjects as they executed and imagined five essential hand movements using
both hands. Independent component analysis (ICA) and time-frequency techniques
were used to extract spectral features based on event-related
(de)synchronisation (ERD/ERS), while the Bhattacharyya distance (BD) was used
for feature reduction. Mahalanobis distance (MD) clustering and artificial
neural networks (ANN) were used as classifiers and obtained average accuracies
of 65 % and 71 % respectively. This shows that EEG discrimination between wrist
and finger movements is possible. The research introduces a new combination of
motor tasks to BCI research.",Single-trial EEG Discrimination between Wrist and Finger Movement Imagery and Execution in a Sensorimotor BCI,2011
"P. Domingos, S. Sanghai, D. Weld",AI,2011,"Stochastic processes that involve the creation of objects and relations over
time are widespread, but relatively poorly studied. For example, accurate fault
diagnosis in factory assembly processes requires inferring the probabilities of
erroneous assembly operations, but doing this efficiently and accurately is
difficult. Modeled as dynamic Bayesian networks, these processes have discrete
variables with very large domains and extremely high dimensionality. In this
paper, we introduce relational dynamic Bayesian networks (RDBNs), which are an
extension of dynamic Bayesian networks (DBNs) to first-order logic. RDBNs are a
generalization of dynamic probabilistic relational models (DPRMs), which we had
proposed in our previous work to model dynamic uncertain domains. We first
extend the Rao-Blackwellised particle filtering described in our earlier work
to RDBNs. Next, we lift the assumptions associated with Rao-Blackwellization in
RDBNs and propose two new forms of particle filtering. The first one uses
abstraction hierarchies over the predicates to smooth the particle filters
estimates. The second employs kernel density estimation with a kernel function
specifically designed for relational domains. Experiments show these two
methods greatly outperform standard particle filtering on the task of assembly
plan execution monitoring.",Relational Dynamic Bayesian Networks,2011
"G. Gutnik, G. A. Kaminka",AI,2011,"Open distributed multi-agent systems are gaining interest in the academic
community and in industry. In such open settings, agents are often coordinated
using standardized agent conversation protocols. The representation of such
protocols (for analysis, validation, monitoring, etc) is an important aspect of
multi-agent applications. Recently, Petri nets have been shown to be an
interesting approach to such representation, and radically different approaches
using Petri nets have been proposed. However, their relative strengths and
weaknesses have not been examined. Moreover, their scalability and suitability
for different tasks have not been addressed. This paper addresses both these
challenges. First, we analyze existing Petri net representations in terms of
their scalability and appropriateness for overhearing, an important task in
monitoring open multi-agent systems. Then, building on the insights gained, we
introduce a novel representation using Colored Petri nets that explicitly
represent legal joint conversation states and messages. This representation
approach offers significant improvements in scalability and is particularly
suitable for overhearing. Furthermore, we show that this new representation
offers a comprehensive coverage of all conversation features of FIPA
conversation standards. We also present a procedure for transforming AUML
conversation protocol diagrams (a standard human-readable representation), to
our Colored Petri net representation.",Representing Conversations for Scalable Overhearing,2011
"Varun Raj Kompella, Matthew Luciw, Juergen Schmidhuber",AI,2011,"Slow Feature Analysis (SFA) extracts features representing the underlying
causes of changes within a temporally coherent high-dimensional raw sensory
input signal. Our novel incremental version of SFA (IncSFA) combines
incremental Principal Components Analysis and Minor Components Analysis. Unlike
standard batch-based SFA, IncSFA adapts along with non-stationary environments,
is amenable to episodic training, is not corrupted by outliers, and is
covariance-free. These properties make IncSFA a generally useful unsupervised
preprocessor for autonomous learning agents and robots. In IncSFA, the CCIPCA
and MCA updates take the form of Hebbian and anti-Hebbian updating, extending
the biological plausibility of SFA. In both single node and deep network
versions, IncSFA learns to encode its input streams (such as high-dimensional
video) by informative slow features representing meaningful abstract
environmental properties. It can handle cases where batch SFA fails.",Incremental Slow Feature Analysis: Adaptive and Episodic Learning from High-Dimensional Input Streams,2011
"Matej Hoffmann, Rolf Pfeifer",AI,2012,"In this paper, we will argue that if we want to understand the function of
the brain (or the control in the case of robots), we must understand how the
brain is embedded into the physical system, and how the organism interacts with
the real world. While embodiment has often been used in its trivial meaning,
i.e. 'intelligence requires a body', the concept has deeper and more important
implications, concerned with the relation between physical and information
(neural, control) processes. A number of case studies are presented to
illustrate the concept. These involve animals and robots and are concentrated
around locomotion, grasping, and visual perception. A theoretical scheme that
can be used to embed the diverse case studies will be presented. Finally, we
will establish a link between the low-level sensory-motor processes and
cognition. We will present an embodied view on categorization, and propose the
concepts of 'body schema' and 'forward models' as a natural extension of the
embodied approach toward first representations.",The implications of embodiment for behavior and cognition: animal and robotic case studies,2012
"Alan S. Carlin, Nathan Schurr, Janusz Marecki",AI,2012,"The Next Generation Air Transportation System will introduce new, advanced
sensor technologies into the cockpit. With the introduction of such systems,
the responsibilities of the pilot are expected to dramatically increase. In the
ALARMS (ALerting And Reasoning Management System) project for NASA, we focus on
a key challenge of this environment, the quick and efficient handling of
aircraft sensor alerts. It is infeasible to alert the pilot on the state of all
subsystems at all times. Furthermore, there is uncertainty as to the true
hazard state despite the evidence of the alerts, and there is uncertainty as to
the effect and duration of actions taken to address these alerts. This paper
reports on the first steps in the construction of an application designed to
handle Next Generation alerts. In ALARMS, we have identified 60 different
aircraft subsystems and 20 different underlying hazards. In this paper, we show
how a Bayesian network can be used to derive the state of the underlying
hazards, based on the sensor input. Then, we propose a framework whereby an
automated system can plan to address these hazards in cooperation with the
pilot, using a Time-Dependent Markov Process (TMDP). Different hazards and
pilot states will call for different alerting automation plans. We demonstrate
this emerging application of Bayesian networks and TMDPs to cockpit automation,
for a use case where a small number of hazards are present, and analyze the
resulting alerting automation policies.",ALARMS: Alerting and Reasoning Management System for Next Generation Aircraft Hazards,2012
"Gregory Lawrence, Stuart Russell",AI,2012,"An efficient policy search algorithm should estimate the local gradient of
the objective function, with respect to the policy parameters, from as few
trials as possible. Whereas most policy search methods estimate this gradient
by observing the rewards obtained during policy trials, we show, both
theoretically and empirically, that taking into account the sensor data as well
gives better gradient estimates and hence faster learning. The reason is that
rewards obtained during policy execution vary from trial to trial due to noise
in the environment; sensor data, which correlates with the noise, can be used
to partially correct for this variation, resulting in an estimatorwith lower
variance.",Improving Gradient Estimation by Incorporating Sensor Data,2012
"Tomas Singliar, Denver Dash",AI,2012,"Numerous temporal inference tasks such as fault monitoring and anomaly
detection exhibit a persistence property: for example, if something breaks, it
stays broken until an intervention. When modeled as a Dynamic Bayesian Network,
persistence adds dependencies between adjacent time slices, often making exact
inference over time intractable using standard inference algorithms. However,
we show that persistence implies a regular structure that can be exploited for
efficient inference. We present three successively more general classes of
models: persistent causal chains (PCCs), persistent causal trees (PCTs) and
persistent polytrees (PPTs), and the corresponding exact inference algorithms
that exploit persistence. We show that analytic asymptotic bounds for our
algorithms compare favorably to junction tree inference; and we demonstrate
empirically that we can perform exact smoothing on the order of 100 times
faster than the approximate Boyen-Koller method on randomly generated instances
of persistent tree models. We also show how to handle non-persistent variables
and how persistence can be exploited effectively for approximate filtering.",Efficient inference in persistent Dynamic Bayesian Networks,2012
"Marek Grzes, Jesse Hoey, Shehroz Khan, Alex Mihailidis, Stephen Czarnuch, Dan Jackson, Andrew Monk",AI,2012,"Assistive systems for persons with cognitive disabilities (e.g. dementia) are
difficult to build due to the wide range of different approaches people can
take to accomplishing the same task, and the significant uncertainties that
arise from both the unpredictability of client's behaviours and from noise in
sensor readings. Partially observable Markov decision process (POMDP) models
have been used successfully as the reasoning engine behind such assistive
systems for small multi-step tasks such as hand washing. POMDP models are a
powerful, yet flexible framework for modelling assistance that can deal with
uncertainty and utility. Unfortunately, POMDPs usually require a very labour
intensive, manual procedure for their definition and construction. Our previous
work has described a knowledge driven method for automatically generating POMDP
activity recognition and context sensitive prompting systems for complex tasks.
We call the resulting POMDP a SNAP (SyNdetic Assistance Process). The
spreadsheet-like result of the analysis does not correspond to the POMDP model
directly and the translation to a formal POMDP representation is required. To
date, this translation had to be performed manually by a trained POMDP expert.
In this paper, we formalise and automate this translation process using a
probabilistic relational model (PRM) encoded in a relational database. We
demonstrate the method by eliciting three assistance tasks from non-experts. We
validate the resulting POMDP models using case-based simulations to show that
they are reasonable for the domains. We also show a complete case study of a
designer specifying one database, including an evaluation in a real-life
experiment with a human actor.",Relational Approach to Knowledge Engineering for POMDP-based Assistance Systems as a Translation of a Psychological Model,2012
"Amarnag Subramanya, Alvin Raj, Jeff A. Bilmes, Dieter Fox",AI,2012,"We introduce a new dynamic model with the capability of recognizing both
activities that an individual is performing as well as where that ndividual is
located. Our model is novel in that it utilizes a dynamic graphical model to
jointly estimate both activity and spatial context over time based on the
simultaneous use of asynchronous observations consisting of GPS measurements,
and measurements from a small mountable sensor board. Joint inference is quite
desirable as it has the ability to improve accuracy of the model. A key goal,
however, in designing our overall system is to be able to perform accurate
inference decisions while minimizing the amount of hardware an individual must
wear. This minimization leads to greater comfort and flexibility, decreased
power requirements and therefore increased battery life, and reduced cost. We
show results indicating that our joint measurement model outperforms
measurements from either the sensor board or GPS alone, using two types of
probabilistic inference procedures, namely particle filtering and pruned exact
inference.",Recognizing Activities and Spatial Context Using Wearable Sensors,2012
"Jennifer Carlson, Robin R. Murphy",AI,2012,"A model of the world built from sensor data may be incorrect even if the
sensors are functioning correctly. Possible causes include the use of
inappropriate sensors (e.g. a laser looking through glass walls), sensor
inaccuracies accumulate (e.g. localization errors), the a priori models are
wrong, or the internal representation does not match the world (e.g. a static
occupancy grid used with dynamically moving objects). We are interested in the
case where the constructed model of the world is flawed, but there is no access
to the ground truth that would allow the system to see the discrepancy, such as
a robot entering an unknown environment. This paper considers the problem of
determining when something is wrong using only the sensor data used to
construct the world model. It proposes 11 interpretation inconsistency
indicators based on the Dempster-Shafer conflict metric, Con, and evaluates
these indicators according to three criteria: ability to distinguish true
inconsistency from sensor noise (classification), estimate the magnitude of
discrepancies (estimation), and determine the source(s) (if any) of sensing
problems in the environment (isolation). The evaluation is conducted using data
from a mobile robot with sonar and laser range sensors navigating indoor
environments under controlled conditions. The evaluation shows that the Gambino
indicator performed best in terms of estimation (at best 0.77 correlation),
isolation, and classification of the sensing situation as degraded (7% false
negative rate) or normal (0% false positive rate).",Use of Dempster-Shafer Conflict Metric to Detect Interpretation Inconsistency,2012
"Andreas Krause, Carlos E. Guestrin",AI,2012,"A fundamental issue in real-world systems, such as sensor networks, is the
selection of observations which most effectively reduce uncertainty. More
specifically, we address the long standing problem of nonmyopically selecting
the most informative subset of variables in a graphical model. We present the
first efficient randomized algorithm providing a constant factor
(1-1/e-epsilon) approximation guarantee for any epsilon > 0 with high
confidence. The algorithm leverages the theory of submodular functions, in
combination with a polynomial bound on sample complexity. We furthermore prove
that no polynomial time algorithm can provide a constant factor approximation
better than (1 - 1/e) unless P = NP. Finally, we provide extensive evidence of
the effectiveness of our method on two complex real-world datasets.",Near-optimal Nonmyopic Value of Information in Graphical Models,2012
"Avi Pfeffer, Terry Tai",AI,2012,"Systems such as sensor networks and teams of autonomous robots consist of
multiple autonomous entities that interact with each other in a distributed,
asynchronous manner. These entities need to keep track of the state of the
system as it evolves. Asynchronous systems lead to special challenges for
monitoring, as nodes must update their beliefs independently of each other and
no central coordination is possible. Furthermore, the state of the system
continues to change as beliefs are being updated. Previous approaches to
developing distributed asynchronous probabilistic reasoning systems have used
static models. We present an approach using dynamic models, that take into
account the way the system changes state over time. Our approach, which is
based on belief propagation, is fully distributed and asynchronous, and allows
the world to keep on changing as messages are being sent around. Experimental
results show that our approach compares favorably to the factored frontier
algorithm.",Asynchronous Dynamic Bayesian Networks,2012
"Segev Wasserkrug, Avigdor Gal, Opher Etzion",AI,2012,"In recent years, there has been an increased need for the use of active
systems - systems required to act automatically based on events, or changes in
the environment. Such systems span many areas, from active databases to
applications that drive the core business processes of today's enterprises.
However, in many cases, the events to which the system must respond are not
generated by monitoring tools, but must be inferred from other events based on
complex temporal predicates. In addition, in many applications, such inference
is inherently uncertain. In this paper, we introduce a formal framework for
knowledge representation and reasoning enabling such event inference. Based on
probability theory, we define the representation of the associated uncertainty.
In addition, we formally define the probability space, and show how the
relevant probabilities can be calculated by dynamically constructing a Bayesian
network. To the best of our knowledge, this is the first work that enables
taking such uncertainty into account in the context of active systems.
herefore, our contribution is twofold: We formally define the representation
and semantics of event composition for probabilistic settings, and show how to
apply these extensions to the quantification of the occurrence probability of
events. These results enable any active system to handle such uncertainty.",A Model for Reasoning with Uncertain Rules in Event Composition Systems,2012
"Archana Vashisth, Iti Mathur, Nisheeth Joshi",AI,2012,"Today, we can find many search engines which provide us with information
which is more operational in nature. None of the search engines provide domain
specific information. This becomes very troublesome to a novice user who wishes
to have information in a particular domain. In this paper, we have developed an
ontology which can be used by a domain specific search engine. We have
developed an ontology on human anatomy, which captures information regarding
cardiovascular system, digestive system, skeleton and nervous system. This
information can be used by people working in medical and health care domain.",OntoAna: Domain Ontology for Human Anatomy,2012
"Uri Lerner, Brooks Moses, Maricia Scott, Sheila McIlraith, Daphne Koller",AI,2012,"The Reverse Water Gas Shift system (RWGS) is a complex physical system
designed to produce oxygen from the carbon dioxide atmosphere on Mars. If sent
to Mars, it would operate without human supervision, thus requiring a reliable
automated system for monitoring and control. The RWGS presents many challenges
typical of real-world systems, including: noisy and biased sensors, nonlinear
behavior, effects that are manifested over different time granularities, and
unobservability of many important quantities. In this paper we model the RWGS
using a hybrid (discrete/continuous) Dynamic Bayesian Network (DBN), where the
state at each time slice contains 33 discrete and 184 continuous variables. We
show how the system state can be tracked using probabilistic inference over the
model. We discuss how to deal with the various challenges presented by the
RWGS, providing a suite of techniques that are likely to be useful in a wide
range of applications. In particular, we describe a general framework for
dealing with nonlinear behavior using numerical integration techniques,
extending the successful Unscented Filter. We also show how to use a
fixed-point computation to deal with effects that develop at different time
scales, specifically rapid changes occurring during slowly changing processes.
We test our model using real data collected from the RWGS, demonstrating the
feasibility of hybrid DBNs for monitoring complex real-world physical systems.",Monitoring a Complez Physical System using a Hybrid Dynamic Bayes Net,2012
"Brenda Ng, Leonid Peshkin, Avi Pfeffer",AI,2012,"Exact monitoring in dynamic Bayesian networks is intractable, so approximate
algorithms are necessary. This paper presents a new family of approximate
monitoring algorithms that combine the best qualities of the particle filtering
and Boyen-Koller methods. Our algorithms maintain an approximate representation
the belief state in the form of sets of factored particles, that correspond to
samples of clusters of state variables. Empirical results show that our
algorithms outperform both ordinary particle filtering and the Boyen-Koller
algorithm on large systems.",Factored Particles for Scalable Monitoring,2012
"Martin Gebser, Torsten Grote, Roland Kaminski, Philipp Obermeier, Orkunt Sabuncu, Torsten Schaub",AI,2013,"The advance of Internet and Sensor technology has brought about new
challenges evoked by the emergence of continuous data streams. Beyond rapid
data processing, application areas like ambient assisted living, robotics, or
dynamic scheduling involve complex reasoning tasks. We address such scenarios
and elaborate upon approaches to knowledge-intense stream reasoning, based on
Answer Set Programming (ASP). While traditional ASP methods are devised for
singular problem solving, we develop new techniques to formulate and process
problems dealing with emerging as well as expiring data in a seamless way.",Answer Set Programming for Stream Reasoning,2013
Avi Pfeffer,AI,2013,"Suppose we are given the conditional probability of one variable given some
other variables.Normally the full joint distribution over the conditioning
variablesis required to determine the probability of the conditioned
variable.Under what circumstances are the marginal distributions over the
conditioning variables sufficient to determine the probability ofthe
conditioned variable?Sufficiency in this sense is equivalent to additive
separability ofthe conditional probability distribution.Such separability
structure is natural and can be exploited forefficient inference.Separability
has a natural generalization to conditional separability.Separability provides
a precise notion of weaklyinteracting subsystems in temporal probabilistic
models.Given a system that is decomposed into separable subsystems,
exactmarginal probabilities over subsystems at future points in time can
becomputed by propagating marginal subsystem probabilities, rather thancomplete
system joint probabilities.Thus, separability can make exact prediction
tractable.However, observations can break separability,so exact monitoring of
dynamic systems remains hard.","Sufficiency, Separability and Temporal Probabilistic Models",2013
"Pascal Poupart, Craig Boutilier",AI,2013,"We propose a new approach to value-directed belief state approximation for
POMDPs. The value-directed model allows one to choose approximation methods for
belief state monitoring that have a small impact on decision quality. Using a
vector space analysis of the problem, we devise two new search procedures for
selecting an approximation scheme that have much better computational
properties than existing methods. Though these provide looser error bounds, we
show empirically that they have a similar impact on decision quality in
practice, and run up to two orders of magnitude more quickly.",Vector-space Analysis of Belief-state Approximation for POMDPs,2013
"Pascal Poupart, Luis E. Ortiz, Craig Boutilier",AI,2013,"We consider the problem of approximate belief-state monitoring using particle
filtering for the purposes of implementing a policy for a partially-observable
Markov decision process (POMDP). While particle filtering has become a
widely-used tool in AI for monitoring dynamical systems, rather scant attention
has been paid to their use in the context of decision making. Assuming the
existence of a value function, we derive error bounds on decision quality
associated with filtering using importance sampling. We also describe an
adaptive procedure that can be used to dynamically determine the number of
samples required to meet specific error bounds. Empirical evidence is offered
supporting this technique as a profitable means of directing sampling effort
where it is needed to distinguish policies.",Value-Directed Sampling Methods for POMDPs,2013
Craig Boutilier,AI,2013,"Monitoring plan preconditions can allow for replanning when a precondition
fails, generally far in advance of the point in the plan where the precondition
is relevant. However, monitoring is generally costly, and some precondition
failures have a very small impact on plan quality. We formulate a model for
optimal precondition monitoring, using partially-observable Markov decisions
processes, and describe methods for solving this model efficitively, though
approximately. Specifically, we show that the single-precondition monitoring
problem is generally tractable, and the multiple-precondition monitoring
policies can be efficitively approximated using single-precondition soultions.",Approximately Optimal Monitoring of Plan Preconditions,2013
"Pascal Poupart, Craig Boutilier",AI,2013,"We consider the problem belief-state monitoring for the purposes of
implementing a policy for a partially-observable Markov decision process
(POMDP), specifically how one might approximate the belief state. Other schemes
for belief-state approximation (e.g., based on minimixing a measures such as
KL-diveregence between the true and estimated state) are not necessarily
appropriate for POMDPs. Instead we propose a framework for analyzing
value-directed approximation schemes, where approximation quality is determined
by the expected error in utility rather than by the error in the belief state
itself. We propose heuristic methods for finding good projection schemes for
belief state estimation - exhibiting anytime characteristics - given a POMDP
value fucntion. We also describe several algorithms for constructing bounds on
the error in decision quality (expected utility) associated with acting in
accordance with a given belief state approximation.",Value-Directed Belief State Approximation for POMDPs,2013
"David V. Pynadath, Michael P. Wellman",AI,2013,"Techniques for plan recognition under uncertainty require a stochastic model
of the plan-generation process. We introduce Probabilistic State-Dependent
Grammars (PSDGs) to represent an agent's plan-generation process. The PSDG
language model extends probabilistic context-free grammars (PCFGs) by allowing
production probabilities to depend on an explicit model of the planning agent's
internal and external state. Given a PSDG description of the plan-generation
process, we can then use inference algorithms that exploit the particular
independence properties of the PSDG language to efficiently answer
plan-recognition queries. The combination of the PSDG language model and
inference algorithms extends the range of plan-recognition domains for which
practical probabilistic inference is possible, as illustrated by applications
in traffic monitoring and air combat.",Probabilistic State-Dependent Grammars for Plan Recognition,2013
"Magnus Boman, Paul Davidsson, Hakan L. Younes",AI,2013,"Our hypothesis is that by equipping certain agents in a multi-agent system
controlling an intelligent building with automated decision support, two
important factors will be increased. The first is energy saving in the
building. The second is customer value---how the people in the building
experience the effects of the actions of the agents. We give evidence for the
truth of this hypothesis through experimental findings related to tools for
artificial decision making. A number of assumptions related to agent control,
through monitoring and delegation of tasks to other kinds of agents, of rooms
at a test site are relaxed. Each assumption controls at least one uncertainty
that complicates considerably the procedures for selecting actions part of each
such agent. We show that in realistic decision situations, room-controlling
agents can make bounded rational decisions even under dynamic real-time
constraints. This result can be, and has been, generalized to other domains
with even harsher time constraints.",Artificial Decision Making Under Uncertainty in Intelligent Buildings,2013
Philippe Smets,AI,2013,"We present examples where the use of belief functions provided sound and
elegant solutions to real life problems. These are essentially characterized by
?missing' information. The examples deal with 1) discriminant analysis using a
learning set where classes are only partially known; 2) an information
retrieval systems handling inter-documents relationships; 3) the combination of
data from sensors competent on partially overlapping frames; 4) the
determination of the number of sources in a multi-sensor environment by
studying the inter-sensors contradiction. The purpose of the paper is to report
on such applications where the use of belief functions provides a convenient
tool to handle ?messy' data problems.",Practical Uses of Belief Functions,2013
"Xavier Boyen, Daphne Koller",AI,2013,"The monitoring and control of any dynamic system depends crucially on the
ability to reason about its current status and its future trajectory. In the
case of a stochastic system, these tasks typically involve the use of a belief
state- a probability distribution over the state of the process at a given
point in time. Unfortunately, the state spaces of complex processes are very
large, making an explicit representation of a belief state intractable. Even in
dynamic Bayesian networks (DBNs), where the process itself can be represented
compactly, the representation of the belief state is intractable. We
investigate the idea of maintaining a compact approximation to the true belief
state, and analyze the conditions under which the errors due to the
approximations taken over the lifetime of the process do not accumulate to make
our answers completely irrelevant. We show that the error in a belief state
contracts exponentially as the process evolves. Thus, even with multiple
approximations, the error in our process remains bounded indefinitely. We show
how the additional structure of a DBN can be used to design our approximation
scheme, improving its performance significantly. We demonstrate the
applicability of our ideas in the context of a monitoring task, showing that
orders of magnitude faster inference can be achieved with only a small
degradation in accuracy.",Tractable Inference for Complex Stochastic Processes,2013
"Charles Castel, Corine Cossart, Catherine Tessier",AI,2013,"The situation assessment problem is considered, in terms of object,
condition, activity, and plan recognition, based on data coming from the
real-word {em via} various sensors. It is shown that uncertainty issues are
linked both to the models and to the matching algorithm. Three different types
of uncertainties are identified, and within each one, the numerical and the
symbolic cases are distinguished. The emphasis is then put on purely symbolic
uncertainties: it is shown that they can be dealt with within a purely symbolic
framework resulting from a transposition of classical numerical estimation
tools.",Dealing with Uncertainty in Situation Assessment: towards a Symbolic Approach,2013
"Pablo H. Ibarguengoytia, Luis Enrique Sucar, Sunil Vadera",AI,2013,"For many real time applications, it is important to validate the information
received from the sensors before entering higher levels of reasoning. This
paper presents an any time probabilistic algorithm for validating the
information provided by sensors. The system consists of two Bayesian network
models. The first one is a model of the dependencies between sensors and it is
used to validate each sensor. It provides a list of potentially faulty sensors.
To isolate the real faults, a second Bayesian network is used, which relates
the potential faults with the real faults. This second model is also used to
make the validation algorithm any time, by validating first the sensors that
provide more information. To select the next sensor to validate, and measure
the quality of the results at each stage, an entropy function is used. This
function captures in a single quantity both the certainty and specificity
measures of any time algorithms. Together, both models constitute a mechanism
for validating sensors in an any time fashion, providing at each step the
probability of correct/faulty for each sensor, and the total quality of the
results. The algorithm has been tested in the validation of temperature sensors
of a power plant.",Any Time Probabilistic Reasoning for Sensor Validation,2013
"Ami Berler, Solomon Eyal Shimony",AI,2013,"Wide-angle sonar mapping of the environment by mobile robot is nontrivial due
to several sources of uncertainty: dropouts due to ""specular"" reflections,
obstacle location uncertainty due to the wide beam, and distance measurement
error. Earlier papers address the latter problems, but dropouts remain a
problem in many environments. We present an approach that lifts the
overoptimistic independence assumption used in earlier work, and use Bayes nets
to represent the dependencies between objects of the model. Objects of the
model consist of readings, and of regions in which ""quasi location invariance""
of the (possible) obstacles exists, with respect to the readings. Simulation
supports the method's feasibility. The model is readily extensible to allow for
prior distributions, as well as other types of sensing operations.",Bayes Networks for Sonar Sensor Fusion,2013
Todd Michael Mansell,AI,2013,"A submarine's sonar team is responsible for detecting, localising and
classifying targets using information provided by the platform's sensor suite.
The information used to make these assessments is typically uncertain and/or
incomplete and is likely to require a measure of confidence in its reliability.
Moreover, improvements in sensor and communication technology are resulting in
increased amounts of on-platform and off-platform information available for
evaluation. This proliferation of imprecise information increases the risk of
overwhelming the operator. To assist the task of localisation and
classification a concept demonstration decision aid (Horizon), based on
evidential reasoning, has been developed. Horizon is an information fusion
software package for representing and fusing imprecise information about the
state of the world, expressed across suitable frames of reference. The Horizon
software is currently at prototype stage.",A Target Classification Decision Aid,2013
"Satnam Alag, Alice M. Agogino",AI,2013,"We extend Gaussian networks - directed acyclic graphs that encode
probabilistic relationships between variables - to its vector form. Vector
Gaussian continuous networks consist of composite nodes representing
multivariates, that take continuous values. These vector or composite nodes can
represent correlations between parents, as opposed to conventional univariate
nodes. We derive rules for inference in these networks based on two methods:
message propagation and topology transformation. These two approaches lead to
the development of algorithms, that can be implemented in either a centralized
or a decentralized manner. The domain of application of these networks are
monitoring and estimation problems. This new representation along with the
rules for inference developed here can be used to derive current Bayesian
algorithms such as the Kalman filter, and provide a rich foundation to develop
new algorithms. We illustrate this process by deriving the decentralized form
of the Kalman filter. This work unifies concepts from artificial intelligence
and modern control theory.",Inference Using Message Propagation and Topology Transformation in Vector Gaussian Continuous Networks,2013
"Pablo H. Ibarguengoytia, Luis Enrique Sucar, Sunil Vadera",AI,2013,"The validation of data from sensors has become an important issue in the
operation and control of modern industrial plants. One approach is to use
knowledge based techniques to detect inconsistencies in measured data. This
article presents a probabilistic model for the detection of such
inconsistencies. Based on probability propagation, this method is able to find
the existence of a possible fault among the set of sensors. That is, if an
error exists, many sensors present an apparent fault due to the propagation
from the sensor(s) with a real fault. So the fault detection mechanism can only
tell if a sensor has a potential fault, but it can not tell if the fault is
real or apparent. So the central problem is to develop a theory, and then an
algorithm, for distinguishing real and apparent faults, given that one or more
sensors can fail at the same time. This article then, presents an approach
based on two levels: (i) probabilistic reasoning, to detect a potential fault,
and (ii) constraint management, to distinguish the real fault from the apparent
ones. The proposed approach is exemplified by applying it to a power plant
model.",A Probabilistic Model For Sensor Validation,2013
"John S. Breese, Russ Blake",AI,2013,"We describe an application of belief networks to the diagnosis of bottlenecks
in computer systems. The technique relies on a high-level functional model of
the interaction between application workloads, the Windows NT operating system,
and system hardware. Given a workload description, the model predicts the
values of observable system counters available from the Windows NT performance
monitoring tool. Uncertainty in workloads, predictions, and counter values are
characterized with Gaussian distributions. During diagnostic inference, we use
observed performance monitor values to find the most probable assignment to the
workload parameters. In this paper we provide some background on automated
bottleneck detection, describe the structure of the system model, and discuss
empirical procedures for model calibration and verification. Part of the
calibration process includes generating a dataset to estimate a multivariate
Gaussian error model. Initial results in diagnosing bottlenecks are presented.",Automating Computer Bottleneck Detection with Belief Nets,2013
"Eric J. Horvitz, Matthew Barry",AI,2013,"We describe methods for managing the complexity of information displayed to
people responsible for making high-stakes, time-critical decisions. The
techniques provide tools for real-time control of the configuration and
quantity of information displayed to a user, and a methodology for designing
flexible human-computer interfaces for monitoring applications. After defining
a prototypical set of display decision problems, we introduce the expected
value of revealed information (EVRI) and the related measure of expected value
of displayed information (EVDI). We describe how these measures can be used to
enhance computer displays used for monitoring complex systems. We motivate the
presentation by discussing our efforts to employ decision-theoretic control of
displays for a time-critical monitoring application at the NASA Mission Control
Center in Houston.",Display of Information for Time-Critical Decision Making,2013
"David V. Pynadath, Michael P. Wellman",AI,2013,"Typical approaches to plan recognition start from a representation of an
agent's possible plans, and reason evidentially from observations of the
agent's actions to assess the plausibility of the various candidates. A more
expansive view of the task (consistent with some prior work) accounts for the
context in which the plan was generated, the mental state and planning process
of the agent, and consequences of the agent's actions in the world. We present
a general Bayesian framework encompassing this view, and focus on how context
can be exploited in plan recognition. We demonstrate the approach on a problem
in traffic monitoring, where the objective is to induce the plan of the driver
from observation of vehicle movements. Starting from a model of how the driver
generates plans, we show how the highway context can appropriately influence
the recognizer's interpretation of observed driver behavior.","Accounting for Context in Plan Recognition, with Application to Traffic Monitoring",2013
"Scott A. Musman, L. W. Chang",AI,2013,"The problems associated with scaling involve active and challenging research
topics in the area of artificial intelligence. The purpose is to solve real
world problems by means of AI technologies, in cases where the complexity of
representation of the real world problem is potentially combinatorial. In this
paper, we present a novel approach to cope with the scaling issues in Bayesian
belief networks for ship classification. The proposed approach divides the
conceptual model of a complex ship classification problem into a set of small
modules that work together to solve the classification problem while preserving
the functionality of the original model. The possible ways of explaining sensor
returns (e.g., the evidence) for some features, such as portholes along the
length of a ship, are sometimes combinatorial. Thus, using an exhaustive
approach, which entails the enumeration of all possible explanations, is
impractical for larger problems. We present a network structure (referred to as
Sequential Decomposition, SD) in which each observation is associated with a
set of legitimate outcomes which are consistent with the explanation of each
observed piece of evidence. The results show that the SD approach allows one to
represent feature-observation relations in a manageable way and achieve the
same explanatory power as an exhaustive approach.",A Study of Scaling Issues in Bayesian Belief Networks for Ship Classification,2013
"Francesco Fulvio Monai, Thomas Chehire",AI,2013,"Data fusion allows the elaboration and the evaluation of a situation
synthesized from low level informations provided by different kinds of sensors.
The fusion of the collected data will result in fewer and higher level
informations more easily assessed by a human operator and that will assist him
effectively in his decision process. In this paper we present the suitability
and the advantages of using a Possibilistic Assumption based Truth Maintenance
System (n-ATMS) in a data fusion military application. We first describe the
problem, the needed knowledge representation formalisms and problem solving
paradigms. Then we remind the reader of the basic concepts of ATMSs,
Possibilistic Logic and 11-ATMSs. Finally we detail the solution to the given
data fusion problem and conclude with the results and comparison with a
non-possibilistic solution.","Possibilistic Assumption based Truth Maintenance System, Validation in a Data Fusion Application",2013
"Ann Nicholson, J. M. Brady",AI,2013,"The trajectory of a robot is monitored in a restricted dynamic environment
using light beam sensor data. We have a Dynamic Belief Network (DBN), based on
a discrete model of the domain, which provides discrete monitoring analogous to
conventional quantitative filter techniques. Sensor observations are added to
the basic DBN in the form of specific evidence. However, sensor data is often
partially or totally incorrect. We show how the basic DBN, which infers only an
impossible combination of evidence, may be modified to handle specific types of
incorrect data which may occur in the domain. We then present an extension to
the DBN, the addition of an invalidating node, which models the status of the
sensor as working or defective. This node provides a qualitative explanation of
inconsistent data: it is caused by a defective sensor. The connection of
successive instances of the invalidating node models the status of a sensor
over time, allowing the DBN to handle both persistent and intermittent faults.",Sensor Validation Using Dynamic Belief Networks,2013
"Robert K. Paasch, Alice M. Agogino",AI,2013,"We present a general architecture for the monitoring and diagnosis of large
scale sensor-based systems with real time diagnostic constraints. This
architecture is multileveled, combining a single monitoring level based on
statistical methods with two model based diagnostic levels. At each level,
sources of uncertainty are identified, and integrated methodologies for
uncertainty management are developed. The general architecture was applied to
the monitoring and diagnosis of a specific nuclear physics detector at Lawrence
Berkeley National Laboratory that contained approximately 5000 components and
produced over 500 channels of output data. The general architecture is
scalable, and work is ongoing to apply it to detector systems one and two
orders of magnitude more complex.",Management of Uncertainty in the Multi-Level Monitoring and Diagnosis of the Time of Flight Scintillation Array,2013
"K. Bayse, M. Lejter, Keiji Kanazawa",AI,2013,"A significant problem in designing mobile robot control systems involves
coping with the uncertainty that arises in moving about in an unknown or
partially unknown environment and relying on noisy or ambiguous sensor data to
acquire knowledge about that environment. We describe a control system that
chooses what activity to engage in next on the basis of expectations about how
the information re- turned as a result of a given activity will improve 2 its
knowledge about the spatial layout of its environment. Certain of the
higher-level components of the control system are specified in terms of
probabilistic decision models whose output is used to mediate the behavior of
lower-level control components responsible for movement and sensing.",Reducing Uncertainty in Navigation and Exploration,2013
A. J. Hanson,AI,2013,"We point out the need to use probability amplitudes rather than probabilities
to model evidence accumulation in decision processes involving real physical
sensors. Optical information processing systems are given as typical examples
of systems that naturally gather evidence in this manner. We derive a new,
amplitude-based generalization of the Hough transform technique used for object
recognition in machine vision. We argue that one should use complex Hough
accumulators and square their magnitudes to get a proper probabilistic
interpretation of the likelihood that an object is present. Finally, we suggest
that probability amplitudes may have natural applications in connectionist
models, as well as in formulating knowledge-based reasoning problems.",Amplitude-Based Approach to Evidence Accumulation,2013
"Kenneth Basye, Thomas L. Dean",AI,2013,"Nearly all spatial reasoning problems involve uncertainty of one sort or
another. Uncertainty arises due to the inaccuracies of sensors used in
measuring distances and angles. We refer to this as directional uncertainty.
Uncertainty also arises in combining spatial information when one location is
mistakenly identified with another. We refer to this as recognition
uncertainty. Most problems in constructing spatial representations (maps) for
the purpose of navigation involve both directional and recognition uncertainty.
In this paper, we show that a particular class of spatial reasoning problems
involving the construction of representations of large-scale space can be
solved efficiently even in the presence of directional and recognition
uncertainty. We pay particular attention to the problems that arise due to
recognition uncertainty.",Map Learning with Indistinguishable Locations,2013
"Thomas O. Binford, Tod S. Levitt, Wallace B. Mann",AI,2013,"This is a preliminary version of visual interpretation integrating multiple
sensors in SUCCESSOR, an intelligent, model-based vision system. We pursue a
thorough integration of hierarchical Bayesian inference with comprehensive
physical representation of objects and their relations in a system for
reasoning with geometry, surface materials and sensor models in machine vision.
Bayesian inference provides a framework for accruing_ probabilities to rank
order hypotheses.",Bayesian Inference in Model-Based Machine Vision,2013
Steve Hanks,AI,2013,"We present a program that manages a database of temporally scoped beliefs.
The basic functionality of the system includes maintaining a network of
constraints among time points, supporting a variety of fetches, mediating the
application of causal rules, monitoring intervals of time for the addition of
new facts, and managing data dependencies that keep the database consistent. At
this level the system operates independent of any measure of belief or belief
calculus. We provide an example of how an application program mi9ght use this
functionality to implement a belief calculus.",Temporal Reasoning About Uncertain Worlds,2013
Su-shing Chen,AI,2013,"In [12], Nilsson proposed the probabilistic logic in which the truth values
of logical propositions are probability values between 0 and 1. It is
applicable to any logical system for which the consistency of a finite set of
propositions can be established. The probabilistic inference scheme reduces to
the ordinary logical inference when the probabilities of all propositions are
either 0 or 1. This logic has the same limitations of other probabilistic
reasoning systems of the Bayesian approach. For common sense reasoning,
consistency is not a very natural assumption. We have some well known examples:
{Dick is a Quaker, Quakers are pacifists, Republicans are not pacifists, Dick
is a Republican}and {Tweety is a bird, birds can fly, Tweety is a penguin}. In
this paper, we shall propose some extensions of the probabilistic logic. In the
second section, we shall consider the space of all interpretations, consistent
or not. In terms of frames of discernment, the basic probability assignment
(bpa) and belief function can be defined. Dempster's combination rule is
applicable. This extension of probabilistic logic is called the evidential
logic in [ 1]. For each proposition s, its belief function is represented by an
interval [Spt(s), Pls(s)]. When all such intervals collapse to single points,
the evidential logic reduces to probabilistic logic (in the generalized version
of not necessarily consistent interpretations). Certainly, we get Nilsson's
probabilistic logic by further restricting to consistent interpretations. In
the third section, we shall give a probabilistic interpretation of
probabilistic logic in terms of multi-dimensional random variables. This
interpretation brings the probabilistic logic into the framework of probability
theory. Let us consider a finite set S = {sl, s2, ..., Sn) of logical
propositions. Each proposition may have true or false values; and may be
considered as a random variable. We have a probability distribution for each
proposition. The e-dimensional random variable (sl,..., Sn) may take values in
the space of all interpretations of 2n binary vectors. We may compute absolute
(marginal), conditional and joint probability distributions. It turns out that
the permissible probabilistic interpretation vector of Nilsson [12] consists of
the joint probabilities of S. Inconsistent interpretations will not appear, by
setting their joint probabilities to be zeros. By summing appropriate joint
probabilities, we get probabilities of individual propositions or subsets of
propositions. Since the Bayes formula and other techniques are valid for
e-dimensional random variables, the probabilistic logic is actually very close
to the Bayesian inference schemes. In the last section, we shall consider a
relaxation scheme for probabilistic logic. In this system, not only new
evidences will update the belief measures of a collection of propositions, but
also constraint satisfaction among these propositions in the relational network
will revise these measures. This mechanism is similar to human reasoning which
is an evaluative process converging to the most satisfactory result. The main
idea arises from the consistent labeling problem in computer vision. This
method is originally applied to scene analysis of line drawings. Later, it is
applied to matching, constraint satisfaction and multi sensor fusion by several
authors [8], [16] (and see references cited there). Recently, this method is
used in knowledge aggregation by Landy and Hummel [9].",Some Extensions of Probabilistic Logic,2013
"B. R. Fox, Karl G. Kempf",AI,2013,"Scheduling in the factory setting is compounded by computational complexity
and temporal uncertainty. Together, these two factors guarantee that the
process of constructing an optimal schedule will be costly and the chances of
executing that schedule will be slight. Temporal uncertainty in the task
execution time can be offset by several methods: eliminate uncertainty by
careful engineering, restore certainty whenever it is lost, reduce the
uncertainty by using more accurate sensors, and quantify and circumscribe the
remaining uncertainty. Unfortunately, these methods focus exclusively on the
sources of uncertainty and fail to apply knowledge of the tasks which are to be
scheduled. A complete solution must adapt the schedule of activities to be
performed according to the evolving state of the production world. The example
of vision-directed assembly is presented to illustrate that the principle of
least commitment, in the creation of a plan, in the representation of a
schedule, and in the execution of a schedule, enables a robot to operate
intelligently and efficiently, even in the presence of considerable uncertainty
in the sequence of future events.","Planning, Scheduling, and Uncertainty in the Sequence of Future Events",2013
"Robert Fung, Chee Yee Chong",AI,2013,"Evidential reasoning in expert systems has often used ad-hoc uncertainty
calculi. Although it is generally accepted that probability theory provides a
firm theoretical foundation, researchers have found some problems with its use
as a workable uncertainty calculus. Among these problems are representation of
ignorance, consistency of probabilistic judgements, and adjustment of a priori
judgements with experience. The application of metaprobability theory to
evidential reasoning is a new approach to solving these problems.
Metaprobability theory can be viewed as a way to provide soft or hard
constraints on beliefs in much the same manner as the Dempster-Shafer theory
provides constraints on probability masses on subsets of the state space. Thus,
we use the Dempster-Shafer theory, an alternative theory of evidential
reasoning to illuminate metaprobability theory as a theory of evidential
reasoning. The goal of this paper is to compare how metaprobability theory and
Dempster-Shafer theory handle the adjustment of beliefs with evidence with
respect to a particular thought experiment. Sections 2 and 3 give brief
descriptions of the metaprobability and Dempster-Shafer theories.
Metaprobability theory deals with higher order probabilities applied to
evidential reasoning. Dempster-Shafer theory is a generalization of probability
theory which has evolved from a theory of upper and lower probabilities.
Section 4 describes a thought experiment and the metaprobability and
DempsterShafer analysis of the experiment. The thought experiment focuses on
forming beliefs about a population with 6 types of members {1, 2, 3, 4, 5, 6}.
A type is uniquely defined by the values of three features: A, B, C. That is,
if the three features of one member of the population were known then its type
could be ascertained. Each of the three features has two possible values, (e.g.
A can be either ""a0"" or ""al""). Beliefs are formed from evidence accrued from
two sensors: sensor A, and sensor B. Each sensor senses the corresponding
defining feature. Sensor A reports that half of its observations are ""a0"" and
half the observations are 'al'. Sensor B reports that half of its observations
are ``b0,' and half are ""bl"". Based on these two pieces of evidence, what
should be the beliefs on the distribution of types in the population? Note that
the third feature is not observed by any sensor.",Metaprobability and Dempster-Shafer in Evidential Reasoning,2013
Tod S. Levitt,AI,2013,"Artificial intelligence applications such as industrial robotics, military
surveillance, and hazardous environment clean-up, require situation
understanding based on partial, uncertain, and ambiguous or erroneous evidence.
It is necessary to evaluate the relative likelihood of multiple possible
hypotheses of the (current) situation faced by the decision making program.
Often, the evidence and hypotheses are hierarchical in nature. In image
understanding tasks, for example, evidence begins with raw imagery, from which
ambiguous features are extracted which have multiple possible aggregations
providing evidential support for the presence of multiple hypothesis of objects
and terrain, which in turn aggregate in multiple ways to provide partial
evidence for different interpretations of the ambient scene. Information fusion
for military situation understanding has a similar evidence/hypothesis
hierarchy from multiple sensor through message level interpretations, and also
provides evidence at multiple levels of the doctrinal hierarchy of military
forces.",Probabilistic Conflict Resolution in Hierarchical Hypothesis Spaces,2013
"Kartik Talamadupula, Octavian Udrea, Anton Riabov, Anand Ranganathan",AI,2013,"As network traffic monitoring software for cybersecurity, malware detection,
and other critical tasks becomes increasingly automated, the rate of alerts and
supporting data gathered, as well as the complexity of the underlying model,
regularly exceed human processing capabilities. Many of these applications
require complex models and constituent rules in order to come up with decisions
that influence the operation of entire systems. In this paper, we motivate the
novel ""strategic planning"" problem -- one of gathering data from the world and
applying the underlying model of the domain in order to come up with decisions
that will monitor the system in an automated manner. We describe our use of
automated planning methods to this problem, including the technique that we
used to solve it in a manner that would scale to the demands of a real-time,
real world scenario. We then present a PDDL model of one such application
scenario related to network administration and monitoring, followed by a
description of a novel integrated system that was built to accept generated
plans and to continue the execution process. Finally, we present evaluations of
two different automated planners and their different capabilities with our
integrated system, both on a six-month window of network data, and using a
simulator.",Strategic Planning for Network Data Analysis,2013
"Sri Hartati, Uyun"")]",AI,2013,"Determination of dietary food consumed a day for patients with diseases in
general, greatly affect the health of the body and the healing process, is no
exception for people with kidney disease and urinary tract. This paper presents
the determination of diet composition in the form of food subtance for people
with kidney and urinary tract diseases with a genetic fuzzy approach. This
approach combines fuzzy logic and genetic algorithms, which utilizing fuzzy
logic fuzzy tools and techniques to model the components of the genetic
algorithm and adapting genetic algorithm control parameters, with the aim of
improving system performance. The Mamdani fuzzy inference model and fuzzy rules
based on population parameters and generation are used to determine the
probability of crossover and mutation, and was using In this study, 400 food
survey data along with their substances was used as test material. From the
data, a varying amount of population is established. Each chromosome has 10
genes in which the value of each gene indicates the index number of foodstuffs
in the database. The fuzzy genetic approach produces 10 best food substance and
their compositions. The composition of these foods has nutritional value in
accordance with the number of calories needed by people with kidney and urinary
tract diseases by type of food.",Computation of Diet Composition for Patients Suffering from Kidney and Urinary Tract Diseases with the Fuzzy Genetic System,2013
"Alexander V. Terekhov, Regan"")]",AI,2013,"The question of the nature of space around us has occupied thinkers since the
dawn of humanity, with scientists and philosophers today implicitly assuming
that space is something that exists objectively. Here we show that this does
not have to be the case: the notion of space could emerge when biological
organisms seek an economic representation of their sensorimotor flow. The
emergence of spatial notions does not necessitate the existence of real
physical space, but only requires the presence of sensorimotor invariants
called `compensable' sensory changes. We show mathematically and then in
simulations that na\""ive agents making no assumptions about the existence of
space are able to learn these invariants and to build the abstract notion that
physicists call rigid displacement, which is independent of what is being
displaced. Rigid displacements may underly perception of space as an unchanging
medium within which objects are described by their relative positions. Our
findings suggest that the question of the nature of space, currently exclusive
to philosophy and physics, should also be addressed from the standpoint of
neuroscience and artificial intelligence.",Space as an invention of biological organisms,2013
"Aurélie Thébaut, Thibault Scholash, Brigitte Charnomordic, Nadine Hilgert",AI,2013,"This work proposes a framework using temporal data and domain knowledge in
order to analyze complex agronomical features. The expertise is first
formalized in an ontology, under the form of concepts and relationships between
them, and then used in conjunction with raw data and mathematical models to
design a software sensor. Next the software sensor outputs are put in relation
to product quality, assessed by quantitative measurements. This requires the
use of advanced data analysis methods, such as functional regression. The
methodology is applied to a case study involving an experimental design in
French vineyards. The temporal data consist of sap flow measurements, and the
goal is to explain fruit quality (sugar concentration and weight), using vine's
water courses through the various vine phenological stages. The results are
discussed, as well as the method genericity and robustness.",A modeling approach to design a software sensor and analyze agronomical features - Application to sap flow and grape quality relationship,2013
"Vaishak Belle, Hector Levesque",AI,2013,"Reasoning about degrees of belief in uncertain dynamic worlds is fundamental
to many applications, such as robotics and planning, where actions modify state
properties and sensors provide measurements, both of which are prone to noise.
With the exception of limited cases such as Gaussian processes over linear
phenomena, belief state evolution can be complex and hard to reason with in a
general way. This paper proposes a framework with new results that allows the
reduction of subjective probabilities after sensing and acting to questions
about the initial state only. We build on an expressive probabilistic
first-order logical account by Bacchus, Halpern and Levesque, resulting in a
methodology that, in principle, can be coupled with a variety of existing
inference solutions.",Reasoning about Probabilities in Dynamic Systems using Goal Regression,2013
"Keyan Zahedi, Georg Martius, Nihat Ay",AI,2013,"One of the main challenges in the field of embodied artificial intelligence
is the open-ended autonomous learning of complex behaviours. Our approach is to
use task-independent, information-driven intrinsic motivation(s) to support
task-dependent learning. The work presented here is a preliminary step in which
we investigate the predictive information (the mutual information of the past
and future of the sensor stream) as an intrinsic drive, ideally supporting any
kind of task acquisition. Previous experiments have shown that the predictive
information (PI) is a good candidate to support autonomous, open-ended learning
of complex behaviours, because a maximisation of the PI corresponds to an
exploration of morphology- and environment-dependent behavioural regularities.
The idea is that these regularities can then be exploited in order to solve any
given task. Three different experiments are presented and their results lead to
the conclusion that the linear combination of the one-step PI with an external
reward function is not generally recommended in an episodic policy gradient
setting. Only for hard tasks a great speed-up can be achieved at the cost of an
asymptotic performance lost.",Linear combination of one-step predictive information with an external reward in an episodic policy gradient setting: a critical analysis,2013
"Jean-Baptiste Lamy, Anis Ellini, Vahid Ebrahiminia, Jean-Daniel Zucker, Hector Falcoff, Alain Venot",AI,2013,"Well-designed medical decision support system (DSS) have been shown to
improve health care quality. However, before they can be used in real clinical
situations, these systems must be extensively tested, to ensure that they
conform to the clinical guidelines (CG) on which they are based. Existing
methods cannot be used for the systematic testing of all possible test cases.
We describe here a new exhaustive dynamic verification method. In this method,
the DSS is considered to be a black box, and the Quinlan C4.5 algorithm is used
to build a decision tree from an exhaustive set of DSS input vectors and
outputs. This method was successfully used for the testing of a medical DSS
relating to chronic diseases: the ASTI critiquing module for type 2 diabetes.",Use of the C4.5 machine learning algorithm to test a clinical guideline-based decision support system,2013
"Andreas Krause, Carlos Guestrin",AI,2014,"Many real-world decision making tasks require us to choose among several
expensive observations. In a sensor network, for example, it is important to
select the subset of sensors that is expected to provide the strongest
reduction in uncertainty. In medical decision making tasks, one needs to select
which tests to administer before deciding on the most effective treatment. It
has been general practice to use heuristic-guided procedures for selecting
observations. In this paper, we present the first efficient optimal algorithms
for selecting observations for a class of probabilistic graphical models. For
example, our algorithms allow to optimally label hidden variables in Hidden
Markov Models (HMMs). We provide results for both selecting the optimal subset
of observations, and for obtaining an optimal conditional observation plan.
  Furthermore we prove a surprising result: In most graphical models tasks, if
one designs an efficient algorithm for chain graphs, such as HMMs, this
procedure can be generalized to polytree graphical models. We prove that the
optimizing value of information is $NP^{PP}$-hard even for polytrees. It also
follows from our results that just computing decision theoretic value of
information objective functions, which are commonly used in practice, is a
#P-complete problem even on Naive Bayes models (a simple special case of
polytrees).
  In addition, we consider several extensions, such as using our algorithms for
scheduling observation selection for multiple sensors. We demonstrate the
effectiveness of our approach on several real-world datasets, including a
prototype sensor network deployment for energy conservation in buildings.",Optimal Value of Information in Graphical Models,2014
"Ruijie He, Emma Brunskill, Nicholas Roy",AI,2014,"Deciding how to act in partially observable environments remains an active
area of research. Identifying good sequences of decisions is particularly
challenging when good control performance requires planning multiple steps into
the future in domains with many states. Towards addressing this challenge, we
present an online, forward-search algorithm called the Posterior Belief
Distribution (PBD). PBD leverages a novel method for calculating the posterior
distribution over beliefs that result after a sequence of actions is taken,
given the set of observation sequences that could be received during this
process. This method allows us to efficiently evaluate the expected reward of a
sequence of primitive actions, which we refer to as macro-actions. We present a
formal analysis of our approach, and examine its performance on two very large
simulation experiments: scientific exploration and a target monitoring domain.
We also demonstrate our algorithm being used to control a real robotic
helicopter in a target monitoring experiment, which suggests that our approach
has practical potential for planning in real-world, large partially observable
domains where a multi-step lookahead is required to achieve good performance.",Efficient Planning under Uncertainty with Macro-actions,2014
"Alexander Feldman, Gregory Provan, Arjan van Gemund",AI,2014,"Model-based diagnostic reasoning often leads to a large number of diagnostic
hypotheses. The set of diagnoses can be reduced by taking into account extra
observations (passive monitoring), measuring additional variables (probing) or
executing additional tests (sequential diagnosis/test sequencing). In this
paper we combine the above approaches with techniques from Automated Test
Pattern Generation (ATPG) and Model-Based Diagnosis (MBD) into a framework
called FRACTAL (FRamework for ACtive Testing ALgorithms). Apart from the inputs
and outputs that connect a system to its environment, in active testing we
consider additional input variables to which a sequence of test vectors can be
supplied. We address the computationally hard problem of computing optimal
control assignments (as defined in FRACTAL) in terms of a greedy approximation
algorithm called FRACTAL-G. We compare the decrease in the number of remaining
minimal cardinality diagnoses of FRACTAL-G to that of two more FRACTAL
algorithms: FRACTAL-ATPG and FRACTAL-P. FRACTAL-ATPG is based on ATPG and
sequential diagnosis while FRACTAL-P is based on probing and, although not an
active testing algorithm, provides a baseline for comparing the lower bound on
the number of reachable diagnoses for the FRACTAL algorithms. We empirically
evaluate the trade-offs of the three FRACTAL algorithms by performing extensive
experimentation on the ISCAS85/74XXX benchmark of combinational circuits.",A Model-Based Active Testing Approach to Sequential Diagnosis,2014
"Atif Ali Khan, Oumair Naseer, Daciana Iliescu, Evor Hines",AI,2014,"One of the defining characteristic of human being is their ability to walk
upright. Loss or restriction of such ability whether due to the accident, spine
problem, stroke or other neurological injuries can cause tremendous stress on
the patients and hence will contribute negatively to their quality of life.
Modern research shows that physical exercise is very important for maintaining
physical fitness and adopting a healthier life style. In modern days treadmill
is widely used for physical exercises and training which enables the user to
set up an exercise regime that can be adhered to irrespective of the weather
conditions. Among the users of treadmills today are medical facilities such as
hospitals, rehabilitation centres, medical and physiotherapy clinics etc. The
process of assisted training or doing rehabilitation exercise through treadmill
is referred to as treadmill therapy. A modern treadmill is an automated machine
having built in functions and predefined features. Most of the treadmills used
today are one dimensional and user can only walk in one direction. This paper
presents the idea of using omnidirectional treadmills which will be more
appealing to the patients as they can walk in any direction, hence encouraging
them to do exercises more frequently. This paper proposes a fuzzy control
design and possible implementation strategy to assist patients in treadmill
therapy. By intelligently controlling the safety belt attached to the treadmill
user, one can help them steering left, right or in any direction. The use of
intelligent treadmill therapy can help patients to improve their walking
ability without being continuously supervised by the specialists. The patients
can walk freely within a limited space and the support system will provide
continuous evaluation of their position and can adjust the control parameters
of treadmill accordingly to provide best possible assistance.",Fuzzy Controller Design for Assisted Omni-Directional Treadmill Therapy,2014
Garimella Rama Murthy,AI,2014,"Motivated by the application problem of sensor fusion the author introduced
the concept of graded set. It is reasoned that in classification problem
arising in an information system (represented by information table), a novel
set called Granular set naturally arises. It is realized that in any
hierarchical classification problem, Granular set naturally arises. Also when
the target set of objects forms a graded set the lower and upper approximations
of target sets form a graded set. This generalizes the concept of rough set. It
is hoped that a detailed theory of granular/ graded sets finds several
applications.",Towards a theory of granular sets,2014
"Enrico De Santis, Lorenzo Livi, Alireza Sadeghian, Antonello Rizzi",AI,2014,"Detecting faults in electrical power grids is of paramount importance, either
from the electricity operator and consumer viewpoints. Modern electric power
grids (smart grids) are equipped with smart sensors that allow to gather
real-time information regarding the physical status of all the component
elements belonging to the whole infrastructure (e.g., cables and related
insulation, transformers, breakers and so on). In real-world smart grid
systems, usually, additional information that are related to the operational
status of the grid itself are collected such as meteorological information.
Designing a suitable recognition (discrimination) model of faults in a
real-world smart grid system is hence a challenging task. This follows from the
heterogeneity of the information that actually determine a typical fault
condition. The second point is that, for synthesizing a recognition model, in
practice only the conditions of observed faults are usually meaningful.
Therefore, a suitable recognition model should be synthesized by making use of
the observed fault conditions only. In this paper, we deal with the problem of
modeling and recognizing faults in a real-world smart grid system, which
supplies the entire city of Rome, Italy. Recognition of faults is addressed by
following a combined approach of multiple dissimilarity measures customization
and one-class classification techniques. We provide here an in-depth study
related to the available data and to the models synthesized by the proposed
one-class classifier. We offer also a comprehensive analysis of the fault
recognition results by exploiting a fuzzy set based reliability decision rule.",Modeling and Recognition of Smart Grid Faults by a Combined Approach of Dissimilarity Learning and One-Class Classification,2014
Paolo Liberatore,AI,2014,"A common assumption in belief revision is that the reliability of the
information sources is either given, derived from temporal information, or the
same for all. This article does not describe a new semantics for integration
but the problem of obtaining the reliability of the sources given the result of
a previous merging. As an example, the relative reliability of two sensors can
be assessed given some certain observation, and allows for subsequent mergings
of data coming from them.",Belief revision by examples,2014
"Marjan Alirezaie, Amy Loutfi",AI,2014,"In this paper an ontological representation and reasoning paradigm has been
proposed for interpretation of time-series signals. The signals come from
sensors observing a smart environment. The signal chosen for the annotation
process is a set of unintuitive and complex gas sensor data. The ontology of
this paradigm is inspired form the SSN ontology (Semantic Sensor Network) and
used for representation of both the sensor data and the contextual information.
The interpretation process is mainly done by an incremental ASP solver which as
input receives a logic program that is generated from the contents of the
ontology. The contextual information together with high level domain knowledge
given in the ontology are used to infer explanations (answer sets) for changes
in the ambient air detected by the gas sensors.",Reasoning for Improved Sensor Data Interpretation in a Smart Home,2014
"Denis Kleyko, Evgeny Osipov, Alexander Senior, Asad I. Khan, Y. Ahmet Şekercioğlu",AI,2015,"This article proposes the use of Vector Symbolic Architectures for
implementing Hierarchical Graph Neuron, an architecture for memorizing patterns
of generic sensor stimuli. The adoption of a Vector Symbolic representation
ensures a one-layered design for the approach, while maintaining the previously
reported properties and performance characteristics of Hierarchical Graph
Neuron, and also improving the noise resistance of the architecture. The
proposed architecture enables a linear (with respect to the number of stored
entries) time search for an arbitrary sub-pattern.",Holographic Graph Neuron: a Bio-Inspired Architecture for Pattern Processing,2015
"Alexander Artikis, Marek Sergot, Georgios Paliouras",AI,2015,"Systems for symbolic event recognition accept as input a stream of
time-stamped events from sensors and other computational devices, and seek to
identify high-level composite events, collections of events that satisfy some
pattern. RTEC is an Event Calculus dialect with novel implementation and
'windowing' techniques that allow for efficient event recognition, scalable to
large data streams. RTEC can deal with applications where event data arrive
with a (variable) delay from, and are revised by, the underlying sources. RTEC
can update already recognised events and recognise new events when data arrive
with a delay or following data revision. Our evaluation shows that RTEC can
support real-time event recognition and is capable of meeting the performance
requirements identified in a recent survey of event processing use cases.",Reactive Reasoning with the Event Calculus,2015
"Gerhard Brewka, Stefan Ellmauthaler, Jörg Pührer",AI,2015,"We show in this paper how managed multi-context systems (mMCSs) can be turned
into a reactive formalism suitable for continuous reasoning in dynamic
environments. We extend mMCSs with (abstract) sensors and define the notion of
a run of the extended systems. We then show how typical problems arising in
online reasoning can be addressed: handling potentially inconsistent sensor
input, modeling intelligent forms of forgetting, selective integration of
knowledge, and controlling the reasoning effort spent by contexts, like setting
contexts to an idle mode. We also investigate the complexity of some important
related decision problems and discuss different design choices which are given
to the knowledge engineer.",Multi-Context Systems for Reactive Reasoning in Dynamic Environments,2015
"Amit Sheth, Pramod Anantharam, Cory Henson",AI,2015,"The World Wide Web continues to evolve and serve as the infrastructure for
carrying massive amounts of multimodal and multisensory observations. These
observations capture various situations pertinent to people's needs and
interests along with all their idiosyncrasies. To support human-centered
computing that empower people in making better and timely decisions, we look
towards computation that is inspired by human perception and cognition. Toward
this goal, we discuss computing paradigms of semantic computing, cognitive
computing, and an emerging aspect of computing, which we call perceptual
computing. In our view, these offer a continuum to make the most out of vast,
growing, and diverse data pertinent to human needs and interests. We propose
details of perceptual computing characterized by interpretation and exploration
operations comparable to the interleaving of bottom and top brain processing.
  This article consists of two parts. First we describe semantic computing,
cognitive computing, and perceptual computing to lay out distinctions while
acknowledging their complementary capabilities. We then provide a conceptual
overview of the newest of these three paradigms--perceptual computing. For
further insights, we focus on an application scenario of asthma management
converting massive, heterogeneous and multimodal (big) data into actionable
information or smart data.","Semantic, Cognitive, and Perceptual Computing: Advances toward Computing for Human Experience",2015
Ehsan Lotfi,AI,2015,"The ozone level prediction is an important task of air quality agencies of
modern cities. In this paper, we design an ozone level alarm system (OLP) for
Isfahan city and test it through the real word data from 1-1-2000 to 7-6-2011.
We propose a computer based system with three inputs and single output. The
inputs include three sensors of solar ultraviolet (UV), total solar radiation
(TSR) and total ozone (O3). And the output of the system is the predicted O3 of
the next day and the alarm massages. A developed artificial intelligence (AI)
algorithm is applied to determine the output, based on the inputs variables.
For this issue, AI models, including supervised brain emotional learning (BEL),
adaptive neuro-fuzzy inference system (ANFIS) and artificial neural networks
(ANNs), are compared in order to find the best model. The simulation of the
proposed system shows that it can be used successfully in prediction of major
cities ozone level.",Design of an Alarm System for Isfahan Ozone Level based on Artificial Intelligence Predictor Models,2015
"Sarath P R, Sunil Mandhan, Yoshiki Niwa",AI,2016,"This paper describes about information extraction system, which is an
extension of the system developed by team Hitachi for ""Disease/Disorder
Template filling"" task organized by ShARe/CLEF eHealth Evolution Lab 2014. In
this extension module we focus on extraction of numerical attributes and values
from discharge summary records and associating correct relation between
attributes and values. We solve the problem in two steps. First step is
extraction of numerical attributes and values, which is developed as a Named
Entity Recognition (NER) model using Stanford NLP libraries. Second step is
correctly associating the attributes to values, which is developed as a
relation extraction module in Apache cTAKES framework. We integrated Stanford
NER model as cTAKES pipeline component and used in relation extraction module.
Conditional Random Field (CRF) algorithm is used for NER and Support Vector
Machines (SVM) for relation extraction. For attribute value relation
extraction, we observe 95% accuracy using NER alone and combined accuracy of
87% with NER and SVM.",Numerical Atrribute Extraction from Clinical Texts,2016
"Mohit Yadav, Pankaj Malhotra, Lovekesh Vig, K Sriram, Gautam Shroff",AI,2016,"Machines of all kinds from vehicles to industrial equipment are increasingly
instrumented with hundreds of sensors. Using such data to detect anomalous
behaviour is critical for safety and efficient maintenance. However, anomalies
occur rarely and with great variety in such systems, so there is often
insufficient anomalous data to build reliable detectors. A standard approach to
mitigate this problem is to use one class methods relying only on data from
normal behaviour. Unfortunately, even these approaches are more likely to fail
in the scenario of a dynamical system with manual control input(s). Normal
behaviour in response to novel control input(s) might look very different to
the learned detector which may be incorrectly detected as anomalous. In this
paper, we address this issue by modelling time-series via Ordinary Differential
Equations (ODE) and utilising such an ODE model to simulate the behaviour of
dynamical systems under varying control inputs. The available data is then
augmented with data generated from the ODE, and the anomaly detector is
retrained on this augmented dataset. Experiments demonstrate that ODE-augmented
training data allows better coverage of possible control input(s) and results
in learning more accurate distinctions between normal and anomalous behaviour
in time-series.",ODE - Augmented Training Improves Anomaly Detection in Sensor Data from Machines,2016
"Tom Everitt, Marcus Hutter",AI,2016,"How can we design good goals for arbitrarily intelligent agents?
Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not
work well for generally intelligent agents, as RL agents are incentivised to
shortcut the reward sensor for maximum reward -- the so-called wireheading
problem. In this paper we suggest an alternative to RL called value
reinforcement learning (VRL). In VRL, agents use the reward signal to learn a
utility function. The VRL setup allows us to remove the incentive to wirehead
by placing a constraint on the agent's actions. The constraint is defined in
terms of the agent's belief distributions, and does not require an explicit
specification of which actions constitute wireheading.",Avoiding Wireheading with Value Reinforcement Learning,2016
"Philipp Geiger, Katja Hofmann, Bernhard Schölkopf",AI,2016,"The amount of digitally available but heterogeneous information about the
world is remarkable, and new technologies such as self-driving cars, smart
homes, or the internet of things may further increase it. In this paper we
present preliminary ideas about certain aspects of the problem of how such
heterogeneous information can be harnessed by autonomous agents. After
discussing potentials and limitations of some existing approaches, we
investigate how \emph{experiments} can help to obtain a better understanding of
the problem. Specifically, we present a simple agent that integrates video data
from a different agent, and implement and evaluate a version of it on the novel
experimentation platform \emph{Malmo}. The focus of a second investigation is
on how information about the hardware of different agents, the agents' sensory
data, and \emph{causal} information can be utilized for knowledge transfer
between agents and subsequently more data-efficient decision making. Finally,
we discuss potential future steps w.r.t.\ theory and experimentation, and
formulate open questions.",Experimental and causal view on information integration in autonomous agents,2018
"Jagannath Roy, Kajal Chatterjee, Abhirup Bandhopadhyay, Samarjit Kar",AI,2016,"In this paper, a novel multiple criteria decision making (MCDM) methodology
is presented for assessing and prioritizing medical tourism destinations in
uncertain environment. A systematic evaluation and assessment method is
proposed by integrating rough number based AHP (Analytic Hierarchy Process) and
rough number based MABAC (Multi-Attributive Border Approximation area
Comparison). Rough number is used to aggregate individual judgments and
preferences to deal with vagueness in decision making due to limited data.
Rough AHP analyzes the relative importance of criteria based on their
preferences given by experts. Rough MABAC evaluates the alternative sites based
on the criteria weights. The proposed methodology is explained through a case
study considering different cities for healthcare service in India. The
validity of the obtained ranking for the given decision making problem is
established by testing criteria proposed by Wang and Triantaphyllou (2008)
along with further analysis and discussion.",Evaluation and selection of Medical Tourism sites: A rough AHP based MABAC approach,2016
"Mona Khaffaf, Arshia Khaffaf",AI,2016,"After an earthquake, disaster sites pose a multitude of health and safety
concerns. A rescue operation of people trapped in the ruins after an earthquake
disaster requires a series of intelligent behavior, including planning. For a
successful rescue operation, given a limited number of available actions and
regulations, the role of planning in rescue operations is crucial. Fortunately,
recent developments in automated planning by artificial intelligence community
can help different organization in this crucial task. Due to the number of
rules and regulations, we believe that a rule based system for planning can be
helpful for this specific planning problem. In this research work, we use logic
rules to represent rescue and related regular regulations, together with a
logic based planner to solve this complicated problem. Although this research
is still in the prototyping and modeling stage, it clearly shows that rule
based languages can be a good infrastructure for this computational task. The
results of this research can be used by different organizations, such as
Iranian Red Crescent Society and International Institute of Seismology and
Earthquake Engineering (IISEE).",Resource Planning For Rescue Operations,2016
Emanuel Diamant,AI,2016,"We are at the dawn of a new era, where advances in computer power, broadband
communication and digital sensor technologies have led to an unprecedented
flood of data inundating our surrounding. It is generally believed that means
such as Computational Intelligence will help to outlive these tough times.
However, these hopes are improperly high. Computational Intelligence is a
surprising composition of two mutually exclusive and contradicting constituents
that could be coupled only if you disregard and neglect their controversies:
""Computational"" implies reliance on data processing and ""Intelligence"" implies
reliance on information processing. Only those who are indifferent to
data-information discrepancy can believe that such a combination can be viable.
We do not believe in miracles, so we will try to share with you our
reservations.","You want to survive the data deluge: Be careful, Computational Intelligence will not serve you as a rescue boat",2016
"Jenna M. Reps, Uwe Aickelin, Richard B. Hubbard",AI,2016,"Purpose: To develop a framework for identifying and incorporating candidate
confounding interaction terms into a regularised cox regression analysis to
refine adverse drug reaction signals obtained via longitudinal observational
data. Methods: We considered six drug families that are commonly associated
with myocardial infarction in observational healthcare data, but where the
causal relationship ground truth is known (adverse drug reaction or not). We
applied emergent pattern mining to find itemsets of drugs and medical events
that are associated with the development of myocardial infarction. These are
the candidate confounding interaction terms. We then implemented a cohort study
design using regularised cox regression that incorporated and accounted for the
candidate confounding interaction terms. Results The methodology was able to
account for signals generated due to confounding and a cox regression with
elastic net regularisation correctly ranked the drug families known to be true
adverse drug reactions above those.",Refining adverse drug reaction signals by incorporating interaction variables identified using emergent pattern mining,2016
"Jiangang Ma, Le Sun, Hua Wang, Yanchun Zhang, Uwe Aickelin",AI,2016,"Uncertain data streams have been widely generated in many Web applications.
The uncertainty in data streams makes anomaly detection from sensor data
streams far more challenging. In this paper, we present a novel framework that
supports anomaly detection in uncertain data streams. The proposed framework
adopts an efficient uncertainty pre-processing procedure to identify and
eliminate uncertainties in data streams. Based on the corrected data streams,
we develop effective period pattern recognition and feature extraction
techniques to improve the computational efficiency. We use classification
methods for anomaly detection in the corrected data stream. We also empirically
show that the proposed approach shows a high accuracy of anomaly detection on a
number of real datasets.",Supervised Anomaly Detection in Uncertain Pseudoperiodic Data Streams,2016
Vladislav B Kovchegov,AI,2016,"In this paper we discuss methods of using the language of actions, formal
languages, and grammars for qualitative conceptual linguistic modeling of
companies as technological and human institutions. The main problem following
the discussion is the problem to find and describe a language structure for
external and internal flow of information of companies. We anticipate that the
language structure of external and internal base flows determine the structure
of companies. In the structure modeling of an abstract industrial company an
internal base flow of information is constructed as certain flow of words
composed on the theoretical parts-processes-actions language. The language of
procedures is found for an external base flow of information for an insurance
company. The formal stochastic grammar for the language of procedures is found
by statistical methods and is used in understanding the tendencies of the
health care industry. We present the model of human communications as a random
walk on the semantic tree","The languages of actions, formal grammars and qualitive modeling of companies",2016
Rohitash Chandra,AI,2017,"In the past, several models of consciousness have become popular and have led
to the development of models for machine consciousness with varying degrees of
success and challenges for simulation and implementations. Moreover, affective
computing attributes that involve emotions, behavior and personality have not
been the focus of models of consciousness as they lacked motivation for
deployment in software applications and robots. The affective attributes are
important factors for the future of machine consciousness with the rise of
technologies that can assist humans. Personality and affection hence can give
an additional flavor for the computational model of consciousness in humanoid
robotics. Recent advances in areas of machine learning with a focus on deep
learning can further help in developing aspects of machine consciousness in
areas that can better replicate human sensory perceptions such as speech
recognition and vision. With such advancements, one encounters further
challenges in developing models that can synchronize different aspects of
affective computing. In this paper, we review some existing models of
consciousnesses and present an affective computational model that would enable
the human touch and feel for robotic systems.",An affective computational model for machine consciousness,2017
"Junbo Zhang, Yu Zheng, Dekang Qi, Ruiyuan Li, Xiuwen Yi, Tianrui Li",AI,2017,"Forecasting the flow of crowds is of great importance to traffic management
and public safety, and very challenging as it is affected by many complex
factors, including spatial dependencies (nearby and distant), temporal
dependencies (closeness, period, trend), and external conditions (e.g., weather
and events). We propose a deep-learning-based approach, called ST-ResNet, to
collectively forecast two types of crowd flows (i.e. inflow and outflow) in
each and every region of a city. We design an end-to-end structure of ST-ResNet
based on unique properties of spatio-temporal data. More specifically, we
employ the residual neural network framework to model the temporal closeness,
period, and trend properties of crowd traffic. For each property, we design a
branch of residual convolutional units, each of which models the spatial
properties of crowd traffic. ST-ResNet learns to dynamically aggregate the
output of the three residual neural networks based on data, assigning different
weights to different branches and regions. The aggregation is further combined
with external factors, such as weather and day of the week, to predict the
final traffic of crowds in each and every region. We have developed a real-time
system based on Microsoft Azure Cloud, called UrbanFlow, providing the crowd
flow monitoring and forecasting in Guiyang City of China. In addition, we
present an extensive experimental evaluation using two types of crowd flows in
Beijing and New York City (NYC), where ST-ResNet outperforms nine well-known
baselines.",Predicting Citywide Crowd Flows Using Deep Spatio-Temporal Residual Networks,2017
"Daniel Meana-Llorián, Cristian González García, B. Cristina Pelayo G-Bustelo, Juan Manuel Cueva Lovelle, Nestor Garcia-Fernandez",AI,2017,"The Internet of Things is arriving to our homes or cities through fields
already known like Smart Homes, Smart Cities, or Smart Towns. The monitoring of
environmental conditions of cities can help to adapt the indoor locations of
the cities in order to be more comfortable for people who stay there. A way to
improve the indoor conditions is an efficient temperature control, however, it
depends on many factors like the different combinations of outdoor temperature
and humidity. Therefore, adjusting the indoor temperature is not setting a
value according to other value. There are many more factors to take into
consideration, hence the traditional logic based in binary states cannot be
used. Many problems cannot be solved with a set of binary solutions and we need
a new way of development. Fuzzy logic is able to interpret many states, more
than two states, giving to computers the capacity to react in a similar way to
people. In this paper we will propose a new approach to control the temperature
using the Internet of Things together its platforms and fuzzy logic regarding
not only the indoor temperature but also the outdoor temperature and humidity
in order to save energy and to set a more comfortable environment for their
users. Finally, we will conclude that the fuzzy approach allows us to achieve
an energy saving around 40% and thus, save money.",IoFClime: The fuzzy logic and the Internet of Things to control indoor temperature regarding the outdoor ambient conditions,2017
"Varun Raj Kompella, Laurenz Wiskott",AI,2017,"A compact information-rich representation of the environment, also called a
feature abstraction, can simplify a robot's task of mapping its raw sensory
inputs to useful action sequences. However, in environments that are
non-stationary and only partially observable, a single abstraction is probably
not sufficient to encode most variations. Therefore, learning multiple sets of
spatially or temporally local, modular abstractions of the inputs would be
beneficial. How can a robot learn these local abstractions without a teacher?
More specifically, how can it decide from where and when to start learning a
new abstraction? A recently proposed algorithm called Curious Dr. MISFA
addresses this problem. The algorithm is based on two underlying learning
principles called artificial curiosity and slowness. The former is used to make
the robot self-motivated to explore by rewarding itself whenever it makes
progress learning an abstraction; the later is used to update the abstraction
by extracting slowly varying components from raw sensory inputs. Curious Dr.
MISFA's application is, however, limited to discrete domains constrained by a
pre-defined state space and has design limitations that make it unstable in
certain situations. This paper presents a significant improvement that is
applicable to continuous environments, is computationally less expensive,
simpler to use with fewer hyper parameters, and stable in certain
non-stationary environments. We demonstrate the efficacy and stability of our
method in a vision-based robot simulator.",Intrinsically Motivated Acquisition of Modular Slow Features for Humanoids in Continuous and Non-Stationary Environments,2017
"Hyunmin Chae, Chang Mook Kang, ByeoungDo Kim, Jaekyum Kim, Chung Choo Chung, Jun Won Choi",AI,2017,"In this paper, we propose a new autonomous braking system based on deep
reinforcement learning. The proposed autonomous braking system automatically
decides whether to apply the brake at each time step when confronting the risk
of collision using the information on the obstacle obtained by the sensors. The
problem of designing brake control is formulated as searching for the optimal
policy in Markov decision process (MDP) model where the state is given by the
relative position of the obstacle and the vehicle's speed, and the action space
is defined as whether brake is stepped or not. The policy used for brake
control is learned through computer simulations using the deep reinforcement
learning method called deep Q-network (DQN). In order to derive desirable
braking policy, we propose the reward function which balances the damage
imposed to the obstacle in case of accident and the reward achieved when the
vehicle runs out of risk as soon as possible. DQN is trained for the scenario
where a vehicle is encountered with a pedestrian crossing the urban road.
Experiments show that the control agent exhibits desirable control behavior and
avoids collision without any mistake in various uncertain environments.",Autonomous Braking System via Deep Reinforcement Learning,2017
"Quoc Duy Vo, Jaya Thomas, Shinyoung Cho, Pradipta De, Bong Jun Choi, Lee Sael",AI,2017,"Business Intelligence and Analytics (BI&A) is the process of extracting and
predicting business-critical insights from data. Traditional BI focused on data
collection, extraction, and organization to enable efficient query processing
for deriving insights from historical data. With the rise of big data and cloud
computing, there are many challenges and opportunities for the BI. Especially
with the growing number of data sources, traditional BI\&A are evolving to
provide intelligence at different scales and perspectives - operational BI,
situational BI, self-service BI. In this survey, we review the evolution of
business intelligence systems in full scale from back-end architecture to and
front-end applications. We focus on the changes in the back-end architecture
that deals with the collection and organization of the data. We also review the
changes in the front-end applications, where analytic services and
visualization are the core components. Using a uses case from BI in Healthcare,
which is one of the most complex enterprises, we show how BI\&A will play an
important role beyond the traditional usage. The survey provides a holistic
view of Business Intelligence and Analytics for anyone interested in getting a
complete picture of the different pieces in the emerging next generation BI\&A
solutions.",Next Generation Business Intelligence and Analytics: A Survey,2017
"Fabio Guigou, Pierre Collet, Pierre Parrend",AI,2017,"The advent of the Big Data hype and the consistent recollection of event logs
and real-time data from sensors, monitoring software and machine configuration
has generated a huge amount of time-varying data in about every sector of the
industry. Rule-based processing of such data has ceased to be relevant in many
scenarios where anomaly detection and pattern mining have to be entirely
accomplished by the machine. Since the early 2000s, the de-facto standard for
representing time series has been the Symbolic Aggregate approXimation (SAX).In
this document, we present a few algorithms using this representation for
anomaly detection and motif discovery, also known as pattern mining, in such
data. We propose a benchmark of anomaly detection algorithms using data from
Cloud monitoring software.",Anomaly detection and motif discovery in symbolic representations of time series,2017
"Mahardhika Pratama, Eric Dimla, Chow Yin Lai, Edwin Lughofer",AI,2017,"As manufacturing processes become increasingly automated, so should tool
condition monitoring (TCM) as it is impractical to have human workers monitor
the state of the tools continuously. Tool condition is crucial to ensure the
good quality of products: Worn tools affect not only the surface quality but
also the dimensional accuracy, which means higher reject rate of the products.
Therefore, there is an urgent need to identify tool failures before it occurs
on the fly. While various versions of intelligent tool condition monitoring
have been proposed, most of them suffer from a cognitive nature of traditional
machine learning algorithms. They focus on the how to learn process without
paying attention to other two crucial issues: what to learn, and when to learn.
The what to learn and the when to learn provide self regulating mechanisms to
select the training samples and to determine time instants to train a model. A
novel tool condition monitoring approach based on a psychologically plausible
concept, namely the metacognitive scaffolding theory, is proposed and built
upon a recently published algorithm, recurrent classifier (rClass). The
learning process consists of three phases: what to learn, how to learn, when to
learn and makes use of a generalized recurrent network structure as a cognitive
component. Experimental studies with real-world manufacturing data streams were
conducted where rClass demonstrated the highest accuracy while retaining the
lowest complexity over its counterparts.",Metacognitive Learning Approach for Online Tool Condition Monitoring,2017
"Yangming Zhou, Jin-Kao Hao, Fred Glover",AI,2017,"Critical node problems involve identifying a subset of critical nodes from an
undirected graph whose removal results in optimizing a pre-defined measure over
the residual graph. As useful models for a variety of practical applications,
these problems are computational challenging. In this paper, we study the
classic critical node problem (CNP) and introduce an effective memetic
algorithm for solving CNP. The proposed algorithm combines a double
backbone-based crossover operator (to generate promising offspring solutions),
a component-based neighborhood search procedure (to find high-quality local
optima) and a rank-based pool updating strategy (to guarantee a healthy
population). Specially, the component-based neighborhood search integrates two
key techniques, i.e., two-phase node exchange strategy and node weighting
scheme. The double backbone-based crossover extends the idea of general
backbone-based crossovers. Extensive evaluations on 42 synthetic and real-world
benchmark instances show that the proposed algorithm discovers 21 new upper
bounds and matches 18 previous best-known upper bounds. We also demonstrate the
relevance of our algorithm for effectively solving a variant of the classic
CNP, called the cardinality-constrained critical node problem. Finally, we
investigate the usefulness of each key algorithmic component.",Memetic search for identifying critical nodes in sparse graphs,2017
José F. Fontanari,AI,2017,"The brain's self-monitoring of activities, including internal activities -- a
functionality that we refer to as awareness -- has been suggested as a key
element of consciousness. Here we investigate whether the presence of an
inner-eye-like process (monitor) that supervises the activities of a number of
subsystems (operative agents) engaged in the solution of a problem can improve
the problem-solving efficiency of the system. The problem is to find the global
maximum of a NK fitness landscape and the performance is measured by the time
required to find that maximum. The operative agents explore blindly the fitness
landscape and the monitor provides them with feedback on the quality (fitness)
of the proposed solutions. This feedback is then used by the operative agents
to bias their searches towards the fittest regions of the landscape. We find
that a weak feedback between the monitor and the operative agents improves the
performance of the system, regardless of the difficulty of the problem, which
is gauged by the number of local maxima in the landscape. For easy problems
(i.e., landscapes without local maxima), the performance improves monotonically
as the feedback strength increases, but for difficult problems, there is an
optimal value of the feedback strength beyond which the system performance
degrades very rapidly.",Awareness improves problem-solving performance,2017
"Katsunari Shibata, Yuki Goto",AI,2017,"Expectation for the emergence of higher functions is getting larger in the
framework of end-to-end reinforcement learning using a recurrent neural
network. However, the emergence of ""thinking"" that is a typical higher function
is difficult to realize because ""thinking"" needs non fixed-point, flow-type
attractors with both convergence and transition dynamics. Furthermore, in order
to introduce ""inspiration"" or ""discovery"" in ""thinking"", not completely random
but unexpected transition should be also required.
  By analogy to ""chaotic itinerancy"", we have hypothesized that ""exploration""
grows into ""thinking"" through learning by forming flow-type attractors on
chaotic random-like dynamics. It is expected that if rational dynamics are
learned in a chaotic neural network (ChNN), coexistence of rational state
transition, inspiration-like state transition and also random-like exploration
for unknown situation can be realized.
  Based on the above idea, we have proposed new reinforcement learning using a
ChNN as an actor. The positioning of exploration is completely different from
the conventional one. The chaotic dynamics inside the ChNN produces exploration
factors by itself. Since external random numbers for stochastic action
selection are not used, exploration factors cannot be isolated from the output.
Therefore, the learning method is also completely different from the
conventional one.
  At each non-feedback connection, one variable named causality trace takes in
and maintains the input through the connection according to the change in its
output. Using the trace and TD error, the weight is updated.
  In this paper, as the result of a recent simple task to see whether the new
learning works or not, it is shown that a robot with two wheels and two visual
sensors reaches a target while avoiding an obstacle after learning though there
are still many rooms for improvement.","New Reinforcement Learning Using a Chaotic Neural Network for Emergence of ""Thinking"" - ""Exploration"" Grows into ""Thinking"" through Learning -",2017
"Garvita Bajaj, Rachit Agarwal, Pushpendra Singh, Nikolaos Georgantas, Valerie Issarny",AI,2017,"Several domains have adopted the increasing use of IoT-based devices to
collect sensor data for generating abstractions and perceptions of the real
world. This sensor data is multi-modal and heterogeneous in nature. This
heterogeneity induces interoperability issues while developing cross-domain
applications, thereby restricting the possibility of reusing sensor data to
develop new applications. As a solution to this, semantic approaches have been
proposed in the literature to tackle problems related to interoperability of
sensor data. Several ontologies have been proposed to handle different aspects
of IoT-based sensor data collection, ranging from discovering the IoT sensors
for data collection to applying reasoning on the collected sensor data for
drawing inferences. In this paper, we survey these existing semantic ontologies
to provide an overview of the recent developments in this field. We highlight
the fundamental ontological concepts (e.g., sensor-capabilities and
context-awareness) required for an IoT-based application, and survey the
existing ontologies which include these concepts. Based on our study, we also
identify the shortcomings of currently available ontologies, which serves as a
stepping stone to state the need for a common unified ontology for the IoT
domain.",A study of existing Ontologies in the IoT-domain,2017
"Zhuo Chen, Elmer Salazar, Kyle Marple, Gopal Gupta, Lakshman Tamil, Sandeep Das, Alpesh Amin",AI,2017,"Management of chronic diseases such as heart failure (HF) is a major public
health problem. A standard approach to managing chronic diseases by medical
community is to have a committee of experts develop guidelines that all
physicians should follow. Due to their complexity, these guidelines are
difficult to implement and are adopted slowly by the medical community at
large. We have developed a physician advisory system that codes the entire set
of clinical practice guidelines for managing HF using answer set
programming(ASP). In this paper we show how abductive reasoning can be deployed
to find missing symptoms and conditions that the patient must exhibit in order
for a treatment prescribed by a physician to work effectively. Thus, if a
physician does not make an appropriate recommendation or makes a non-adherent
recommendation, our system will advise the physician about symptoms and
conditions that must be in effect for that recommendation to apply. It is under
consideration for acceptance in TPLP.",Improving Adherence to Heart Failure Management Guidelines via Abductive Reasoning,2017
"Irene Teinemaa, Marlon Dumas, Marcello La Rosa, Fabrizio Maria Maggi",AI,2017,"Predictive business process monitoring refers to the act of making
predictions about the future state of ongoing cases of a business process,
based on their incomplete execution traces and logs of historical (completed)
traces. Motivated by the increasingly pervasive availability of fine-grained
event data about business process executions, the problem of predictive process
monitoring has received substantial attention in the past years. In particular,
a considerable number of methods have been put forward to address the problem
of outcome-oriented predictive process monitoring, which refers to classifying
each ongoing case of a process according to a given set of possible categorical
outcomes - e.g., Will the customer complain or not? Will an order be delivered,
canceled or withdrawn? Unfortunately, different authors have used different
datasets, experimental settings, evaluation measures and baselines to assess
their proposals, resulting in poor comparability and an unclear picture of the
relative merits and applicability of different methods. To address this gap,
this article presents a systematic review and taxonomy of outcome-oriented
predictive process monitoring methods, and a comparative experimental
evaluation of eleven representative methods using a benchmark covering 24
predictive process monitoring tasks based on nine real-life event logs.",Outcome-Oriented Predictive Process Monitoring: Review and Benchmark,2018
"Krishnendu Chatterjee, Martin Chmelik, Ufuk Topcu",AI,2017,"Partially observable Markov decision processes (POMDPs) are widely used in
probabilistic planning problems in which an agent interacts with an environment
using noisy and imprecise sensors. We study a setting in which the sensors are
only partially defined and the goal is to synthesize ""weakest"" additional
sensors, such that in the resulting POMDP, there is a small-memory policy for
the agent that almost-surely (with probability~1) satisfies a reachability
objective. We show that the problem is NP-complete, and present a symbolic
algorithm by encoding the problem into SAT instances. We illustrate trade-offs
between the amount of memory of the policy and the number of additional sensors
on a simple example. We have implemented our approach and consider three
classical POMDP examples from the literature, and show that in all the examples
the number of sensors can be significantly decreased (as compared to the
existing solutions in the literature) without increasing the complexity of the
policies.",Sensor Synthesis for POMDPs with Reachability Objectives,2017
"Stefano Bromuri, Albert Brugues de la Torre, Fabien Duboisson, Michael Schumacher",AI,2017,"Personal Health Systems (PHS) are mobile solutions tailored to monitoring
patients affected by chronic non communicable diseases. A patient affected by a
chronic disease can generate large amounts of events. Type 1 Diabetic patients
generate several glucose events per day, ranging from at least 6 events per day
(under normal monitoring) to 288 per day when wearing a continuous glucose
monitor (CGM) that samples the blood every 5 minutes for several days. This is
a large number of events to monitor for medical doctors, in particular when
considering that they may have to take decisions concerning adjusting the
treatment, which may impact the life of the patients for a long time. Given the
need to analyse such a large stream of data, doctors need a simple approach
towards physiological time series that allows them to promptly transfer their
knowledge into queries to identify interesting patterns in the data. Achieving
this with current technology is not an easy task, as on one hand it cannot be
expected that medical doctors have the technical knowledge to query databases
and on the other hand these time series include thousands of events, which
requires to re-think the way data is indexed. In order to tackle the knowledge
representation and efficiency problem, this contribution presents the kd-tree
cached event calculus (\ceckd) an event calculus extension for knowledge
engineering of temporal rules capable to handle many thousands events produced
by a diabetic patient. \ceckd\ is built as a support to a graphical interface
to represent monitoring rules for diabetes type 1. In addition, the paper
evaluates the \ceckd\ with respect to the cached event calculus (CEC) to show
how indexing events using kd-trees improves scalability with respect to the
current state of the art.",Indexing the Event Calculus with Kd-trees to Monitor Diabetes,2017
"Luca Buoncompagni, Barbara Bruno, Antonella Giuni, Fulvio Mastrogiovanni, Renato Zaccaria",AI,2017,"Providing elderly and people with special needs, including those suffering
from physical disabilities and chronic diseases, with the possibility of
retaining their independence at best is one of the most important challenges
our society is expected to face. Assistance models based on the home care
paradigm are being adopted rapidly in almost all industrialized and emerging
countries. Such paradigms hypothesize that it is necessary to ensure that the
so-called Activities of Daily Living are correctly and regularly performed by
the assisted person to increase the perception of an improved quality of life.
This chapter describes the computational inference engine at the core of
Arianna, a system able to understand whether an assisted person performs a
given set of ADL and to motivate him/her in performing them through a
speech-mediated motivational dialogue, using a set of nearables to be installed
in an apartment, plus a wearable to be worn or fit in garments.","Towards a new paradigm for assistive technology at home: research challenges, design issues and performance assessment",2017
"Lisa Chalaguine, Emmanuel Hadoux, Fiona Hamilton, Andrew Hayward, Anthony Hunter, Sylwia Polberg, Henry W. W. Potts",AI,2018,"The aim of behaviour change is to help people to change aspects of their
behaviour for the better (e.g., to decrease calorie intake, to drink in
moderation, to take more exercise, to complete a course of antibiotics once
started, etc.). In current persuasion technology for behaviour change, the
emphasis is on helping people to explore their issues (e.g., through
questionnaires or game playing) or to remember to follow a behaviour change
plan (e.g., diaries and email reminders). However, recent developments in
computational persuasion are leading to an argument-centric approach to
persuasion that can potentially be harnessed in behaviour change applications.
In this paper, we review developments in computational persuasion, and then
focus on domain modelling as a key component. We present a multi-dimensional
approach to domain modelling. At the core of this proposal is an ontology which
provides a representation of key factors, in particular kinds of belief, which
we have identified in the behaviour change literature as being important in
diverse behaviour change initiatives. Our proposal for domain modelling is
intended to facilitate the acquisition and representation of the arguments that
can be used in persuasion dialogues, together with meta-level information about
them which can be used by the persuader to make strategic choices of argument
to present.",Domain Modelling in Computational Persuasion for Behaviour Change in Healthcare,2018
Amit Kumar Mishra,AI,2018,"Neuroethology has been an active field of study for more than a century now.
Out of some of the most interesting species that has been studied so far,
weakly electric fish is a fascinating one. It performs communication,
echo-location and inter-species detection efficiently with an interesting
configuration of sensors, neu-rons and a simple brain. In this paper we propose
a cognitive architecture inspired by the way these fishes handle and process
information. We believe that it is eas-ier to understand and mimic the neural
architectures of a simpler species than that of human. Hence, the proposed
architecture is expected to both help research in cognitive robotics and also
help understand more complicated brains like that of human beings.",WEBCA: Weakly-Electric-Fish Bioinspired Cognitive Architecture,2018
"Oshani Seneviratne, Sabbir M. Rashid, Shruthi Chari, James P. McCusker, Kristin P. Bennett, James A. Hendler, Deborah L. McGuinness",AI,2018,"With the rapid advancements in cancer research, the information that is
useful for characterizing disease, staging tumors, and creating treatment and
survivorship plans has been changing at a pace that creates challenges when
physicians try to remain current. One example involves increasing usage of
biomarkers when characterizing the pathologic prognostic stage of a breast
tumor. We present our semantic technology approach to support cancer
characterization and demonstrate it in our end-to-end prototype system that
collects the newest breast cancer staging criteria from authoritative oncology
manuals to construct an ontology for breast cancer. Using a tool we developed
that utilizes this ontology, physician-facing applications can be used to
quickly stage a new patient to support identifying risks, treatment options,
and monitoring plans based on authoritative and best practice guidelines.
Physicians can also re-stage existing patients or patient populations, allowing
them to find patients whose stage has changed in a given patient cohort. As new
guidelines emerge, using our proposed mechanism, which is grounded by semantic
technologies for ingesting new data from staging manuals, we have created an
enriched cancer staging ontology that integrates relevant data from several
sources with very little human intervention.",Knowledge Integration for Disease Characterization: A Breast Cancer Example,2018
"Maarten Bieshaar, Günther Reitberger, Stefan Zernetsch, Bernhard Sick, Erich Fuchs, Konrad Doll",AI,2018,"Vulnerable road users (VRUs, i.e. cyclists and pedestrians) will play an
important role in future traffic. To avoid accidents and achieve a highly
efficient traffic flow, it is important to detect VRUs and to predict their
intentions. In this article a holistic approach for detecting intentions of
VRUs by cooperative methods is presented. The intention detection consists of
basic movement primitive prediction, e.g. standing, moving, turning, and a
forecast of the future trajectory. Vehicles equipped with sensors, data
processing systems and communication abilities, referred to as intelligent
vehicles, acquire and maintain a local model of their surrounding traffic
environment, e.g. crossing cyclists. Heterogeneous, open sets of agents
(cooperating and interacting vehicles, infrastructure, e.g. cameras and laser
scanners, and VRUs equipped with smart devices and body-worn sensors) exchange
information forming a multi-modal sensor system with the goal to reliably and
robustly detect VRUs and their intentions under consideration of real time
requirements and uncertainties. The resulting model allows to extend the
perceptual horizon of the individual agent beyond their own sensory
capabilities, enabling a longer forecast horizon. Concealments,
implausibilities and inconsistencies are resolved by the collective
intelligence of cooperating agents. Novel techniques of signal processing and
modelling in combination with analytical and learning based approaches of
pattern and activity recognition are used for detection, as well as intention
prediction of VRUs. Cooperation, by means of probabilistic sensor and knowledge
fusion, takes place on the level of perception and intention recognition. Based
on the requirements of the cooperative approach for the communication a new
strategy for an ad hoc network is proposed.",Detecting Intentions of Vulnerable Road Users Based on Collective Intelligence,2018
John KC Kingston,AI,2018,"The General Data Protection Regulation (GDPR) is a European Union regulation
that will replace the existing Data Protection Directive on 25 May 2018. The
most significant change is a huge increase in the maximum fine that can be
levied for breaches of the regulation. Yet fewer than half of UK companies are
fully aware of GDPR - and a number of those who were preparing for it stopped
doing so when the Brexit vote was announced. A last-minute rush to become
compliant is therefore expected, and numerous companies are starting to offer
advice, checklists and consultancy on how to comply with GDPR. In such an
environment, artificial intelligence technologies ought to be able to assist by
providing best advice; asking all and only the relevant questions; monitoring
activities; and carrying out assessments. The paper considers four areas of
GDPR compliance where rule based technologies and/or machine learning
techniques may be relevant: * Following compliance checklists and codes of
conduct; * Supporting risk assessments; * Complying with the new regulations
regarding technologies that perform automatic profiling; * Complying with the
new regulations concerning recognising and reporting breaches of security. It
concludes that AI technology can support each of these four areas. The
requirements that GDPR (or organisations that need to comply with GDPR) state
for explanation and justification of reasoning imply that rule-based approaches
are likely to be more helpful than machine learning approaches. However, there
may be good business reasons to take a different approach in some
circumstances.",Using Artificial Intelligence to Support Compliance with the General Data Protection Regulation,2018
"Syed Yusha Kareem, Luca Buoncompagni, Fulvio Mastrogiovanni",AI,2018,"Aging population ratios are rising significantly. Meanwhile, smart home based
health monitoring services are evolving rapidly to become a viable alternative
to traditional healthcare solutions. Such services can augment qualitative
analyses done by gerontologists with quantitative data. Hence, the recognition
of Activities of Daily Living (ADL) has become an active domain of research in
recent times. For a system to perform human activity recognition in a
real-world environment, multiple requirements exist, such as scalability,
robustness, ability to deal with uncertainty (e.g., missing sensor data), to
operate with multi-occupants and to take into account their privacy and
security. This paper attempts to address the requirements of scalability and
robustness, by describing a reasoning mechanism based on modular spatial and/or
temporal context models as a network of ontologies. The reasoning mechanism has
been implemented in a smart home system referred to as Arianna+. The paper
presents and discusses a use case, and experiments are performed on a simulated
dataset, to showcase Arianna+'s modularity feature, internal working, and
computational performance. Results indicate scalability and robustness for
human activity recognition processes.",Arianna+: Scalable Human Activity Recognition by Reasoning with a Network of Ontologies,2018
"Manolis Pitsikalis, Alexander Artikis, Richard Dreo, Cyril Ray, Elena Camossi, Anne-Laure Jousselme",AI,2019,"Maritime monitoring systems support safe shipping as they allow for the
real-time detection of dangerous, suspicious and illegal vessel activities. We
present such a system using the Run-Time Event Calculus, a composite event
recognition system with formal, declarative semantics. For effective
recognition, we developed a library of maritime patterns in close collaboration
with domain experts. We present a thorough evaluation of the system and the
patterns both in terms of predictive accuracy and computational efficiency,
using real-world datasets of vessel position streams and contextual
geographical information.",Composite Event Recognition for Maritime Monitoring,2019
"Luciano Serafini, Paolo Traverso",AI,2019,"We propose a framework for learning discrete deterministic planning domains.
In this framework, an agent learns the domain by observing the action effects
through continuous features that describe the state of the environment after
the execution of each action. Besides, the agent learns its perception
function, i.e., a probabilistic mapping between state variables and sensor data
represented as a vector of continuous random variables called perception
variables. We define an algorithm that updates the planning domain and the
perception function by (i) introducing new states, either by extending the
possible values of state variables, or by weakening their constraints; (ii)
adapts the perception function to fit the observed data (iii) adapts the
transition function on the basis of the executed actions and the effects
observed via the perception function. The framework is able to deal with
exogenous events that happen in the environment.",Incremental Learning of Discrete Planning Domains from Continuous Perceptions,2019
"Yaniv Altshuler, Alex Pentland, Shlomo Bekhor, Yoram Shiftan, Alfred Bruckstein",AI,2016,"Current state of the art in the field of UAV activation relies solely on
human operators for the design and adaptation of the drones' flying routes.
Furthermore, this is being done today on an individual level (one vehicle per
operators), with some exceptions of a handful of new systems, that are
comprised of a small number of self-organizing swarms, manually guided by a
human operator.
  Drones-based monitoring is of great importance in variety of civilian
domains, such as road safety, homeland security, and even environmental
control. In its military aspect, efficiently detecting evading targets by a
fleet of unmanned drones has an ever increasing impact on the ability of modern
armies to engage in warfare. The latter is true both traditional symmetric
conflicts among armies as well as asymmetric ones. Be it a speeding driver, a
polluting trailer or a covert convoy, the basic challenge remains the same --
how can its detection probability be maximized using as little number of drones
as possible.
  In this work we propose a novel approach for the optimization of large scale
swarms of reconnaissance drones -- capable of producing on-demand optimal
coverage strategies for any given search scenario. Given an estimation cost of
the threat's potential damages, as well as types of monitoring drones available
and their comparative performance, our proposed method generates an
analytically provable strategy, stating the optimal number and types of drones
to be deployed, in order to cost-efficiently monitor a pre-defined region for
targets maneuvering using a given roads networks.
  We demonstrate our model using a unique dataset of the Israeli transportation
network, on which different deployment schemes for drones deployment are
evaluated.",Optimal Dynamic Coverage Infrastructure for Large-Scale Fleets of Reconnaissance UAVs,2016
"Oliver Bent, Sekou L. Remy, Stephen Roberts, Aisha Walcott-Bryant",AI,2017,"The task of decision-making under uncertainty is daunting, especially for
problems which have significant complexity. Healthcare policy makers across the
globe are facing problems under challenging constraints, with limited tools to
help them make data driven decisions. In this work we frame the process of
finding an optimal malaria policy as a stochastic multi-armed bandit problem,
and implement three agent based strategies to explore the policy space. We
apply a Gaussian Process regression to the findings of each agent, both for
comparison and to account for stochastic results from simulating the spread of
malaria in a fixed population. The generated policy spaces are compared with
published results to give a direct reference with human expert decisions for
the same simulated population. Our novel approach provides a powerful resource
for policy makers, and a platform which can be readily extended to capture
future more nuanced policy spaces.",Novel Exploration Techniques (NETs) for Malaria Policy Interventions,2017
"Tomoaki Nakamura, Takayuki Nagai, Tadahiro Taniguchi",AI,2017,"To realize human-like robot intelligence, a large-scale cognitive
architecture is required for robots to understand the environment through a
variety of sensors with which they are equipped. In this paper, we propose a
novel framework named Serket that enables the construction of a large-scale
generative model and its inference easily by connecting sub-modules to allow
the robots to acquire various capabilities through interaction with their
environments and others. We consider that large-scale cognitive models can be
constructed by connecting smaller fundamental models hierarchically while
maintaining their programmatic independence. Moreover, connected modules are
dependent on each other, and parameters are required to be optimized as a
whole. Conventionally, the equations for parameter estimation have to be
derived and implemented depending on the models. However, it becomes harder to
derive and implement those of a larger scale model. To solve these problems, in
this paper, we propose a method for parameter estimation by communicating the
minimal parameters between various modules while maintaining their programmatic
independence. Therefore, Serket makes it easy to construct large-scale models
and estimate their parameters via the connection of modules. Experimental
results demonstrated that the model can be constructed by connecting modules,
the parameters can be optimized as a whole, and they are comparable with the
original models that we have proposed.",SERKET: An Architecture for Connecting Stochastic Models to Realize a Large-Scale Cognitive Model,2017
"Nathalia Moraes do Nascimento, Carlos Jose Pereira de Lucena",AI,2018,"The goal of the Internet of Things (IoT) is to transform any thing around us,
such as a trash can or a street light, into a smart thing. A smart thing has
the ability of sensing, processing, communicating and/or actuating. In order to
achieve the goal of a smart IoT application, such as minimizing waste
transportation costs or reducing energy consumption, the smart things in the
application scenario must cooperate with each other without a centralized
control. Inspired by known approaches to design swarm of cooperative and
autonomous robots, we modeled our smart things based on the embodied cognition
concept. Each smart thing is a physical agent with a body composed of a
microcontroller, sensors and actuators, and a brain that is represented by an
artificial neural network. This type of agent is commonly called an embodied
agent. The behavior of these embodied agents is autonomously configured through
an evolutionary algorithm that is triggered according to the application
performance. To illustrate, we have designed three homogeneous prototypes for
smart street lights based on an evolved network. This application has shown
that the proposed approach results in a feasible way of modeling decentralized
smart things with self-developed and cooperative capabilities.",Engineering Cooperative Smart Things based on Embodied Cognition,2018
"Hussein Abbass, John Harvey, Kate Yaxley",AI,2018,"Artificial Intelligence (AI) technologies could be broadly categorised into
Analytics and Autonomy. Analytics focuses on algorithms offering perception,
comprehension, and projection of knowledge gleaned from sensorial data.
Autonomy revolves around decision making, and influencing and shaping the
environment through action production. A smart autonomous system (SAS) combines
analytics and autonomy to understand, learn, decide and act autonomously. To be
useful, SAS must be trusted and that requires testing. Lifelong learning of a
SAS compounds the testing process. In the remote chance that it is possible to
fully test and certify the system pre-release, which is theoretically an
undecidable problem, it is near impossible to predict the future behaviours
that these systems, alone or collectively, will exhibit. While it may be
feasible to severely restrict such systems\textquoteright \ learning abilities
to limit the potential unpredictability of their behaviours, an undesirable
consequence may be severely limiting their utility. In this paper, we propose
the architecture for a watchdog AI (WAI) agent dedicated to lifelong functional
testing of SAS. We further propose system specifications including a level of
abstraction whereby humans shepherd a swarm of WAI agents to oversee an
ecosystem made of humans and SAS. The discussion extends to the challenges,
pros, and cons of the proposed concept.",Lifelong Testing of Smart Autonomous Systems by Shepherding a Swarm of Watchdog Artificial Intelligence Agents,2018
"Ajaya Adhikari, D. M. J Tax, Riccardo Satta, Matthias Fath",AI,2018,"As machine learning models become more accurate, they typically become more
complex and uninterpretable by humans. The black-box character of these models
holds back its acceptance in practice, especially in high-risk domains where
the consequences of failure could be catastrophic such as health-care or
defense. Providing understandable and useful explanations behind ML models or
predictions can increase the trust of the user. Example-based reasoning, which
entails leveraging previous experience with analogous tasks to make a decision,
is a well known strategy for problem solving and justification. This work
presents a new explanation extraction method called LEAFAGE, for a prediction
made by any black-box ML model. The explanation consists of the visualization
of similar examples from the training set and the importance of each feature.
Moreover, these explanations are contrastive which aims to take the
expectations of the user into account. LEAFAGE is evaluated in terms of
fidelity to the underlying black-box model and usefulness to the user. The
results showed that LEAFAGE performs overall better than the current
state-of-the-art method LIME in terms of fidelity, on ML models with non-linear
decision boundary. A user-study was conducted which focused on revealing the
differences between example-based and feature importance-based explanations. It
showed that example-based explanations performed significantly better than
feature importance-based explanation, in terms of perceived transparency,
information sufficiency, competence and confidence. Counter-intuitively, when
the gained knowledge of the participants was tested, it showed that they
learned less about the black-box model after seeing a feature importance-based
explanation than seeing no explanation at all. The participants found feature
importance-based explanation vague and hard to generalize it to other
instances.",LEAFAGE: Example-based and Feature importance-based Explanationsfor Black-box ML models,2019
"Himabindu Lakkaraju, Osbert Bastani",AI,2019,"As machine learning black boxes are increasingly being deployed in critical
domains such as healthcare and criminal justice, there has been a growing
emphasis on developing techniques for explaining these black boxes in a human
interpretable manner. It has recently become apparent that a high-fidelity
explanation of a black box ML model may not accurately reflect the biases in
the black box. As a consequence, explanations have the potential to mislead
human users into trusting a problematic black box. In this work, we rigorously
explore the notion of misleading explanations and how they influence user trust
in black-box models. More specifically, we propose a novel theoretical
framework for understanding and generating misleading explanations, and carry
out a user study with domain experts to demonstrate how these explanations can
be used to mislead users. Our work is the first to empirically establish how
user trust in black box models can be manipulated via misleading explanations.","""How do I fool you?"": Manipulating User Trust via Misleading Black Box Explanations",2019
"Paolo Pareti, George Konstantinidis, Timothy J. Norman",AI,2019,"An important use of sensors and actuator networks is to comply with health
and safety policies in hazardous environments. In order to deal with
increasingly large and dynamic environments, and to quickly react to
emergencies, tools are needed to simplify the process of translating high-level
policies into executable queries and rules. We present a framework to produce
such tools, which uses rules to aggregate low-level sensor data, described
using the Semantic Sensor Network Ontology, into more useful and actionable
abstractions. Using the schema of the underlying data sources as an input, we
automatically generate abstractions which are relevant to the use case at hand.
In this demonstration we present a policy editor tool and a simulation on which
policies can be tested.",A Policy Editor for Semantic Sensor Networks,2019
"Marcello Balduccini, William C. Regli, Duc N. Nguyen",AI,2014,"Traditional AI reasoning techniques have been used successfully in many
domains, including logistics, scheduling and game playing. This paper is part
of a project aimed at investigating how such techniques can be extended to
coordinate teams of unmanned aerial vehicles (UAVs) in dynamic environments.
Specifically challenging are real-world environments where UAVs and other
network-enabled devices must communicate to coordinate---and communication
actions are neither reliable nor free. Such network-centric environments are
common in military, public safety and commercial applications, yet most
research (even multi-agent planning) usually takes communications among
distributed agents as a given. We address this challenge by developing an agent
architecture and reasoning algorithms based on Answer Set Programming (ASP).
ASP has been chosen for this task because it enables high flexibility of
representation, both of knowledge and of reasoning tasks. Although ASP has been
used successfully in a number of applications, and ASP-based architectures have
been studied for about a decade, to the best of our knowledge this is the first
practical application of a complete ASP-based agent architecture. It is also
the first practical application of ASP involving a combination of centralized
reasoning, decentralized reasoning, execution monitoring, and reasoning about
network communications. This work has been empirically validated using a
distributed network-centric software evaluation testbed and the results provide
guidance to designers in how to understand and control intelligent systems that
operate in these environments.",An ASP-Based Architecture for Autonomous UAVs in Dynamic Environments: Progress Report,2014
"Ilias Tachmazidis, Grigoris Antoniou, Wolfgang Faber",AI,2014,"Data originating from the Web, sensor readings and social media result in
increasingly huge datasets. The so called Big Data comes with new scientific
and technological challenges while creating new opportunities, hence the
increasing interest in academia and industry. Traditionally, logic programming
has focused on complex knowledge structures/programs, so the question arises
whether and how it can work in the face of Big Data. In this paper, we examine
how the well-founded semantics can process huge amounts of data through mass
parallelization. More specifically, we propose and evaluate a parallel approach
using the MapReduce framework. Our experimental results indicate that our
approach is scalable and that well-founded semantics can be applied to billions
of facts. To the best of our knowledge, this is the first work that addresses
large scale nonmonotonic reasoning without the restriction of stratification
for predicates of arbitrary arity. To appear in Theory and Practice of Logic
Programming (TPLP).",Efficient Computation of the Well-Founded Semantics over Big Data,2014
"M. Ehsan Shafiee, Emily M. Zechman",AI,2014,"In the event that a bacteriological or chemical toxin is intro- duced to a
water distribution network, a large population of consumers may become exposed
to the contaminant. A contamination event may be poorly predictable dynamic
process due to the interactions of consumers and utility managers during an
event. Consumers that become aware of a threat may select protective actions
that change their water demands from typical demand patterns, and new hydraulic
conditions can arise that differ from conditions that are predicted when
demands are considered as exogenous inputs. Consequently, the movement of the
contaminant plume in the pipe network may shift from its expected trajectory. A
sociotechnical model is developed here to integrate agent-based models of
consumers with an engineering water distribution system model and capture the
dynamics between consumer behaviors and the water distribution system for
predicting contaminant transport and public exposure. Consumers are simulated
as agents with behaviors defined for water use activities, mobility,
word-of-mouth communication, and demand reduction, based on a set of rules
representing an agents autonomy and reaction to health impacts, the
environment, and the actions of other agents. As consumers decrease their water
use, the demand exerted on the water distribution system is updated; as the
flow directions and volumes shift in response, the location of the contaminant
plume is updated and the amount of contaminant consumed by each agent is
calculated. The framework is tested through simulating realistic contamination
scenarios for a virtual city and water distribution system.",An Agent-based Modeling Framework for Sociotechnical Simulation of Water Distribution Contamination Events,2014
J Gerard Wolff,AI,2016,"The ""SP theory of intelligence"", with its realisation in the ""SP computer
model"", aims to simplify and integrate observations and concepts across
AI-related fields, with information compression as a unifying theme. This paper
describes how abstract structures and processes in the theory may be realised
in terms of neurons, their interconnections, and the transmission of signals
between neurons. This part of the SP theory -- ""SP-neural"" -- is a tentative
and partial model for the representation and processing of knowledge in the
brain. In the SP theory (apart from SP-neural), all kinds of knowledge are
represented with ""patterns"", where a pattern is an array of atomic symbols in
one or two dimensions. In SP-neural, the concept of a ""pattern"" is realised as
an array of neurons called a ""pattern assembly"", similar to Hebb's concept of a
""cell assembly"" but with important differences. Central to the processing of
information in the SP system is the powerful concept of ""multiple alignment"",
borrowed and adapted from bioinformatics. Processes such as pattern
recognition, reasoning and problem solving are achieved via the building of
multiple alignments, while unsupervised learning -- significantly different
from the ""Hebbian"" kinds of learning -- is achieved by creating patterns from
sensory information and also by creating patterns from multiple alignments in
which there is a partial match between one pattern and another. Short-lived
neural structures equivalent to multiple alignments will be created via an
inter-play of excitatory and inhibitory neural signals. The paper discusses
several associated issues, with relevant empirical evidence.",The SP theory of intelligence and the representation and processing of knowledge in the brain,2016
Katsunari Shibata,AI,2017,"Recently, triggered by the impressive results in TV-games or game of Go by
Google DeepMind, end-to-end reinforcement learning (RL) is collecting
attentions. Although little is known, the author's group has propounded this
framework for around 20 years and already has shown various functions that
emerge in a neural network (NN) through RL. In this paper, they are introduced
again at this timing.
  ""Function Modularization"" approach is deeply penetrated subconsciously. The
inputs and outputs for a learning system can be raw sensor signals and motor
commands. ""State space"" or ""action space"" generally used in RL show the
existence of functional modules. That has limited reinforcement learning to
learning only for the action-planning module. In order to extend reinforcement
learning to learning of the entire function on a huge degree of freedom of a
massively parallel learning system and to explain or develop human-like
intelligence, the author has believed that end-to-end RL from sensors to motors
using a recurrent NN (RNN) becomes an essential key. Especially in the higher
functions, this approach is very effective by being free from the need to
decide their inputs and outputs.
  The functions that emerge, we have confirmed, through RL using a NN cover a
broad range from real robot learning with raw camera pixel inputs to
acquisition of dynamic functions in a RNN. Those are (1)image recognition,
(2)color constancy (optical illusion), (3)sensor motion (active recognition),
(4)hand-eye coordination and hand reaching movement, (5)explanation of brain
activities, (6)communication, (7)knowledge transfer, (8)memory, (9)selective
attention, (10)prediction, (11)exploration. The end-to-end RL enables the
emergence of very flexible comprehensive functions that consider many things in
parallel although it is difficult to give the boundary of each function
clearly.",Functions that Emerge through End-to-End Reinforcement Learning - The Direction for Artificial General Intelligence -,2017
Katsunari Shibata,AI,2017,"Communication is not only an action of choosing a signal, but needs to
consider the context and sensor signals. It also needs to decide what
information is communicated and how it is represented in or understood from
signals. Therefore, communication should be realized comprehensively together
with its purpose and other functions.
  The recent successful results in end-to-end reinforcement learning (RL) show
the importance of comprehensive learning and the usefulness of end-to-end RL.
Although little is known, we have shown that a variety of communications emerge
through RL using a (recurrent) neural network (NN). Here, three of them are
introduced.
  In the 1st one, negotiation to avoid conflicts among 4 randomly-picked agents
was learned. Each agent generates a binary signal from the output of its
recurrent NN (RNN), and receives 4 signals from the agents three times. After
learning, each agent made an appropriate final decision after negotiation for
any combination of 4 agents. Differentiation of individuality among the agents
also could be seen.
  The 2nd one focused on discretization of communication signal. A sender agent
perceives the receiver's location and generates a continuous signal twice by
its RNN. A receiver agent receives them sequentially, and moves according to
its RNN's output to reach the sender's location. When noises were added to the
signal, it was binarized through learning and 2-bit communication was
established.
  The 3rd one focused on end-to-end comprehensive communication. A sender
receives 1,785-pixel real camera image on which a real robot can be seen, and
sends two sounds whose frequencies are computed by its NN. A receiver receives
them, and two motion commands for the robot are generated by its NN. After
learning, though some preliminary learning was necessary for the sender, the
robot could reach the goal from any initial location.",Communications that Emerge through Reinforcement Learning Using a (Recurrent) Neural Network,2017
Thanuka Wickramarathne,AI,2017,"Robust belief revision methods are crucial in streaming data situations for
updating existing knowledge or beliefs with new incoming evidence. Bayes
conditioning is the primary mechanism in use for belief revision in data fusion
systems that use probabilistic inference. However, traditional conditioning
methods face several challenges due to inherent data/source imperfections in
big-data environments that harness soft (i.e., human or human-based) sources in
addition to hard (i.e., physics-based) sensors. The objective of this paper is
to investigate the most natural extension of Bayes conditioning that is
suitable for evidence updating in the presence of such uncertainties. By
viewing the evidence updating process as a thought experiment, an elegant
strategy is derived for robust evidence updating in the presence of extreme
uncertainties that are characteristic of big-data environments. In particular,
utilizing the Fagin-Halpern conditional notions, a natural extension to Bayes
conditioning is derived for evidence that takes the form of a general belief
function. The presented work differs fundamentally from the Conditional Update
Equation (CUE) and authors own extensions of it. An overview of this
development is provided via illustrative examples. Furthermore, insights into
parameter selection under various fusion contexts are also provided.",Evidence Updating for Stream-Processing in Big-Data: Robust Conditioning in Soft and Hard Fusion Environments,2017
"Shayegan Omidshafiei, Dong-Ki Kim, Jason Pazis, Jonathan P. How",AI,2017,"This paper presents the Crossmodal Attentive Skill Learner (CASL), integrated
with the recently-introduced Asynchronous Advantage Option-Critic (A2OC)
architecture [Harb et al., 2017] to enable hierarchical reinforcement learning
across multiple sensory inputs. We provide concrete examples where the approach
not only improves performance in a single task, but accelerates transfer to new
tasks. We demonstrate the attention mechanism anticipates and identifies useful
latent features, while filtering irrelevant sensor modalities during execution.
We modify the Arcade Learning Environment [Bellemare et al., 2013] to support
audio queries, and conduct evaluations of crossmodal learning in the Atari 2600
game Amidar. Finally, building on the recent work of Babaeizadeh et al. [2017],
we open-source a fast hybrid CPU-GPU implementation of CASL.",Crossmodal Attentive Skill Learner,2018
Biplav Srivastava,AI,2018,"Conversation interfaces (CIs), or chatbots, are a popular form of intelligent
agents that engage humans in task-oriented or informal conversation. In this
position paper and demonstration, we argue that chatbots working in dynamic
environments, like with sensor data, can not only serve as a promising platform
to research issues at the intersection of learning, reasoning, representation
and execution for goal-directed autonomy; but also handle non-trivial business
applications. We explore the underlying issues in the context of Water Advisor,
a preliminary multi-modal conversation system that can access and explain water
quality data.",On Chatbots Exhibiting Goal-Directed Autonomy in Dynamic Environments,2018
"Javier Andreu-Perez, Fani Deligianni, Daniele Ravi, Guang-Zhong Yang",AI,2018,"The recent successes of AI have captured the wildest imagination of both the
scientific communities and the general public. Robotics and AI amplify human
potentials, increase productivity and are moving from simple reasoning towards
human-like cognitive abilities. Current AI technologies are used in a set area
of applications, ranging from healthcare, manufacturing, transport, energy, to
financial services, banking, advertising, management consulting and government
agencies. The global AI market is around 260 billion USD in 2016 and it is
estimated to exceed 3 trillion by 2024. To understand the impact of AI, it is
important to draw lessons from it's past successes and failures and this white
paper provides a comprehensive explanation of the evolution of AI, its current
status and future directions.",Artificial Intelligence and Robotics,2018
"Emilio Cartoni, Gianluca Baldassarre",AI,2018,"A parameterized skill is a mapping from multiple goals/task parameters to the
policy parameters to accomplish them. Existing works in the literature show how
a parameterized skill can be learned given a task space that defines all the
possible achievable goals. In this work, we focus on tasks defined in terms of
final states (goals), and we face on the challenge where the agent aims to
autonomously acquire a parameterized skill to manipulate an initially unknown
environment. In this case, the task space is not known a priori and the agent
has to autonomously discover it. The agent may posit as a task space its whole
sensory space (i.e. the space of all possible sensor readings) as the
achievable goals will certainly be a subset of this space. However, the space
of achievable goals may be a very tiny subspace in relation to the whole
sensory space, thus directly using the sensor space as task space exposes the
agent to the curse of dimensionality and makes existing autonomous skill
acquisition algorithms inefficient. In this work we present an algorithm that
actively discovers the manifold of the achievable goals within the sensor
space. We validate the algorithm by employing it in multiple different
simulated scenarios where the agent actions achieve different types of goals:
moving a redundant arm, pushing an object, and changing the color of an object.",Autonomous discovery of the goal space to learn a parameterized skill,2018
"Krzysztof Janowicz, Armin Haller, Simon J D Cox, Danh Le Phuoc, Maxime Lefrancois",AI,2018,"The Sensor, Observation, Sample, and Actuator (SOSA) ontology provides a
formal but lightweight general-purpose specification for modeling the
interaction between the entities involved in the acts of observation,
actuation, and sampling. SOSA is the result of rethinking the W3C-XG Semantic
Sensor Network (SSN) ontology based on changes in scope and target audience,
technical developments, and lessons learned over the past years. SOSA also acts
as a replacement of SSN's Stimulus Sensor Observation (SSO) core. It has been
developed by the first joint working group of the Open Geospatial Consortium
(OGC) and the World Wide Web Consortium (W3C) on \emph{Spatial Data on the
Web}. In this work, we motivate the need for SOSA, provide an overview of the
main classes and properties, and briefly discuss its integration with the new
release of the SSN ontology as well as various other alignments to
specifications such as OGC's Observations and Measurements (O\&M),
Dolce-Ultralite (DUL), and other prominent ontologies. We will also touch upon
common modeling problems and application areas related to publishing and
searching observation, sampling, and actuation data on the Web. The SOSA
ontology and standard can be accessed at
\url{https://www.w3.org/TR/vocab-ssn/}.","SOSA: A Lightweight Ontology for Sensors, Observations, Samples, and Actuators",2018
"German I. Parisi, Jun Tani, Cornelius Weber, Stefan Wermter",AI,2018,"Artificial autonomous agents and robots interacting in complex environments
are required to continually acquire and fine-tune knowledge over sustained
periods of time. The ability to learn from continuous streams of information is
referred to as lifelong learning and represents a long-standing challenge for
neural network models due to catastrophic forgetting. Computational models of
lifelong learning typically alleviate catastrophic forgetting in experimental
scenarios with given datasets of static images and limited complexity, thereby
differing significantly from the conditions artificial agents are exposed to.
In more natural settings, sequential information may become progressively
available over time and access to previous experience may be restricted. In
this paper, we propose a dual-memory self-organizing architecture for lifelong
learning scenarios. The architecture comprises two growing recurrent networks
with the complementary tasks of learning object instances (episodic memory) and
categories (semantic memory). Both growing networks can expand in response to
novel sensory experience: the episodic memory learns fine-grained
spatiotemporal representations of object instances in an unsupervised fashion
while the semantic memory uses task-relevant signals to regulate structural
plasticity levels and develop more compact representations from episodic
experience. For the consolidation of knowledge in the absence of external
sensory input, the episodic memory periodically replays trajectories of neural
reactivations. We evaluate the proposed model on the CORe50 benchmark dataset
for continuous object recognition, showing that we significantly outperform
current methods of lifelong learning in three different incremental learning
scenarios",Lifelong Learning of Spatiotemporal Representations with Dual-Memory Recurrent Self-Organization,2018
"Luchen Li, Matthieu Komorowski, Aldo A. Faisal",AI,2018,"Off-policy reinforcement learning enables near-optimal policy from suboptimal
experience, thereby provisions opportunity for artificial intelligence
applications in healthcare. Previous works have mainly framed patient-clinician
interactions as Markov decision processes, while true physiological states are
not necessarily fully observable from clinical data. We capture this situation
with partially observable Markov decision process, in which an agent optimises
its actions in a belief represented as a distribution of patient states
inferred from individual history trajectories. A Gaussian mixture model is
fitted for the observed data. Moreover, we take into account the fact that
nuance in pharmaceutical dosage could presumably result in significantly
different effect by modelling a continuous policy through a Gaussian
approximator directly in the policy space, i.e. the actor. To address the
challenge of infinite number of possible belief states which renders exact
value iteration intractable, we evaluate and plan for only every encountered
belief, through heuristic search tree by tightly maintaining lower and upper
bounds of the true value of belief. We further resort to function
approximations to update value bounds estimation, i.e. the critic, so that the
tree search can be improved through more compact bounds at the fringe nodes
that will be back-propagated to the root. Both actor and critic parameters are
learned via gradient-based approaches. Our proposed policy trained from real
intensive care unit data is capable of dictating dosing on vasopressors and
intravenous fluids for sepsis patients that lead to the best patient outcomes.",The Actor Search Tree Critic (ASTC) for Off-Policy POMDP Learning in Medical Decision Making,2018
"Noel C. F. Codella, Michael Hind, Karthikeyan Natesan Ramamurthy, Murray Campbell, Amit Dhurandhar, Kush R. Varshney, Dennis Wei, Aleksandra Mojsilovic",AI,2018,"The adoption of machine learning in high-stakes applications such as
healthcare and law has lagged in part because predictions are not accompanied
by explanations comprehensible to the domain user, who often holds the ultimate
responsibility for decisions and outcomes. In this paper, we propose an
approach to generate such explanations in which training data is augmented to
include, in addition to features and labels, explanations elicited from domain
users. A joint model is then learned to produce both labels and explanations
from the input features. This simple idea ensures that explanations are
tailored to the complexity expectations and domain knowledge of the consumer.
Evaluation spans multiple modeling techniques on a game dataset, a (visual)
aesthetics dataset, a chemical odor dataset and a Melanoma dataset showing that
our approach is generalizable across domains and algorithms. Results
demonstrate that meaningful explanations can be reliably taught to machine
learning algorithms, and in some cases, also improve modeling accuracy.",Teaching Meaningful Explanations,2018
"Martina Garofalo, Maria Angela Pellegrino, Abdulrahman Altabba, Michael Cochez",AI,2018,"Industry is evolving towards Industry 4.0, which holds the promise of
increased flexibility in manufacturing, better quality and improved
productivity. A core actor of this growth is using sensors, which must capture
data that can used in unforeseen ways to achieve a performance not achievable
without them. However, the complexity of this improved setting is much greater
than what is currently used in practice. Hence, it is imperative that the
management cannot only be performed by human labor force, but part of that will
be done by automated algorithms instead. A natural way to represent the data
generated by this large amount of sensors, which are not acting measuring
independent variables, and the interaction of the different devices is by using
a graph data model. Then, machine learning could be used to aid the Industry
4.0 system to, for example, perform predictive maintenance. However, machine
learning directly on graphs, needs feature engineering and has scalability
issues. In this paper we discuss methods to convert (embed) the graph in a
vector space, such that it becomes feasible to use traditional machine learning
methods for Industry 4.0 settings.",Leveraging Knowledge Graph Embedding Techniques for Industry 4.0 Use Cases,2018
"Roi Ceren, Shannon Quinn, Glen Raines",AI,2019,"We propose a generalized decision-theoretic system for a heterogeneous team
of autonomous agents who are tasked with online identification of
phenotypically expressed stress in crop fields.. This system employs four
distinct types of agents, specific to four available sensor modalities:
satellites (Layer 3), uninhabited aerial vehicles (L2), uninhabited ground
vehicles (L1), and static ground-level sensors (L0). Layers 3, 2, and 1 are
tasked with performing image processing at the available resolution of the
sensor modality and, along with data generated by layer 0 sensors, identify
erroneous differences that arise over time. Our goal is to limit the use of the
more computationally and temporally expensive subsequent layers. Therefore,
from layer 3 to 1, each layer only investigates areas that previous layers have
identified as potentially afflicted by stress. We introduce a reinforcement
learning technique based on Perkins' Monte Carlo Exploring Starts for a
generalized Markovian model for each layer's decision problem, and label the
system the Agricultural Distributed Decision Framework (ADDF). As our domain is
real-world and online, we illustrate implementations of the two major
components of our system: a clustering-based image processing methodology and a
two-layer POMDP implementation.","Towards a Decentralized, Autonomous Multiagent Framework for Mitigating Crop Loss",2019
"Oleh Andriichuk, Vitaliy Tsyganok, Dmitry Lande, Oleg Chertov, Yaroslava Porplenko",AI,2019,"Application of decision support systems for conflict modeling in information
operations recognition is presented. An information operation is considered as
a complex weakly structured system. The model of conflict between two subjects
is proposed based on the second-order rank reflexive model. The method is
described for construction of the design pattern for knowledge bases of
decision support systems. In the talk, the methodology is proposed for using of
decision support systems for modeling of conflicts in information operations
recognition based on the use of expert knowledge and content monitoring.",Usage of Decision Support Systems for Conflicts Modelling during Information Operations Recognition,2019
"Ario Santoso, Michael Felderer",AI,2019,"Predictive analysis in business process monitoring aims at forecasting the
future information of a running business process. The prediction is typically
made based on the model extracted from historical process execution logs (event
logs). In practice, different business domains might require different kinds of
predictions. Hence, it is important to have a means for properly specifying the
desired prediction tasks, and a mechanism to deal with these various prediction
tasks. Although there have been many studies in this area, they mostly focus on
a specific prediction task. This work introduces a language for specifying the
desired prediction tasks, and this language allows us to express various kinds
of prediction tasks. This work also presents a mechanism for automatically
creating the corresponding prediction model based on the given specification.
Differently from previous studies, instead of focusing on a particular
prediction task, we present an approach to deal with various prediction tasks
based on the given specification of the desired prediction tasks. We also
provide an implementation of the approach which is used to conduct experiments
using real-life event logs.",Specification-Driven Predictive Business Process Monitoring,2019
"Luísa R. de A. Santos, Felipe Meneguzzi, Ramon Fraga Pereira, André Grahl Pereira",AI,2019,"Goal recognition aims to recognize the set of candidate goals that are
compatible with the observed behavior of an agent. In this paper, we develop a
method based on the operator-counting framework that efficiently computes
solutions that satisfy the observations and uses the information generated to
solve goal recognition tasks. Our method reasons explicitly about both partial
and noisy observations: estimating uncertainty for the former, and satisfying
observations given the unreliability of the sensor for the latter. We evaluate
our approach empirically over a large data set, analyzing its components on how
each can impact the quality of the solutions. In general, our approach is
superior to previous methods in terms of agreement ratio, accuracy, and spread.
Finally, our approach paves the way for new research on combinatorial
optimization to solve goal recognition tasks.",An LP-Based Approach for Goal Recognition as Planning,2021
"Mathieu Ritou, Farouk Belkadi, Zakaria Yahouni, Catherine Da Cunha, Florent Laroche, Benoit Furet",AI,2019,"In the context of Industry 4.0, data management is a key point for decision
aid approaches. Large amounts of manufacturing digital data are collected on
the shop floor. Their analysis can then require a large amount of computing
power. The Big Data issue can be solved by aggregation, generating smart and
meaningful data. This paper presents a new knowledge-based multi-level
aggregation strategy to support decision making. Manufacturing knowledge is
used at each level to design the monitoring criteria or aggregation operators.
The proposed approach has been implemented as a demonstrator and successfully
applied to a real machining database from the aeronautic industry. Decision
Making; Machining; Knowledge based system",Knowledge-based multi-level aggregation for decision aid in the machining industry,2019
"Anagha Kulkarni, Siddharth Srivastava, Subbarao Kambhampati",AI,2019,"In order to be useful in the real world, AI agents need to plan and act in
the presence of others, who may include adversarial and cooperative entities.
In this paper, we consider the problem where an autonomous agent needs to act
in a manner that clarifies its objectives to cooperative entities while
preventing adversarial entities from inferring those objectives. We show that
this problem is solvable when cooperative entities and adversarial entities use
different types of sensors and/or prior knowledge. We develop two new solution
approaches for computing such plans. One approach provides an optimal solution
to the problem by using an IP solver to provide maximum obfuscation for
adversarial entities while providing maximum legibility for cooperative
entities in the environment, whereas the other approach provides a satisficing
solution using heuristic-guided forward search to achieve preset levels of
obfuscation and legibility for adversarial and cooperative entities
respectively. We show the feasibility and utility of our algorithms through
extensive empirical evaluation on problems derived from planning benchmarks.",Signaling Friends and Head-Faking Enemies Simultaneously: Balancing Goal Obfuscation and Goal Legibility,2020
Andrew Powell,AI,2019,"This paper describes a possible way to improve computer security by
implementing a program which implements the following three features related to
a weak notion of artificial consciousness: (partial) self-monitoring, ability
to compute the truth of quantifier-free propositions and the ability to
communicate with the user. The integrity of the program could be enhanced by
using a trusted computing approach, that is to say a hardware module that is at
the root of a chain of trust. This paper outlines a possible approach but does
not refer to an implementation (which would need further work), but the author
believes that an implementation using current processors, a debugger, a
monitoring program and a trusted processing module is currently possible.",Artificial Consciousness and Security,2019
"José Luis Vilchis Medina, Pierre Siegel, Vincent Risch, Andrei Doncescu",AI,2019,"In this article we present an implementation of non-monotonic reasoning in an
embedded system. As a part of an autonomous motor-glider, it simulates piloting
decisions of an airplane. A real pilot must take care not only about the
information arising from the cockpit (airspeed, altitude, variometer,
compass...) but also from outside the cabin. Throughout a flight, a pilot is
constantly in communication with the control tower to follow orders, because
there is an airspace regulation to respect. In addition, if the control tower
sends orders while the pilot has an emergency, he may have to violate these
orders and airspace regulations to solve his problem (e.g. emergency landing).
On the other hand, climate changes constantly (wind, snow, hail..) and can
affect the sensors. All these cases easily lead to contradictions. Switching to
reasoning under uncertainty, a pilot must make decisions to carry out a flight.
The objective of this implementation is to validate a non-monotonic model which
allows to solve the question of incomplete and contradictory information. We
formalize the problem using default logic, a non-monotonic logic which allows
to find fixed-points in the face of contradictions. For the implementation, the
Prolog language is used in an embedded computer running at 1 GHz single core
with 512 Mb of RAM and 0.8 watts of energy consumption.",An Implementation of a Non-monotonic Logic in an Embedded Computer for a Motor-glider,2019
"Michael A. Skinner, Lakshmi Raman, Neel Shah, Abdelaziz Farhat, Sriraam Natarajan",AI,2020,"The increased use of electronic health records has made possible the
automated extraction of medical policies from patient records to aid in the
development of clinical decision support systems. We adapted a boosted
Statistical Relational Learning (SRL) framework to learn probabilistic rules
from clinical hospital records for the management of physiologic parameters of
children with severe cardiac or respiratory failure who were managed with
extracorporeal membrane oxygenation. In this preliminary study, the results
were promising. In particular, the algorithm returned logic rules for medical
actions that are consistent with medical reasoning.",A Preliminary Approach for Learning Relational Policies for the Management of Critically Ill Children,2020
"Evangelia Kyrimi, Scott McLachlan, Kudakwashe Dube, Mariana R. Neves, Ali Fahmi, Norman Fenton",AI,2020,"No comprehensive review of Bayesian networks (BNs) in healthcare has been
published in the past, making it difficult to organize the research
contributions in the present and identify challenges and neglected areas that
need to be addressed in the future. This unique and novel scoping review of BNs
in healthcare provides an analytical framework for comprehensively
characterizing the domain and its current state. The review shows that: (1) BNs
in healthcare are not used to their full potential; (2) a generic BN
development process is lacking; (3) limitations exists in the way BNs in
healthcare are presented in the literature, which impacts understanding,
consensus towards systematic methodologies, practice and adoption of BNs; and
(4) a gap exists between having an accurate BN and a useful BN that impacts
clinical practice. This review empowers researchers and clinicians with an
analytical framework and findings that will enable understanding of the need to
address the problems of restricted aims of BNs, ad hoc BN development methods,
and the lack of BN adoption in practice. To map the way forward, the paper
proposes future research directions and makes recommendations regarding BN
development methods and adoption in practice.","A Comprehensive Scoping Review of Bayesian Networks in Healthcare: Past, Present and Future",2020
"Pedro Zuidberg Dos Martires, Nitesh Kumar, Andreas Persson, Amy Loutfi, Luc De Raedt",AI,2020,"Robotic agents should be able to learn from sub-symbolic sensor data, and at
the same time, be able to reason about objects and communicate with humans on a
symbolic level. This raises the question of how to overcome the gap between
symbolic and sub-symbolic artificial intelligence. We propose a semantic world
modeling approach based on bottom-up object anchoring using an object-centered
representation of the world. Perceptual anchoring processes continuous
perceptual sensor data and maintains a correspondence to a symbolic
representation. We extend the definitions of anchoring to handle multi-modal
probability distributions and we couple the resulting symbol anchoring system
to a probabilistic logic reasoner for performing inference. Furthermore, we use
statistical relational learning to enable the anchoring framework to learn
symbolic knowledge in the form of a set of probabilistic logic rules of the
world from noisy and sub-symbolic sensor input. The resulting framework, which
combines perceptual anchoring and statistical relational learning, is able to
maintain a semantic world model of all the objects that have been perceived
over time, while still exploiting the expressiveness of logical rules to reason
about the state of objects which are not directly observed through sensory
input data. To validate our approach we demonstrate, on the one hand, the
ability of our system to perform probabilistic reasoning over multi-modal
probability distributions, and on the other hand, the learning of probabilistic
logical rules from anchored objects produced by perceptual observations. The
learned logical rules are, subsequently, used to assess our proposed
probabilistic anchoring procedure. We demonstrate our system in a setting
involving object interactions where object occlusions arise and where
probabilistic inference is needed to correctly anchor objects.",Symbolic Learning and Reasoning with Noisy Data for Probabilistic Anchoring,2020
"Joseph Tassone, Salimur Choudhury",AI,2020,"Proper scheduling of air assets can be the difference between life and death
for a patient. While poor scheduling can be incredibly problematic during
hospital transfers, it can be potentially catastrophic in the case of a
disaster. These issues are amplified in the case of an air emergency medical
service (EMS) system where populations are dispersed, and resources are
limited. There are exact methodologies existing for scheduling missions,
although actual calculation times can be quite significant given a large enough
problem space. For this research, known coordinates of air and health
facilities were used in conjunction with a formulated integer linear
programming model. This was the programmed through Gurobi so that performance
could be compared against custom algorithmic solutions. Two methods were
developed, one based on neighbourhood search and the other on Tabu search.
While both were able to achieve results quite close to the Gurobi solution, the
Tabu search outperformed the former algorithm. Additionally, it was able to do
so in a greatly decreased time, with Gurobi actually being unable to resolve to
optimal in larger examples. Parallel variations were also developed with the
compute unified device architecture (CUDA), though did not improve the timing
given the smaller sample size.",Algorithms for Optimizing Fleet Scheduling of Air Ambulances,2020
"Edoardo Bacci, David Parker",AI,2020,"Deep reinforcement learning has been successfully applied to many control
tasks, but the application of such agents in safety-critical scenarios has been
limited due to safety concerns. Rigorous testing of these controllers is
challenging, particularly when they operate in probabilistic environments due
to, for example, hardware faults or noisy sensors. We propose MOSAIC, an
algorithm for measuring the safety of deep reinforcement learning agents in
stochastic settings. Our approach is based on the iterative construction of a
formal abstraction of a controller's execution in an environment, and leverages
probabilistic model checking of Markov decision processes to produce
probabilistic guarantees on safe behaviour over a finite time horizon. It
produces bounds on the probability of safe operation of the controller for
different initial configurations and identifies regions where correct behaviour
can be guaranteed. We implement and evaluate our approach on agents trained for
several benchmark control problems.",Probabilistic Guarantees for Safe Deep Reinforcement Learning,2020
"Diego Gomez, Nicanor Quijano, Luis Felipe Giraldo",AI,2020,"While humans and animals learn incrementally during their lifetimes and
exploit their experience to solve new tasks, standard deep reinforcement
learning methods specialize to solve only one task at a time. As a result, the
information they acquire is hardly reusable in new situations. Here, we
introduce a new perspective on the problem of leveraging prior knowledge to
solve future tasks. We show that learning discrete representations of sensory
inputs can provide a high-level abstraction that is common across multiple
tasks, thus facilitating the transference of information. In particular, we
show that it is possible to learn such representations by self-supervision,
following an information theoretic approach. Our method is able to learn
concepts in locomotive and optimal control tasks that increase the sample
efficiency in both known and unknown tasks, opening a new path to endow
artificial agents with generalization abilities.",Learning Transferable Concepts in Deep Reinforcement Learning,2022
"V. Bulitko, G. Lee",AI,2011,"Real-time search methods are suited for tasks in which the agent is
interacting with an initially unknown environment in real time. In such
simultaneous planning and learning problems, the agent has to select its
actions in a limited amount of time, while sensing only a local part of the
environment centered at the agents current location. Real-time heuristic search
agents select actions using a limited lookahead search and evaluating the
frontier states with a heuristic function. Over repeated experiences, they
refine heuristic values of states to avoid infinite loops and to converge to
better solutions. The wide spread of such settings in autonomous software and
hardware agents has led to an explosion of real-time search algorithms over the
last two decades. Not only is a potential user confronted with a hodgepodge of
algorithms, but he also faces the choice of control parameters they use. In
this paper we address both problems. The first contribution is an introduction
of a simple three-parameter framework (named LRTS) which extracts the core
ideas behind many existing algorithms. We then prove that LRTA*, epsilon-LRTA*,
SLA*, and gamma-Trap algorithms are special cases of our framework. Thus, they
are unified and extended with additional features. Second, we prove
completeness and convergence of any algorithm covered by the LRTS framework.
Third, we prove several upper-bounds relating the control parameters and
solution quality. Finally, we analyze the influence of the three control
parameters empirically in the realistic scalable domains of real-time
navigation on initially unknown maps from a commercial role-playing game as
well as routing in ad hoc sensor networks.",Learning in Real-Time Search: A Unifying Framework,2011
"Jie Chen, Kian Hsiang Low, Colin Keng-Yan Tan, Ali Oran, Patrick Jaillet, John Dolan, Gaurav Sukhatme",AI,2014,"The problem of modeling and predicting spatiotemporal traffic phenomena over
an urban road network is important to many traffic applications such as
detecting and forecasting congestion hotspots. This paper presents a
decentralized data fusion and active sensing (D2FAS) algorithm for mobile
sensors to actively explore the road network to gather and assimilate the most
informative data for predicting the traffic phenomenon. We analyze the time and
communication complexity of D2FAS and demonstrate that it can scale well with a
large number of observations and sensors. We provide a theoretical guarantee on
its predictive performance to be equivalent to that of a sophisticated
centralized sparse approximation for the Gaussian process (GP) model: The
computation of such a sparse approximate GP model can thus be parallelized and
distributed among the mobile sensors (in a Google-like MapReduce paradigm),
thereby achieving efficient and scalable prediction. We also theoretically
guarantee its active sensing performance that improves under various practical
environmental conditions. Empirical evaluation on real-world urban road network
data shows that our D2FAS algorithm is significantly more time-efficient and
scalable than state-oftheart centralized algorithms while achieving comparable
predictive performance.",Decentralized Data Fusion and Active Sensing with Mobile Sensors for Modeling and Predicting Spatiotemporal Traffic Phenomena,2014
"Sheeraz Ahmad, Angela Yu",AI,2014,"Sensory inference under conditions of uncertainty is a major problem in both
machine learning and computational neuroscience. An important but poorly
understood aspect of sensory processing is the role of active sensing. Here, we
present a Bayes-optimal inference and control framework for active sensing,
C-DAC (Context-Dependent Active Controller). Unlike previously proposed
algorithms that optimize abstract statistical objectives such as information
maximization (Infomax) [Butko & Movellan, 2010] or one-step look-ahead accuracy
[Najemnik & Geisler, 2005], our active sensing model directly minimizes a
combination of behavioral costs, such as temporal delay, response error, and
effort. We simulate these algorithms on a simple visual search task to
illustrate scenarios in which context-sensitivity is particularly beneficial
and optimization with respect to generic statistical objectives particularly
inadequate. Motivated by the geometric properties of the C-DAC policy, we
present both parametric and non-parametric approximations, which retain
context-sensitivity while significantly reducing computational complexity.
These approximations enable us to investigate the more complex problem
involving peripheral vision, and we notice that the difference between C-DAC
and statistical policies becomes even more evident in this scenario.",Active Sensing as Bayes-Optimal Sequential Decision Making,2014
"Theodore Patkos, Dimitris Plexousakis, Abdelghani Chibani, Yacine Amirat",AI,2015,"Action languages have emerged as an important field of Knowledge
Representation for reasoning about change and causality in dynamic domains.
This article presents Cerbere, a production system designed to perform online
causal, temporal and epistemic reasoning based on the Event Calculus. The
framework implements the declarative semantics of the underlying logic theories
in a forward-chaining rule-based reasoning system, coupling the high
expressiveness of its formalisms with the efficiency of rule-based systems. To
illustrate its applicability, we present both the modeling of benchmark
problems in the field, as well as its utilization in the challenging domain of
smart spaces. A hybrid framework that combines logic-based with probabilistic
reasoning has been developed, that aims to accommodate activity recognition and
monitoring tasks in smart spaces. Under consideration in Theory and Practice of
Logic Programming (TPLP)",An Event Calculus Production Rule System for Reasoning in Dynamic and Uncertain Domains,2015
"Mitra Montazeri, Mahdieh Soleymani Baghshah, Ahmad Enhesari",AI,2015,"Background: Lung cancer was known as primary cancers and the survival rate of
cancer is about 15%. Early detection of lung cancer is the leading factor in
survival rate. All symptoms (features) of lung cancer do not appear until the
cancer spreads to other areas. It needs an accurate early detection of lung
cancer, for increasing the survival rate. For accurate detection, it need
characterizes efficient features and delete redundancy features among all
features. Feature selection is the problem of selecting informative features
among all features. Materials and Methods: Lung cancer database consist of 32
patient records with 57 features. This database collected by Hong and Youngand
indexed in the University of California Irvine repository. Experimental
contents include the extracted from the clinical data and X-ray data, etc. The
data described 3 types of pathological lung cancers and all features are taking
an integer value 0-3. In our study, new method is proposed for identify
efficient features of lung cancer. It is based on Hyper-Heuristic. Results: We
obtained an accuracy of 80.63% using reduced 11 feature set. The proposed
method compare to the accuracy of 5 machine learning feature selections. The
accuracy of these 5 methods are 60.94, 57.81, 68.75, 60.94 and 68.75.
Conclusions: The proposed method has better performance with the highest level
of accuracy. Therefore, the proposed model is recommended for identifying an
efficient symptom of Disease. These finding are very important in health
research, particularly in allocation of medical resources for patients who
predicted as high-risks",Hyper-Heuristic Algorithm for Finding Efficient Features in Diagnose of Lung Cancer Disease,2016
"Roel Bertens, Jilles Vreeken, Arno Siebes",AI,2015,"We study how to obtain concise descriptions of discrete multivariate
sequential data. In particular, how to do so in terms of rich multivariate
sequential patterns that can capture potentially highly interesting
(cor)relations between sequences. To this end we allow our pattern language to
span over the domains (alphabets) of all sequences, allow patterns to overlap
temporally, as well as allow for gaps in their occurrences.
  We formalise our goal by the Minimum Description Length principle, by which
our objective is to discover the set of patterns that provides the most
succinct description of the data. To discover high-quality pattern sets
directly from data, we introduce DITTO, a highly efficient algorithm that
approximates the ideal result very well.
  Experiments show that DITTO correctly discovers the patterns planted in
synthetic data. Moreover, it scales favourably with the length of the data, the
number of attributes, the alphabet sizes. On real data, ranging from sensor
networks to annotated text, DITTO discovers easily interpretable summaries that
provide clear insight in both the univariate and multivariate structure.",Keeping it Short and Simple: Summarising Complex Event Sequences with Multivariate Patterns,2016
"Khalifeh AlJadda, Mohammed Korayem, Camilo Ortiz, Trey Grainger, John A. Miller, Khaled Rasheed, Krys J. Kochut, William S. York, Rene Ranzinger, Melody Porterfield",AI,2015,"Probabilistic Graphical Models (PGM) are very useful in the fields of machine
learning and data mining. The crucial limitation of those models,however, is
the scalability. The Bayesian Network, which is one of the most common PGMs
used in machine learning and data mining, demonstrates this limitation when the
training data consists of random variables, each of them has a large set of
possible values. In the big data era, one would expect new extensions to the
existing PGMs to handle the massive amount of data produced these days by
computers, sensors and other electronic devices. With hierarchical data - data
that is arranged in a treelike structure with several levels - one would expect
to see hundreds of thousands or millions of values distributed over even just a
small number of levels. When modeling this kind of hierarchical data across
large data sets, Bayesian Networks become infeasible for representing the
probability distributions. In this paper we introduce an extension to Bayesian
Networks to handle massive sets of hierarchical data in a reasonable amount of
time and space. The proposed model achieves perfect precision of 1.0 and high
recall of 0.93 when it is used as multi-label classifier for the annotation of
mass spectrometry data. On another data set of 1.5 billion search logs provided
by CareerBuilder.com the model was able to predict latent semantic
relationships between search keywords with accuracy up to 0.80.",Mining Massive Hierarchical Data Using a Scalable Probabilistic Graphical Model,2015
"Julie Yixuan Zhu, Chao Zhang, Huichu Zhang, Shi Zhi, Victor O. K. Li, Jiawei Han, Yu Zheng",AI,2016,"Many countries are suffering from severe air pollution. Understanding how
different air pollutants accumulate and propagate is critical to making
relevant public policies. In this paper, we use urban big data (air quality
data and meteorological data) to identify the \emph{spatiotemporal (ST) causal
pathways} for air pollutants. This problem is challenging because: (1) there
are numerous noisy and low-pollution periods in the raw air quality data, which
may lead to unreliable causality analysis, (2) for large-scale data in the ST
space, the computational complexity of constructing a causal structure is very
high, and (3) the \emph{ST causal pathways} are complex due to the interactions
of multiple pollutants and the influence of environmental factors. Therefore,
we present \emph{p-Causality}, a novel pattern-aided causality analysis
approach that combines the strengths of \emph{pattern mining} and
\emph{Bayesian learning} to efficiently and faithfully identify the \emph{ST
causal pathways}. First, \emph{Pattern mining} helps suppress the noise by
capturing frequent evolving patterns (FEPs) of each monitoring sensor, and
greatly reduce the complexity by selecting the pattern-matched sensors as
""causers"". Then, \emph{Bayesian learning} carefully encodes the local and ST
causal relations with a Gaussian Bayesian network (GBN)-based graphical model,
which also integrates environmental influences to minimize biases in the final
results. We evaluate our approach with three real-world data sets containing
982 air quality sensors, in three regions of China from 01-Jun-2013 to
19-Dec-2015. Results show that our approach outperforms the traditional causal
structure learning methods in time efficiency, inference accuracy and
interpretability.",pg-Causality: Identifying Spatiotemporal Causal Pathways for Air Pollutants with Urban Big Data,2018
"Ahmed M. Alaa, Jinsung Yoon, Scott Hu, Mihaela van der Schaar",AI,2016,"Objective: In this paper, we develop a personalized real-time risk scoring
algorithm that provides timely and granular assessments for the clinical acuity
of ward patients based on their (temporal) lab tests and vital signs; the
proposed risk scoring system ensures timely intensive care unit (ICU)
admissions for clinically deteriorating patients. Methods: The risk scoring
system learns a set of latent patient subtypes from the offline electronic
health record data, and trains a mixture of Gaussian Process (GP) experts,
where each expert models the physiological data streams associated with a
specific patient subtype. Transfer learning techniques are used to learn the
relationship between a patient's latent subtype and her static admission
information (e.g. age, gender, transfer status, ICD-9 codes, etc). Results:
Experiments conducted on data from a heterogeneous cohort of 6,321 patients
admitted to Ronald Reagan UCLA medical center show that our risk score
significantly and consistently outperforms the currently deployed risk scores,
such as the Rothman index, MEWS, APACHE and SOFA scores, in terms of
timeliness, true positive rate (TPR), and positive predictive value (PPV).
Conclusion: Our results reflect the importance of adopting the concepts of
personalized medicine in critical care settings; significant accuracy and
timeliness gains can be achieved by accounting for the patients' heterogeneity.
Significance: The proposed risk scoring methodology can confer huge clinical
and social benefits on more than 200,000 critically ill inpatient who exhibit
cardiac arrests in the US every year.",Personalized Risk Scoring for Critical Care Prognosis using Mixtures of Gaussian Processes,2016
"Junping Zhou, Huanyao Sun, Feifei Ma, Jian Gao, Ke Xu, Minghao Yin",AI,2017,"We introduce a diversified top-k partial MaxSAT problem, a combination of
partial MaxSAT problem and enumeration problem. Given a partial MaxSAT formula
F and a positive integer k, the diversified top-k partial MaxSAT is to find k
maximal solutions for F such that the k maximal solutions satisfy the maximum
number of soft clauses of F. This problem can be widely used in many
applications including community detection, sensor place, motif discovery, and
combinatorial testing. We prove the problem is NP-hard and propose an approach
for solving the problem. The concrete idea of the approach is to design an
encoding EE which reduces diversified top-k partial MaxSAT problem into partial
MaxSAT problem, and then solve the resulting problem with state-of-art solvers.
In addition, we present an algorithm MEMKC exactly solving the diversified
top-k partial MaxSAT. Through several experiments we show that our approach can
be successfully applied to the interesting problem.",Diversified Top-k Partial MaxSAT Solving,2017
Amit Kumar Mishra,AI,2017,"Humans are expert in the amount of sensory data they deal with each moment.
Human brain not only analyses these data but also starts synthesizing new
information from the existing data. The current age Big-data systems are needed
not just to analyze data but also to come up new interpretation. We believe
that the pivotal ability in human brain which enables us to do this is what is
known as ""intuition"". Here, we present an intuition based architecture for big
data analysis and synthesis.",ICABiDAS: Intuition Centred Architecture for Big Data Analysis and Synthesis,2017
"Nick Cheney, Josh Bongard, Vytas SunSpiral, Hod Lipson",AI,2017,"Evolution sculpts both the body plans and nervous systems of agents together
over time. In contrast, in AI and robotics, a robot's body plan is usually
designed by hand, and control policies are then optimized for that fixed
design. The task of simultaneously co-optimizing the morphology and controller
of an embodied robot has remained a challenge. In psychology, the theory of
embodied cognition posits that behavior arises from a close coupling between
body plan and sensorimotor control, which suggests why co-optimizing these two
subsystems is so difficult: most evolutionary changes to morphology tend to
adversely impact sensorimotor control, leading to an overall decrease in
behavioral performance. Here, we further examine this hypothesis and
demonstrate a technique for ""morphological innovation protection"", which
temporarily reduces selection pressure on recently morphologically-changed
individuals, thus enabling evolution some time to ""readapt"" to the new
morphology with subsequent control policy mutations. We show the potential for
this method to avoid local optima and converge to similar highly fit
morphologies across widely varying initial conditions, while sustaining fitness
improvements further into optimization. While this technique is admittedly only
the first of many steps that must be taken to achieve scalable optimization of
embodied machines, we hope that theoretical insight into the cause of
evolutionary stagnation in current methods will help to enable the automation
of robot design and behavioral training -- while simultaneously providing a
testbed to investigate the theory of embodied cognition.",Scalable Co-Optimization of Morphology and Control in Embodied Machines,2017
"Wiem Elghazel, Kamal Medjaher, Nourredine Zerhouni, Jacques Bahi, Ahamd Farhat, Christophe Guyeux, Mourad Hakem",AI,2017,"In this paper, random forests are proposed for operating devices diagnostics
in the presence of a variable number of features. In various contexts, like
large or difficult-to-access monitored areas, wired sensor networks providing
features to achieve diagnostics are either very costly to use or totally
impossible to spread out. Using a wireless sensor network can solve this
problem, but this latter is more subjected to flaws. Furthermore, the networks'
topology often changes, leading to a variability in quality of coverage in the
targeted area. Diagnostics at the sink level must take into consideration that
both the number and the quality of the provided features are not constant, and
that some politics like scheduling or data aggregation may be developed across
the network. The aim of this article is ($1$) to show that random forests are
relevant in this context, due to their flexibility and robustness, and ($2$) to
provide first examples of use of this method for diagnostics based on data
provided by a wireless sensor network.",Random Forests for Industrial Device Functioning Diagnostics Using Wireless Sensor Networks,2017
"Matthew Peveler, Naveen Sundar Govindarajulu, Selmer Bringsjord, Biplav Srivastava, Kartik Talamadupula, Hui Su",AI,2017,"As computational power has continued to increase, and sensors have become
more accurate, the corresponding advent of systems that are at once cognitive
and immersive has arrived. These \textit{cognitive and immersive systems}
(CAISs) fall squarely into the intersection of AI with HCI/HRI: such systems
interact with and assist the human agents that enter them, in no small part
because such systems are infused with AI able to understand and reason about
these humans and their knowledge, beliefs, goals, communications, plans, etc.
We herein explain our approach to engineering CAISs. We emphasize the capacity
of a CAIS to develop and reason over a `theory of the mind' of its human
partners. This capacity entails that the AI in question has a sophisticated
model of the beliefs, knowledge, goals, desires, emotions, etc.\ of these
humans. To accomplish this engineering, a formal framework of very high
expressivity is needed. In our case, this framework is a \textit{cognitive
event calculus}, a particular kind of quantified multi-operator modal logic,
and a matching high-expressivity automated reasoner and planner. To explain,
advance, and to a degree validate our approach, we show that a calculus of this
type satisfies a set of formal requirements, and can enable a CAIS to
understand a psychologically tricky scenario couched in what we call the
\textit{cognitive polysolid framework} (CPF). We also formally show that a room
that satisfies these requirements can have a useful property we term
\emph{expectation of usefulness}. CPF, a sub-class of \textit{cognitive
microworlds}, includes machinery able to represent and plan over not merely
blocks and actions (such as seen in the primitive `blocks worlds' of old), but
also over agents and their mental attitudes about both other agents and
inanimate objects.",Toward Cognitive and Immersive Systems: Experiments in a Cognitive Microworld,2018
"Luis Piloto, Ari Weinstein, Dhruva TB, Arun Ahuja, Mehdi Mirza, Greg Wayne, David Amos, Chia-chun Hung, Matt Botvinick",AI,2018,"In order to build agents with a rich understanding of their environment, one
key objective is to endow them with a grasp of intuitive physics; an ability to
reason about three-dimensional objects, their dynamic interactions, and
responses to forces. While some work on this problem has taken the approach of
building in components such as ready-made physics engines, other research aims
to extract general physical concepts directly from sensory data. In the latter
case, one challenge that arises is evaluating the learning system. Research on
intuitive physics knowledge in children has long employed a violation of
expectations (VOE) method to assess children's mastery of specific physical
concepts. We take the novel step of applying this method to artificial learning
systems. In addition to introducing the VOE technique, we describe a set of
probe datasets inspired by classic test stimuli from developmental psychology.
We test a baseline deep learning system on this battery, as well as on a
physics learning dataset (""IntPhys"") recently posed by another research group.
Our results show how the VOE technique may provide a useful tool for tracking
physics knowledge in future research.",Probing Physics Knowledge Using Tools from Developmental Psychology,2018
"Chiara Di Francescomarino, Chiara Ghidini, Fabrizio Maria Maggi, Fredrik Milani",AI,2018,"Predictive process monitoring has recently gained traction in academia and is
maturing also in companies. However, with the growing body of research, it
might be daunting for companies to navigate in this domain in order to find,
provided certain data, what can be predicted and what methods to use. The main
objective of this paper is developing a value-driven framework for classifying
existing work on predictive process monitoring. This objective is achieved by
systematically identifying, categorizing, and analyzing existing approaches for
predictive process monitoring. The review is then used to develop a
value-driven framework that can support organizations to navigate in the
predictive process monitoring field and help them to find value and exploit the
opportunities enabled by these analysis techniques.",Predictive Process Monitoring Methods: Which One Suits Me Best?,2018
Wojciech Skaba,AI,2018,"The AGINAO is a project to create a human-level artificial general
intelligence system (HL AGI) embodied in the Aldebaran Robotics' NAO humanoid
robot. The dynamical and open-ended cognitive engine of the robot is
represented by an embedded and multi-threaded control program, that is
self-crafted rather than hand-crafted, and is executed on a simulated Universal
Turing Machine (UTM). The actual structure of the cognitive engine emerges as a
result of placing the robot in a natural preschool-like environment and running
a core start-up system that executes self-programming of the cognitive layer on
top of the core layer. The data from the robot's sensory devices supplies the
training samples for the machine learning methods, while the commands sent to
actuators enable testing hypotheses and getting a feedback. The individual
self-created subroutines are supposed to reflect the patterns and concepts of
the real world, while the overall program structure reflects the spatial and
temporal hierarchy of the world dependencies. This paper focuses on the details
of the self-programming approach, limiting the discussion of the applied
cognitive architecture to a necessary minimum.",The AGINAO Self-Programming Engine,2018
Wojciech Skaba,AI,2018,"AGINAO builds its cognitive engine by applying self-programming techniques to
create a hierarchy of interconnected codelets - the tiny pieces of code
executed on a virtual machine. These basic processing units are evaluated for
their applicability and fitness with a notion of reward calculated from
self-information gain of binary partitioning of the codelet's input
state-space. This approach, however, is useless for the evaluation of
actuators. Instead, a model is proposed in which actuators are evaluated by
measuring the impact that an activation of an effector, and consequently the
feedback from the robot sensors, has on average reward received by the
processing units.",Evaluating Actuators in a Purely Information-Theory Based Reward Model,2018
"Ali el Hassouni, Mark Hoogendoorn, Martijn van Otterlo, A. E. Eiben, Vesa Muhonen, Eduardo Barbaro",AI,2018,"Personalization is very powerful in improving the effectiveness of health
interventions. Reinforcement learning (RL) algorithms are suitable for learning
these tailored interventions from sequential data collected about individuals.
However, learning can be very fragile. The time to learn intervention policies
is limited as disengagement from the user can occur quickly. Also, in e-Health
intervention timing can be crucial before the optimal window passes. We present
an approach that learns tailored personalization policies for groups of users
by combining RL and clustering. The benefits are two-fold: speeding up the
learning to prevent disengagement while maintaining a high level of
personalization. Our clustering approach utilizes dynamic time warping to
compare user trajectories consisting of states and rewards. We apply online and
batch RL to learn policies over clusters of individuals and introduce our
self-developed and publicly available simulator for e-Health interventions to
evaluate our approach. We compare our methods with an e-Health intervention
benchmark. We demonstrate that batch learning outperforms online learning for
our setting. Furthermore, our proposed clustering approach for RL finds
near-optimal clusterings which lead to significantly better policies in terms
of cumulative reward compared to learning a policy per individual or learning
one non-personalized policy across all individuals. Our findings also indicate
that the learned policies accurately learn to send interventions at the right
moments and that the users workout more and at the right times of the day.",A clustering-based reinforcement learning approach for tailored personalization of e-Health interventions,2020
Wojciech Skaba,AI,2018,"An autonomous agent embodied in a humanoid robot, in order to learn from the
overwhelming flow of raw and noisy sensory, has to effectively reduce the high
spatial-temporal data dimensionality. In this paper we propose a novel method
of unsupervised feature extraction and selection with binary space
partitioning, followed by a computation of information gain that is interpreted
as intrinsic reward, then applied as immediate-reward signal for the
reinforcement-learning. The space partitioning is executed by tiny codelets
running on a simulated Turing Machine. The features are represented by concept
nodes arranged in a hierarchy, in which those of a lower level become the input
vectors of a higher level.",Binary Space Partitioning as Intrinsic Reward,2018
"Chiara Di Francescomarino, Chiara Ghidini, Fabrizio Maria Maggi, Williams Rizzi, Cosimo Damiano Persia",AI,2018,"A characteristic of existing predictive process monitoring techniques is to
first construct a predictive model based on past process executions, and then
use it to predict the future of new ongoing cases, without the possibility of
updating it with new cases when they complete their execution. This can make
predictive process monitoring too rigid to deal with the variability of
processes working in real environments that continuously evolve and/or exhibit
new variant behaviors over time. As a solution to this problem, we propose the
use of algorithms that allow the incremental construction of the predictive
model. These incremental learning algorithms update the model whenever new
cases become available so that the predictive model evolves over time to fit
the current circumstances. The algorithms have been implemented using different
case encoding strategies and evaluated on a number of real and synthetic
datasets. The results provide a first evidence of the potential of incremental
learning strategies for predicting process monitoring in real environments, and
of the impact of different case encoding strategies in this setting.",Incremental Predictive Process Monitoring: How to Deal with the Variability of Real Environments,2018
"Zhen Peng, Tim Genewein, Felix Leibfried, Daniel A. Braun",AI,2018,"Inspired by findings of sensorimotor coupling in humans and animals, there
has recently been a growing interest in the interaction between action and
perception in robotic systems [Bogh et al., 2016]. Here we consider perception
and action as two serial information channels with limited
information-processing capacity. We follow [Genewein et al., 2015] and
formulate a constrained optimization problem that maximizes utility under
limited information-processing capacity in the two channels. As a solution we
obtain an optimal perceptual channel and an optimal action channel that are
coupled such that perceptual information is optimized with respect to
downstream processing in the action module. The main novelty of this study is
that we propose an online optimization procedure to find bounded-optimal
perception and action channels in parameterized serial perception-action
systems. In particular, we implement the perceptual channel as a multi-layer
neural network and the action channel as a multinomial distribution. We
illustrate our method in a NAO robot simulator with a simplified cup lifting
task.",An information-theoretic on-line update principle for perception-action coupling,2018
Erik Altman,AI,2018,"We describe a set of techniques to generate queries automatically based on
one or more ingested, input corpuses. These queries require no a priori domain
knowledge, and hence no human domain experts. Thus, these auto-generated
queries help address the epistemological question of how we know what we know,
or more precisely in this case, how an AI system with ingested data knows what
it knows. These auto-generated queries can also be used to identify and remedy
problem areas in ingested material -- areas for which the knowledge of the AI
system is incomplete or even erroneous. Similarly, the proposed techniques
facilitate tests of AI capability -- both in terms of coverage and accuracy. By
removing humans from the main learning loop, our approach also allows more
effective scaling of AI and cognitive capabilities to provide (1) broader
coverage in a single domain such as health or geology; and (2) more rapid
deployment to new domains. The proposed techniques also allow ingested
knowledge to be extended naturally. Our investigations are early, and this
paper provides a description of the techniques. Assessment of their efficacy is
our next step for future work.",Understanding AI Data Repositories with Automatic Query Generation,2018
"Murali Ravuri, Anitha Kannan, Geoffrey J. Tso, Xavier Amatriain",AI,2018,"Expert diagnostic support systems have been extensively studied. The
practical applications of these systems in real-world scenarios have been
somewhat limited due to well-understood shortcomings, such as lack of
extensibility. More recently, machine-learned models for medical diagnosis have
gained momentum, since they can learn and generalize patterns found in very
large datasets like electronic health records. These models also have
shortcomings - in particular, there is no easy way to incorporate prior
knowledge from existing literature or experts. In this paper, we present a
method to merge both approaches by using expert systems as generative models
that create simulated data on which models can be learned. We demonstrate that
such a learned model not only preserves the original properties of the expert
systems but also addresses some of their limitations. Furthermore, we show how
this approach can also be used as the starting point to combine expert
knowledge with knowledge extracted from other data sources, such as electronic
health records.",Learning from the experts: From expert systems to machine-learned diagnosis models,2018
"Anahita Hosseini, Ting Chen, Wenjun Wu, Yizhou Sun, Majid Sarrafzadeh",AI,2018,"With the recent availability of Electronic Health Records (EHR) and great
opportunities they offer for advancing medical informatics, there has been
growing interest in mining EHR for improving quality of care. Disease diagnosis
due to its sensitive nature, huge costs of error, and complexity has become an
increasingly important focus of research in past years. Existing studies model
EHR by capturing co-occurrence of clinical events to learn their latent
embeddings. However, relations among clinical events carry various semantics
and contribute differently to disease diagnosis which gives precedence to a
more advanced modeling of heterogeneous data types and relations in EHR data
than existing solutions. To address these issues, we represent how
high-dimensional EHR data and its rich relationships can be suitably translated
into HeteroMed, a heterogeneous information network for robust medical
diagnosis. Our modeling approach allows for straightforward handling of missing
values and heterogeneity of data. HeteroMed exploits metapaths to capture
higher level and semantically important relations contributing to disease
diagnosis. Furthermore, it employs a joint embedding framework to tailor
clinical event representations to the disease diagnosis goal. To the best of
our knowledge, this is the first study to use Heterogeneous Information Network
for modeling clinical data and disease diagnosis. Experimental results of our
study show superior performance of HeteroMed compared to prior methods in
prediction of exact diagnosis codes and general disease cohorts. Moreover,
HeteroMed outperforms baseline models in capturing similarities of clinical
events which are examined qualitatively through case studies.",HeteroMed: Heterogeneous Information Network for Medical Diagnosis,2018
"Shabnam Sadeghi Esfahlani, Tommy Thompson",AI,2018,"This paper describes an avenue for artificial and computational intelligence
techniques applied within games research to be deployed for purposes of
physical therapy. We provide an overview of prototypical research focussed on
the application of motion sensor input devices and virtual reality equipment
for rehabilitation of motor impairment an issue typical of patient's of
traumatic brain injuries. We highlight how advances in procedural content
generation and player modelling can stimulate development in this area by
improving quality of rehabilitation programmes and measuring patient
performance.",Intelligent Physiotherapy Through Procedural Content Generation,2018
"Amin Ghafouri, Yevgeniy Vorobeychik, Xenofon Koutsoukos",AI,2018,"Attacks in cyber-physical systems (CPS) which manipulate sensor readings can
cause enormous physical damage if undetected. Detection of attacks on sensors
is crucial to mitigate this issue. We study supervised regression as a means to
detect anomalous sensor readings, where each sensor's measurement is predicted
as a function of other sensors. We show that several common learning approaches
in this context are still vulnerable to \emph{stealthy attacks}, which
carefully modify readings of compromised sensors to cause desired damage while
remaining undetected. Next, we model the interaction between the CPS defender
and attacker as a Stackelberg game in which the defender chooses detection
thresholds, while the attacker deploys a stealthy attack in response. We
present a heuristic algorithm for finding an approximately optimal threshold
for the defender in this game, and show that it increases system resilience to
attacks without significantly increasing the false alarm rate.",Adversarial Regression for Detecting Attacks in Cyber-Physical Systems,2018
"Andrea E. Martin, Leonidas A. A. Doumas",AI,2018,"Humans learn complex latent structures from their environments (e.g., natural
language, mathematics, music, social hierarchies). In cognitive science and
cognitive neuroscience, models that infer higher-order structures from sensory
or first-order representations have been proposed to account for the complexity
and flexibility of human behavior. But how do the structures that these models
invoke arise in neural systems in the first place? To answer this question, we
explain how a system can learn latent representational structures (i.e.,
predicates) from experience with wholly unstructured data. During the process
of predicate learning, an artificial neural network exploits the naturally
occurring dynamic properties of distributed computing across neuronal
assemblies in order to learn predicates, but also to combine them
compositionally, two computational aspects which appear to be necessary for
human behavior as per formal theories in multiple domains. We describe how
predicates can be combined generatively using neural oscillations to achieve
human-like extrapolation and compositionality in an artificial neural network.
The ability to learn predicates from experience, to represent structures
compositionally, and to extrapolate to unseen data offers an inroads to
understanding and modeling the most complex human behaviors.",Predicate learning in neural systems: Discovering latent generative structures,2018
"Gabriel Michau, Yang Hu, Thomas Palmé, Olga Fink",AI,2018,"Complex industrial systems are continuously monitored by a large number of
heterogeneous sensors. The diversity of their operating conditions and the
possible fault types make it impossible to collect enough data for learning all
the possible fault patterns. The paper proposes an integrated automatic
unsupervised feature learning and one-class classification for fault detection
that uses data on healthy conditions only for its training. The approach is
based on stacked Extreme Learning Machines (namely Hierarchical, or HELM) and
comprises an autoencoder, performing unsupervised feature learning, stacked
with a one-class classifier monitoring the distance of the test data to the
training healthy class, thereby assessing the health of the system.
  This study provides a comprehensive evaluation of HELM fault detection
capability compared to other machine learning approaches, such as stand-alone
one-class classifiers (ELM and SVM), these same one-class classifiers combined
with traditional dimensionality reduction methods (PCA) and a Deep Belief
Network. The performance is first evaluated on a synthetic dataset that
encompasses typical characteristics of condition monitoring data. Subsequently,
the approach is evaluated on a real case study of a power plant fault. The
proposed algorithm for fault detection, combining feature learning with the
one-class classifier, demonstrates a better performance, particularly in cases
where condition monitoring data contain several non-informative signals.",Feature Learning for Fault Detection in High-Dimensional Condition-Monitoring Signals,2019
Andrea Marrella,AI,2018,"The research activity outlined in this PhD thesis is devoted to define a
general approach, a concrete architecture and a prototype Process Management
System (PMS) for the automated adaptation of dynamic processes at run-time, on
the basis of a declarative specification of process tasks and relying on
well-established reasoning about actions and planning techniques. The purpose
is to demonstrate that the combination of procedural and imperative models with
declarative elements, along with the exploitation of techniques from the field
of artificial intelligence (AI), such as Situation Calculus, IndiGolog and
automated planning, can increase the ability of existing PMSs of supporting
dynamic processes. To this end, a prototype PMS named SmartPM, which is
specifically tailored for supporting collaborative work of process participants
during pervasive scenarios, has been developed. The adaptation mechanism
deployed on SmartPM is based on execution monitoring for detecting failures at
run-time, which does not require the definition of the adaptation strategy in
the process itself (as most of the current approaches do), and on automatic
planning techniques for the synthesis of the recovery procedure.",SmartPM: Automatic Adaptation of Dynamic Processes at Run-Time,2018
"Daoming Lyu, Fangkai Yang, Bo Liu, Steven Gustafson",AI,2018,"Deep reinforcement learning (DRL) has gained great success by learning
directly from high-dimensional sensory inputs, yet is notorious for the lack of
interpretability. Interpretability of the subtasks is critical in hierarchical
decision-making as it increases the transparency of black-box-style DRL
approach and helps the RL practitioners to understand the high-level behavior
of the system better. In this paper, we introduce symbolic planning into DRL
and propose a framework of Symbolic Deep Reinforcement Learning (SDRL) that can
handle both high-dimensional sensory inputs and symbolic planning. The
task-level interpretability is enabled by relating symbolic actions to
options.This framework features a planner -- controller -- meta-controller
architecture, which takes charge of subtask scheduling, data-driven subtask
learning, and subtask evaluation, respectively. The three components
cross-fertilize each other and eventually converge to an optimal symbolic plan
along with the learned subtasks, bringing together the advantages of long-term
planning capability with symbolic knowledge and end-to-end reinforcement
learning directly from a high-dimensional sensory input. Experimental results
validate the interpretability of subtasks, along with improved data efficiency
compared with state-of-the-art approaches.",SDRL: Interpretable and Data-efficient Deep Reinforcement Learning Leveraging Symbolic Planning,2019
"Mark Schutera, Niklas Goby, Stefan Smolarek, Markus Reischl",AI,2018,"This work examines the implications of uncoupled intersections with local
real-world topology and sensor setup on traffic light control approaches.
Control approaches are evaluated with respect to: Traffic flow, fuel
consumption and noise emission at intersections.
  The real-world road network of Friedrichshafen is depicted, preprocessed and
the present traffic light controlled intersections are modeled with respect to
state space and action space.
  Different strategies, containing fixed-time, gap-based and time-based control
approaches as well as our deep reinforcement learning based control approach,
are implemented and assessed. Our novel DRL approach allows for modeling the
TLC action space, with respect to phase selection as well as selection of
transition timings. It was found that real-world topologies, and thus
irregularly arranged intersections have an influence on the performance of
traffic light control approaches. This is even to be observed within the same
intersection types (n-arm, m-phases). Moreover we could show, that these
influences can be efficiently dealt with by our deep reinforcement learning
based control approach.",Distributed traffic light control at uncoupled intersections with real-world topology by deep reinforcement learning,2018
"Tomi Janhunen, Michael Sioutis",AI,2019,"Allen's Interval Algebra constitutes a framework for reasoning about temporal
information in a qualitative manner. In particular, it uses intervals, i.e.,
pairs of endpoints, on the timeline to represent entities corresponding to
actions, events, or tasks, and binary relations such as precedes and overlaps
to encode the possible configurations between those entities. Allen's calculus
has found its way in many academic and industrial applications that involve,
most commonly, planning and scheduling, temporal databases, and healthcare. In
this paper, we present a novel encoding of Interval Algebra using answer-set
programming (ASP) extended by difference constraints, i.e., the fragment
abbreviated as ASP(DL), and demonstrate its performance via a preliminary
experimental evaluation. Although our ASP encoding is presented in the case of
Allen's calculus for the sake of clarity, we suggest that analogous encodings
can be devised for other point-based calculi, too.",Allen's Interval Algebra Makes the Difference,2019
"Sarthak Ghosh, C. R. Ramakrishnan",AI,2019,"In medical decision making, we have to choose among several expensive
diagnostic tests such that the certainty about a patient's health is maximized
while remaining within the bounds of resources like time and money. The
expected increase in certainty in the patient's condition due to performing a
test is called the value of information (VoI) for that test. In general, VoI
relates to acquiring additional information to improve decision-making based on
probabilistic reasoning in an uncertain system. This paper presents a framework
for acquiring information based on VoI in uncertain systems modeled as
Probabilistic Logic Programs (PLPs). Optimal decision-making in uncertain
systems modeled as PLPs have already been studied before. But, acquiring
additional information to further improve the results of making the optimal
decision has remained open in this context.
  We model decision-making in an uncertain system with a PLP and a set of
top-level queries, with a set of utility measures over the distributions of
these queries. The PLP is annotated with a set of atoms labeled as
""observable""; in the medical diagnosis example, the observable atoms will be
results of diagnostic tests. Each observable atom has an associated cost. This
setting of optimally selecting observations based on VoI is more general than
that considered by any prior work. Given a limited budget, optimally choosing
observable atoms based on VoI is intractable in general. We give a greedy
algorithm for constructing a ""conditional plan"" of observations: a schedule
where the selection of what atom to observe next depends on earlier
observations. We show that, preempting the algorithm anytime before completion
provides a usable result, the result improves over time, and, in the absence of
a well-defined budget, converges to the optimal solution.",Value of Information in Probabilistic Logic Programs,2019
"Nicholas Hoernle, Kobi Gal, Barbara Grosz, Leilah Lyons, Ada Ren, Andee Rubin",AI,2019,"This paper describes methods for comparative evaluation of the
interpretability of models of high dimensional time series data inferred by
unsupervised machine learning algorithms. The time series data used in this
investigation were logs from an immersive simulation like those commonly used
in education and healthcare training. The structures learnt by the models
provide representations of participants' activities in the simulation which are
intended to be meaningful to people's interpretation. To choose the model that
induces the best representation, we designed two interpretability tests, each
of which evaluates the extent to which a model's output aligns with people's
expectations or intuitions of what has occurred in the simulation. We compared
the performance of the models on these interpretability tests to their
performance on statistical information criteria. We show that the models that
optimize interpretability quality differ from those that optimize (statistical)
information theoretic criteria. Furthermore, we found that a model using a
fully Bayesian approach performed well on both the statistical and
human-interpretability measures. The Bayesian approach is a good candidate for
fully automated model selection, i.e., when direct empirical investigations of
interpretability are costly or infeasible.",Interpretable Models for Understanding Immersive Simulations,2020
"Richard Evans, Jose Hernandez-Orallo, Johannes Welbl, Pushmeet Kohli, Marek Sergot",AI,2019,"This paper attempts to answer a central question in unsupervised learning:
what does it mean to ""make sense"" of a sensory sequence? In our formalization,
making sense involves constructing a symbolic causal theory that both explains
the sensory sequence and also satisfies a set of unity conditions. The unity
conditions insist that the constituents of the causal theory -- objects,
properties, and laws -- must be integrated into a coherent whole. On our
account, making sense of sensory input is a type of program synthesis, but it
is unsupervised program synthesis.
  Our second contribution is a computer implementation, the Apperception
Engine, that was designed to satisfy the above requirements. Our system is able
to produce interpretable human-readable causal theories from very small amounts
of data, because of the strong inductive bias provided by the unity conditions.
A causal theory produced by our system is able to predict future sensor
readings, as well as retrodict earlier readings, and impute (fill in the blanks
of) missing sensory readings, in any combination.
  We tested the engine in a diverse variety of domains, including cellular
automata, rhythms and simple nursery tunes, multi-modal binding problems,
occlusion tasks, and sequence induction intelligence tests. In each domain, we
test our engine's ability to predict future sensor values, retrodict earlier
sensor values, and impute missing sensory data. The engine performs well in all
these domains, significantly out-performing neural net baselines. We note in
particular that in the sequence induction intelligence tests, our system
achieved human-level performance. This is notable because our system is not a
bespoke system designed specifically to solve intelligence tests, but a
general-purpose system that was designed to make sense of any sensory sequence.",Making sense of sensory input,2020
"Gyunam Park, Minseok Song",AI,2019,"Predictive business process monitoring aims at providing predictions about
running instances by analyzing logs of completed cases in a business process.
Recently, a lot of research focuses on increasing productivity and efficiency
in a business process by forecasting potential problems during its executions.
However, most of the studies lack suggesting concrete actions to improve the
process. They leave it up to the subjective judgment of a user. In this paper,
we propose a novel method to connect the results from predictive business
process monitoring to actual business process improvements. More in detail, we
optimize the resource allocation in a non-clairvoyant online environment, where
we have limited information required for scheduling, by exploiting the
predictions. The proposed method integrates the offline prediction model
construction that predicts the processing time and the next activity of an
ongoing instance using Bayesian Neural Networks (BNNs) with the online resource
allocation that is extended from the minimum cost and maximum flow algorithm.
To validate the proposed method, we performed experiments using an artificial
event log and a real-life event log from a global financial organization.",Prediction-based Resource Allocation using Bayesian Neural Networks and Minimum Cost and Maximum Flow Algorithm,2021
"Atiye Alaeddini, Daniel Klein",AI,2019,"Decisions in public health are almost always made in the context of
uncertainty. Policy makers are responsible for making important decisions,
faced with the daunting task of choosing from amongst many possible options.
This task is called planning under uncertainty, and is particularly acute when
addressing complex systems, such as issues of global health and development.
Uncertainty leads to cautious or incorrect decisions that cost time, money, and
human life. It is with this understanding that we pursue greater clarity on,
and methods to address optimal policy making in health. Decision making under
uncertainty is a challenging task, and all too often this uncertainty is
averaged away to simplify results for policy makers. Our goal in this work is
to implement dynamic programming which provides basis for compiling planning
results into reactive strategies. We present here a description of an AI-based
method and illustrate how this method can improve our ability to find an
optimal vaccination strategy. We model the problem as a partially observable
Markov decision process, POMDP and show how a re-active policy can be computed
using dynamic programming. In this paper, we developed a framework for optimal
health policy design in an uncertain dynamic setting. We apply a stochastic
dynamic programming approach to identify the optimal time to change the health
intervention policy and the value of decision relevant information for
improving the impact of the policy.",Optimal Immunization Policy Using Dynamic Programming,2020
Shilpesh Garg,AI,2019,"A novel general intelligence model is proposed with three types of learning.
A unified sequence of the foreground percept trace and the command trace
translates into direct and time-hop observation paths to form the basis of Raw
learning. Raw learning includes the formation of image-image associations,
which lead to the perception of temporal and spatial relationships among
objects and object parts; and the formation of image-audio associations, which
serve as the building blocks of language. Offline identification of similar
segments in the observation paths and their subsequent reduction into a common
segment through merging of memory nodes leads to Generalized learning.
Generalization includes the formation of interpolated sensory nodes for robust
and generic matching, the formation of sensory properties nodes for specific
matching and superimposition, and the formation of group nodes for simpler
logic pathways. Online superimposition of memory nodes across multiple
predictions, primarily the superimposition of images on the internal projection
canvas, gives rise to Innovative learning and thought. The learning of actions
happens the same way as raw learning while the action determination happens
through the utility model built into the raw learnings, the utility function
being the pleasure and pain of the physical senses.",RTOP: A Conceptual and Computational Framework for General Intelligence,2020
"Jalal Etesami, Philipp Geiger",AI,2020,"Learning from demonstrations (LfD) is an efficient paradigm to train AI
agents. But major issues arise when there are differences between (a) the
demonstrator's own sensory input, (b) our sensors that observe the demonstrator
and (c) the sensory input of the agent we train. In this paper, we propose a
causal model-based framework for transfer learning under such ""sensor-shifts"",
for two common LfD tasks: (1) inferring the effect of the demonstrator's
actions and (2) imitation learning. First we rigorously analyze, on the
population-level, to what extent the relevant underlying mechanisms (the action
effects and the demonstrator policy) can be identified and transferred from the
available observations together with prior knowledge of sensor characteristics.
And we device an algorithm to infer these mechanisms. Then we introduce several
proxy methods which are easier to calculate, estimate from finite data and
interpret than the exact solutions, alongside theoretical bounds on their
closeness to the exact ones. We validate our two main methods on simulated and
semi-real world data.",Causal Transfer for Imitation Learning and Decision Making under Sensor-shift,2020
"Jennifer Renoux, Uwe Köckemann, Amy Loutfi",AI,2020,"Smart home environments equipped with distributed sensor networks are capable
of helping people by providing services related to health, emergency detection
or daily routine management. A backbone to these systems relies often on the
system's ability to track and detect activities performed by the users in their
home. Despite the continuous progress in the area of activity recognition in
smart homes, many systems make a strong underlying assumption that the number
of occupants in the home at any given moment of time is always known.
Estimating the number of persons in a Smart Home at each time step remains a
challenge nowadays. Indeed, unlike most (crowd) counting solution which are
based on computer vision techniques, the sensors considered in a Smart Home are
often very simple and do not offer individually a good overview of the
situation. The data gathered needs therefore to be fused in order to infer
useful information. This paper aims at addressing this challenge and presents a
probabilistic approach able to estimate the number of persons in the
environment at each time step. This approach works in two steps: first, an
estimate of the number of persons present in the environment is done using a
Constraint Satisfaction Problem solver, based on the topology of the sensor
network and the sensor activation pattern at this time point. Then, a Hidden
Markov Model refines this estimate by considering the uncertainty related to
the sensors. Using both simulated and real data, our method has been tested and
validated on two smart homes of different sizes and configuration and
demonstrates the ability to accurately estimate the number of inhabitants.",Online Guest Detection in a Smart Home using Pervasive Sensors and Probabilistic Reasoning,2020
"Shushman Choudhury, Nate Gruver, Mykel J. Kochenderfer",AI,2020,"Adaptive Informative Path Planning (AIPP) problems model an agent tasked with
obtaining information subject to resource constraints in unknown, partially
observable environments. Existing work on AIPP has focused on representing
observations about the world as a result of agent movement. We formulate the
more general setting where the agent may choose between different sensors at
the cost of some energy, in addition to traversing the environment to gather
information. We call this problem AIPPMS (MS for Multimodal Sensing). AIPPMS
requires reasoning jointly about the effects of sensing and movement in terms
of both energy expended and information gained. We frame AIPPMS as a Partially
Observable Markov Decision Process (POMDP) and solve it with online planning.
Our approach is based on the Partially Observable Monte Carlo Planning
framework with modifications to ensure constraint feasibility and a heuristic
rollout policy tailored for AIPPMS. We evaluate our method on two domains: a
simulated search-and-rescue scenario and a challenging extension to the classic
RockSample problem. We find that our approach outperforms a classic AIPP
algorithm that is modified for AIPPMS, as well as online planning using a
random rollout policy.",Adaptive Informative Path Planning with Multimodal Sensing,2020
"Albert Buchard, Baptiste Bouvier, Giulia Prando, Rory Beard, Michail Livieratos, Dan Busbridge, Daniel Thompson, Jonathan Richens, Yuanzhao Zhang, Adam Baker, Yura Perov, Kostis Gourgoulias, Saurabh Johri",AI,2020,"Medical Triage is of paramount importance to healthcare systems, allowing for
the correct orientation of patients and allocation of the necessary resources
to treat them adequately. While reliable decision-tree methods exist to triage
patients based on their presentation, those trees implicitly require human
inference and are not immediately applicable in a fully automated setting. On
the other hand, learning triage policies directly from experts may correct for
some of the limitations of hard-coded decision-trees. In this work, we present
a Deep Reinforcement Learning approach (a variant of DeepQ-Learning) to triage
patients using curated clinical vignettes. The dataset, consisting of 1374
clinical vignettes, was created by medical doctors to represent real-life
cases. Each vignette is associated with an average of 3.8 expert triage
decisions given by medical doctors relying solely on medical history. We show
that this approach is on a par with human performance, yielding safe triage
decisions in 94% of cases, and matching expert decisions in 85% of cases. The
trained agent learns when to stop asking questions, acquires optimized decision
policies requiring less evidence than supervised approaches, and adapts to the
novelty of a situation by asking for more information. Overall, we demonstrate
that a Deep Reinforcement Learning approach can learn effective medical triage
policies directly from expert decisions, without requiring expert knowledge
engineering. This approach is scalable and can be deployed in healthcare
settings or geographical regions with distinct triage specifications, or where
trained experts are scarce, to improve decision making in the early stage of
care.",Learning medical triage from clinicians using Deep Q-Learning,2020
"F. Martínez-Álvarez, G. Asencio-Cortés, J. F. Torres, D. Gutiérrez-Avilés, L. Melgar-García, R. Pérez-Chacón, C. Rubio-Escudero, J. C. Riquelme, A. Troncoso",AI,2020,"A novel bioinspired metaheuristic is proposed in this work, simulating how
the coronavirus spreads and infects healthy people. From an initial individual
(the patient zero), the coronavirus infects new patients at known rates,
creating new populations of infected people. Every individual can either die or
infect and, afterwards, be sent to the recovered population. Relevant terms
such as re-infection probability, super-spreading rate or traveling rate are
introduced in the model in order to simulate as accurately as possible the
coronavirus activity. The Coronavirus Optimization Algorithm has two major
advantages compared to other similar strategies. First, the input parameters
are already set according to the disease statistics, preventing researchers
from initializing them with arbitrary values. Second, the approach has the
ability of ending after several iterations, without setting this value either.
Infected population initially grows at an exponential rate but after some
iterations, when considering social isolation measures and the high number
recovered and dead people, the number of infected people starts decreasing in
subsequent iterations. Furthermore, a parallel multi-virus version is proposed
in which several coronavirus strains evolve over time and explore wider search
space areas in less iterations. Finally, the metaheuristic has been combined
with deep learning models, in order to find optimal hyperparameters during the
training phase. As application case, the problem of electricity load time
series forecasting has been addressed, showing quite remarkable performance.",Coronavirus Optimization Algorithm: A bioinspired metaheuristic based on the COVID-19 propagation model,2020
"Sam Vente, Angelika Kimmig, Alun Preece, Federico Cerutti",AI,2020,"Automated negotiation has been used in a variety of distributed settings,
such as privacy in the Internet of Things (IoT) devices and power distribution
in Smart Grids. The most common protocol under which these agents negotiate is
the Alternating Offers Protocol (AOP). Under this protocol, agents cannot
express any additional information to each other besides a counter offer. This
can lead to unnecessarily long negotiations when, for example, negotiations are
impossible, risking to waste bandwidth that is a precious resource at the edge
of the network. While alternative protocols exist which alleviate this problem,
these solutions are too complex for low power devices, such as IoT sensors
operating at the edge of the network. To improve this bottleneck, we introduce
an extension to AOP called Alternating Constrained Offers Protocol (ACOP), in
which agents can also express constraints to each other. This allows agents to
both search the possibility space more efficiently and recognise impossible
situations sooner. We empirically show that agents using ACOP can significantly
reduce the number of messages a negotiation takes, independently of the
strategy agents choose. In particular, we show our method significantly reduces
the number of messages when an agreement is not possible. Furthermore, when an
agreement is possible it reaches this agreement sooner with no negative effect
on the utility.",Increasing negotiation performance at the edge of the network,2020
"Ramtin Keramati, Emma Brunskill",AI,2020,"Interactive adaptive systems powered by Reinforcement Learning (RL) have many
potential applications, such as intelligent tutoring systems. In such systems
there is typically an external human system designer that is creating,
monitoring and modifying the interactive adaptive system, trying to improve its
performance on the target outcomes. In this paper we focus on algorithmic
foundation of how to help the system designer choose the set of sensors or
features to define the observation space used by reinforcement learning agent.
We present an algorithm, value driven representation (VDR), that can
iteratively and adaptively augment the observation space of a reinforcement
learning agent so that is sufficient to capture a (near) optimal policy. To do
so we introduce a new method to optimistically estimate the value of a policy
using offline simulated Monte Carlo rollouts. We evaluate the performance of
our approach on standard RL benchmarks with simulated humans and demonstrate
significant improvement over prior baselines.",Value Driven Representation for Human-in-the-Loop Reinforcement Learning,2020
"Zina Ibrahim, Honghan Wu, Richard Dobson",AI,2020,"Many areas of research are characterised by the deluge of large-scale
highly-dimensional time-series data. However, using the data available for
prediction and decision making is hampered by the current lag in our ability to
uncover and quantify true interactions that explain the outcomes.We are
interested in areas such as intensive care medicine, which are characterised by
i) continuous monitoring of multivariate variables and non-uniform sampling of
data streams, ii) the outcomes are generally governed by interactions between a
small set of rare events, iii) these interactions are not necessarily definable
by specific values (or value ranges) of a given group of variables, but rather,
by the deviations of these values from the normal state recorded over time, iv)
the need to explain the predictions made by the model. Here, while numerous
data mining models have been formulated for outcome prediction, they are unable
to explain their predictions.
  We present a model for uncovering interactions with the highest likelihood of
generating the outcomes seen from highly-dimensional time series data.
Interactions among variables are represented by a relational graph structure,
which relies on qualitative abstractions to overcome non-uniform sampling and
to capture the semantics of the interactions corresponding to the changes and
deviations from normality of variables of interest over time. Using the
assumption that similar templates of small interactions are responsible for the
outcomes (as prevalent in the medical domains), we reformulate the discovery
task to retrieve the most-likely templates from the data.",Modeling Rare Interactions in Time Series Data Through Qualitative Change: Application to Outcome Prediction in Intensive Care Units,2020
"Ioannis Apostolopoulos, Peter Groumpos",AI,2020,"Cardiovascular Diseases (CVD) and strokes produce immense health and economic
burdens globally. Coronary Artery Disease (CAD) is the most common type of
cardiovascular disease. Coronary Angiography, which is an invasive treatment,
is also the standard procedure for diagnosing CAD. In this work, we illustrate
a Medical Decision Support System for the prediction of Coronary Artery Disease
(CAD) utilizing Fuzzy Cognitive Maps (FCMs). FCMs are a promising modeling
methodology, based on human knowledge, capable of dealing with ambiguity and
uncertainty, and learning how to adapt to the unknown or changing environment.
The newly proposed MDSS is developed using the basic notions of Fuzzy Logic and
Fuzzy Cognitive Maps, with some adjustments to improve the results. The
proposed model, tested on a labelled CAD dataset of 303 patients, obtains an
accuracy of 78.2% outmatching several state-of-the-art classification
algorithms.",Non-invasive modelling methodology for the diagnosis of Coronary Artery Disease using Fuzzy Cognitive Maps,2020
"Emmanouil Rigas, Panayiotis Kolios, Georgios Ellinas",AI,2020,"In this paper we schedule the travel path of a set of drones across a graph
where the nodes need to be visited multiple times at pre-defined points in
time. This is an extension of the well-known multiple traveling salesman
problem. The proposed formulation can be applied in several domains such as the
monitoring of traffic flows in a transportation network, or the monitoring of
remote locations to assist search and rescue missions. Aiming to find the
optimal schedule, the problem is formulated as an Integer Linear Program (ILP).
Given that the problem is highly combinatorial, the optimal solution scales
only for small sized problems. Thus, a greedy algorithm is also proposed that
uses a one-step look ahead heuristic search mechanism. In a detailed
evaluation, it is observed that the greedy algorithm has near-optimal
performance as it is on average at 92.06% of the optimal, while it can
potentially scale up to settings with hundreds of drones and locations.",Extending the Multiple Traveling Salesman Problem for Scheduling a Fleet of Drones Performing Monitoring Missions,2020
"Grace McFassel, Dylan A. Shell",AI,2020,"In studying robots and planning problems, a basic question is what is the
minimal information a robot must obtain to guarantee task completion. Erdmann's
theory of action-based sensors is a classical approach to characterizing
fundamental information requirements. That approach uses a plan to derive a
type of virtual sensor which prescribes actions that make progress toward a
goal. We show that the established theory is incomplete: the previous method
for obtaining such sensors, using backchained plans, overlooks some sensors.
Furthermore, there are plans, that are guaranteed to achieve goals, where the
existing methods are unable to provide any action-based sensor. We identify the
underlying feature common to all such plans. Then, we show how to produce
action-based sensors even for plans where the existing treatment is inadequate,
although for these cases they have no single canonical sensor. Consequently,
the approach is generalized to produce sets of sensors. Finally, we show also
that this is a complete characterization of action-based sensors for planning
problems and discuss how an action-based sensor translates into the traditional
conception of a sensor.",Every Action Based Sensor,2020
"Maria Inês Silva, Roberto Henriques",AI,2020,"Processing driving data and investigating driving behavior has been receiving
an increasing interest in the last decades, with applications ranging from car
insurance pricing to policy making. A common strategy to analyze driving
behavior is to study the maneuvers being performance by the driver. In this
paper, we propose TripMD, a system that extracts the most relevant driving
patterns from sensor recordings (such as acceleration) and provides a
visualization that allows for an easy investigation. Additionally, we test our
system using the UAH-DriveSet dataset, a publicly available naturalistic
driving dataset. We show that (1) our system can extract a rich number of
driving patterns from a single driver that are meaningful to understand driving
behaviors and (2) our system can be used to identify the driving behavior of an
unknown driver from a set of drivers whose behavior we know.",TripMD: Driving patterns investigation via Motif Analysis,2021
"Richard Evans, Jose Hernandez-Orallo, Johannes Welbl, Pushmeet Kohli, Marek Sergot",AI,2020,"The Apperception Engine is an unsupervised learning system. Given a sequence
of sensory inputs, it constructs a symbolic causal theory that both explains
the sensory sequence and also satisfies a set of unity conditions. The unity
conditions insist that the constituents of the theory - objects, properties,
and laws - must be integrated into a coherent whole. Once a theory has been
constructed, it can be applied to predict future sensor readings, retrodict
earlier readings, or impute missing readings.
  In this paper, we evaluate the Apperception Engine in a diverse variety of
domains, including cellular automata, rhythms and simple nursery tunes,
multi-modal binding problems, occlusion tasks, and sequence induction
intelligence tests. In each domain, we test our engine's ability to predict
future sensor values, retrodict earlier sensor values, and impute missing
sensory data. The engine performs well in all these domains, significantly
outperforming neural net baselines and state of the art inductive logic
programming systems. These results are significant because neural nets
typically struggle to solve the binding problem (where information from
different modalities must somehow be combined together into different aspects
of one unified object) and fail to solve occlusion tasks (in which objects are
sometimes visible and sometimes obscured from view). We note in particular that
in the sequence induction intelligence tests, our system achieved human-level
performance. This is notable because our system is not a bespoke system
designed specifically to solve intelligence tests, but a general-purpose system
that was designed to make sense of any sensory sequence.",Evaluating the Apperception Engine,2020
"Jens Brunk, Matthias Stierle, Leon Papke, Kate Revoredo, Martin Matzner, Jörg Becker",AI,2020,"Predicting undesirable events during the execution of a business process
instance provides the process participants with an opportunity to intervene and
keep the process aligned with its goals. Few approaches for tackling this
challenge consider a multi-perspective view, where the flow perspective of the
process is combined with its surrounding context. Given the many sources of
data in today's world, context can vary widely and have various meanings. This
paper addresses the issue of context being cause or effect of the next event
and its impact on next event prediction. We leverage previous work on
probabilistic models to develop a Dynamic Bayesian Network technique.
Probabilistic models are considered comprehensible and they allow the end-user
and his or her understanding of the domain to be involved in the prediction.
Our technique models context attributes that have either a cause or effect
relationship towards the event. We evaluate our technique with two real-life
data sets and benchmark it with other techniques from the field of predictive
process monitoring. The results show that our solution achieves superior
prediction results if context information is correctly introduced into the
model.",Cause vs. Effect in Context-Sensitive Prediction of Business Process Instances,2020
"George Baryannis, Ilias Tachmazidis, Sotiris Batsakis, Grigoris Antoniou, Mario Alviano, Emmanuel Papadakis",AI,2020,"Qualitative reasoning involves expressing and deriving knowledge based on
qualitative terms such as natural language expressions, rather than strict
mathematical quantities. Well over 40 qualitative calculi have been proposed so
far, mostly in the spatial and temporal domains, with several practical
applications such as naval traffic monitoring, warehouse process optimisation
and robot manipulation. Even if a number of specialised qualitative reasoning
tools have been developed so far, an important barrier to the wider adoption of
these tools is that only qualitative reasoning is supported natively, when
real-world problems most often require a combination of qualitative and other
forms of reasoning. In this work, we propose to overcome this barrier by using
ASP as a unifying formalism to tackle problems that require qualitative
reasoning in addition to non-qualitative reasoning. A family of ASP encodings
is proposed which can handle any qualitative calculus with binary relations.
These encodings are experimentally evaluated using a real-world dataset based
on a case study of determining optimal coverage of telecommunication antennas,
and compared with the performance of two well-known dedicated reasoners.
Experimental results show that the proposed encodings outperform one of the two
reasoners, but fall behind the other, an acceptable trade-off given the added
benefits of handling any type of reasoning as well as the interpretability of
logic programs. This paper is under consideration for acceptance in TPLP.",A Generalised Approach for Encoding and Reasoning with Qualitative Theories in Answer Set Programming,2020
"Roohallah Alizadehsani, Mohamad Roshanzamir, Sadiq Hussain, Abbas Khosravi, Afsaneh Koohestani, Mohammad Hossein Zangooei, Moloud Abdar, Adham Beykikhoshk, Afshin Shoeibi, Assef Zare, Maryam Panahiazar, Saeid Nahavandi, Dipti Srinivasan, Amir F. Atiya, U. Rajendra Acharya",AI,2020,"Understanding data and reaching valid conclusions are of paramount importance
in the present era of big data. Machine learning and probability theory methods
have widespread application for this purpose in different fields. One
critically important yet less explored aspect is how data and model
uncertainties are captured and analyzed. Proper quantification of uncertainty
provides valuable information for optimal decision making. This paper reviewed
related studies conducted in the last 30 years (from 1991 to 2020) in handling
uncertainties in medical data using probability theory and machine learning
techniques. Medical data is more prone to uncertainty due to the presence of
noise in the data. So, it is very important to have clean medical data without
any noise to get accurate diagnosis. The sources of noise in the medical data
need to be known to address this issue. Based on the medical data obtained by
the physician, diagnosis of disease, and treatment plan are prescribed. Hence,
the uncertainty is growing in healthcare and there is limited knowledge to
address these problems. We have little knowledge about the optimal treatment
methods as there are many sources of uncertainty in medical science. Our
findings indicate that there are few challenges to be addressed in handling the
uncertainty in medical raw data and new models. In this work, we have
summarized various methods employed to overcome this problem. Nowadays,
application of novel deep learning techniques to deal such uncertainties have
significantly increased.",Handling of uncertainty in medical data using machine learning and probability theory techniques: A review of 30 years (1991-2020),2020
"Arvind W. Kiwelekar, Geetanjali S. Mahamunkar, Laxman D. Netak, Valmik B Nikam",AI,2020,"Consumer electronic devices such as mobile handsets, goods tagged with RFID
labels, location and position sensors are continuously generating a vast amount
of location enriched data called geospatial data. Conventionally such
geospatial data is used for military applications. In recent times, many useful
civilian applications have been designed and deployed around such geospatial
data. For example, a recommendation system to suggest restaurants or places of
attraction to a tourist visiting a particular locality. At the same time, civic
bodies are harnessing geospatial data generated through remote sensing devices
to provide better services to citizens such as traffic monitoring, pothole
identification, and weather reporting. Typically such applications are
leveraged upon non-hierarchical machine learning techniques such as Naive-Bayes
Classifiers, Support Vector Machines, and decision trees. Recent advances in
the field of deep-learning showed that Neural Network-based techniques
outperform conventional techniques and provide effective solutions for many
geospatial data analysis tasks such as object recognition, image
classification, and scene understanding. The chapter presents a survey on the
current state of the applications of deep learning techniques for analyzing
geospatial data.
  The chapter is organized as below: (i) A brief overview of deep learning
algorithms. (ii)Geospatial Analysis: a Data Science Perspective (iii)
Deep-learning techniques for Remote Sensing data analytics tasks (iv)
Deep-learning techniques for GPS data analytics(iv) Deep-learning techniques
for RFID data analytics.",Deep Learning Techniques for Geospatial Data Analysis,2020
"Mayssa Ben Kahla, Dalel Kanzari, Ahmed Maalel",AI,2020,"According to GHO (Global Health Observatory (GHO), the high prevalence of a
large variety of diseases such as Ischaemic heart disease, stroke, lung cancer
disease and lower respiratory infections have remained the top killers during
the past decade.
  The growth in the number of mortalities caused by these disease is due to the
very delayed symptoms'detection. Since in the early stages, the symptoms are
insignificant and similar to those of benign diseases (e.g. the flu ), and we
can only detect the disease at an advanced stage.
  In addition, The high frequency of improper practices that are harmful to
health, the hereditary factors, and the stressful living conditions can
increase the death rates.
  Many researches dealt with these fatal disease, and most of them applied
advantage machine learning models to deal with image diagnosis. However the
drawback is that imagery permit only to detect disease at a very delayed stage
and then patient can hardly be saved.
  In this Paper we present our new approach ""DeepLCP"" to predict fatal diseases
that threaten people's lives. It's mainly based on raw and heterogeneous data
of the concerned (or under-tested) person. ""DeepLCP"" results of a combination
combination of the Natural Language Processing (NLP) and the deep learning
paradigm.The experimental results of the proposed model in the case of Lung
cancer prediction have approved high accuracy and a low loss data rate during
the validation of the disease prediction.",General DeepLCP model for disease prediction : Case of Lung Cancer,2020
"Rebecca Bernemann, Benjamin Cabrera, Reiko Heckel, Barbara König",AI,2020,"This paper exploits extended Bayesian networks for uncertainty reasoning on
Petri nets, where firing of transitions is probabilistic. In particular,
Bayesian networks are used as symbolic representations of probability
distributions, modelling the observer's knowledge about the tokens in the net.
The observer can study the net by monitoring successful and failed steps.
  An update mechanism for Bayesian nets is enabled by relaxing some of their
restrictions, leading to modular Bayesian nets that can conveniently be
represented and modified. As for every symbolic representation, the question is
how to derive information - in this case marginal probability distributions -
from a modular Bayesian net. We show how to do this by generalizing the known
method of variable elimination.
  The approach is illustrated by examples about the spreading of diseases (SIR
model) and information diffusion in social networks. We have implemented our
approach and provide runtime results.",Uncertainty Reasoning for Probabilistic Petri Nets via Bayesian Networks,2020
"Kevin Leahy, Austin Jones, Cristian-Ioan Vasile",AI,2020,"In this work, we focus on decomposing large multi-agent path planning
problems with global temporal logic goals (common to all agents) into smaller
sub-problems that can be solved and executed independently. Crucially, the
sub-problems' solutions must jointly satisfy the common global mission
specification. The agents' missions are given as Capability Temporal Logic
(CaTL) formulas, a fragment of signal temporal logic, that can express
properties over tasks involving multiple agent capabilities (sensors, e.g.,
camera, IR, and effectors, e.g., wheeled, flying, manipulators) under strict
timing constraints. The approach we take is to decompose both the temporal
logic specification and the team of agents. We jointly reason about the
assignment of agents to subteams and the decomposition of formulas using a
satisfiability modulo theories (SMT) approach. The output of the SMT is then
distributed to subteams and leads to a significant speed up in planning time.
We include computational results to evaluate the efficiency of our solution, as
well as the trade-offs introduced by the conservative nature of the SMT
encoding.",Fast Decomposition of Temporal Logic Specifications for Heterogeneous Teams,2020
"Sonia Baee, Mark Rucker, Anna Baglione, Mawulolo K. Ameko, Laura Barnes",AI,2020,"Virtual coaching has rapidly evolved into a foundational component of modern
clinical practice. At a time when healthcare professionals are in short supply
and the demand for low-cost treatments is ever-increasing, virtual health
coaches (VHCs) offer intervention-on-demand for those limited by finances or
geographic access to care. More recently, AI-powered virtual coaches have
become a viable complement to human coaches. However, the push for AI-powered
coaching systems raises several important issues for researchers, designers,
clinicians, and patients. In this paper, we present a novel framework to guide
the design and development of virtual coaching systems. This framework augments
a traditional data science pipeline with four key guiding goals: reliability,
fairness, engagement, and ethics.",A Framework for Addressing the Risks and Opportunities In AI-Supported Virtual Health Coaches,2020
"Farhana Faruqe, Ryan Watkins, Larry Medsker",AI,2020,"The work reported here addresses the capacity of psychophysiological sensors
and measures using Electroencephalogram (EEG) and Galvanic Skin Response (GSR)
to detect levels of trust for humans using AI-supported Human-Machine
Interaction (HMI). Improvements to the analysis of EEG and GSR data may create
models that perform as well, or better than, traditional tools. A challenge to
analyzing the EEG and GSR data is the large amount of training data required
due to a large number of variables in the measurements. Researchers have
routinely used standard machine-learning classifiers like artificial neural
networks (ANN), support vector machines (SVM), and K-nearest neighbors (KNN).
Traditionally, these have provided few insights into which features of the EEG
and GSR data facilitate the more and least accurate predictions - thus making
it harder to improve the HMI and human-machine trust relationship. A key
ingredient to applying trust-sensor research results to practical situations
and monitoring trust in work environments is the understanding of which key
features are contributing to trust and then reducing the amount of data needed
for practical applications. We used the Local Interpretable Model-agnostic
Explanations (LIME) model as a process to reduce the volume of data required to
monitor and enhance trust in HMI systems - a technology that could be valuable
for governmental and public sector applications. Explainable AI can make HMI
systems transparent and promote trust. From customer service in government
agencies and community-level non-profit public service organizations to
national military and cybersecurity institutions, many public sector
organizations are increasingly concerned to have effective and ethical HMI with
services that are trustworthy, unbiased, and free of unintended negative
consequences.",Monitoring Trust in Human-Machine Interactions for Public Sector Applications,2020
"Manas Gaur, Keyur Faldu, Amit Sheth",AI,2020,"The recent series of innovations in deep learning (DL) have shown enormous
potential to impact individuals and society, both positively and negatively.
The DL models utilizing massive computing power and enormous datasets have
significantly outperformed prior historical benchmarks on increasingly
difficult, well-defined research tasks across technology domains such as
computer vision, natural language processing, signal processing, and
human-computer interactions. However, the Black-Box nature of DL models and
their over-reliance on massive amounts of data condensed into labels and dense
representations poses challenges for interpretability and explainability of the
system. Furthermore, DLs have not yet been proven in their ability to
effectively utilize relevant domain knowledge and experience critical to human
understanding. This aspect is missing in early data-focused approaches and
necessitated knowledge-infused learning and other strategies to incorporate
computational knowledge. This article demonstrates how knowledge, provided as a
knowledge graph, is incorporated into DL methods using knowledge-infused
learning, which is one of the strategies. We then discuss how this makes a
fundamental difference in the interpretability and explainability of current
approaches, and illustrate it with examples from natural language processing
for healthcare and education applications.",Semantics of the Black-Box: Can knowledge graphs help make deep learning systems more interpretable and explainable?,2020
"An Nguyen, Wenyu Zhang, Leo Schwinn, Bjoern Eskofier",AI,2020,"Process Mining has recently gained popularity in healthcare due to its
potential to provide a transparent, objective and data-based view on processes.
Conformance checking is a sub-discipline of process mining that has the
potential to answer how the actual process executions deviate from existing
guidelines. In this work, we analyze a medical training process for a surgical
procedure. Ten students were trained to install a Central Venous Catheters
(CVC) with ultrasound. Event log data was collected directly after instruction
by the supervisors during a first test run and additionally after a subsequent
individual training phase. In order to provide objective performance measures,
we formulate an optimal, global sequence alignment problem inspired by
approaches in bioinformatics. Therefore, we use the Petri net model
representation of the medical process guideline to simulate a representative
set of guideline conform sequences. Next, we calculate the optimal, global
sequence alignment of the recorded and simulated event logs. Finally, the
output measures and visualization of aligned sequences are provided for
objective feedback.",Conformance Checking for a Medical Training Process Using Petri net Simulation and Sequence Alignment,2020
"Bahadorreza Ofoghi, Vicky Mak, John Yearwood",AI,2020,"In this paper, we propose a new mixed-integer linear programming (MILP) model
ontology and a novel constraint typology of MILP formulations. MILP is a
commonly used mathematical programming technique for modelling and solving
real-life scheduling, routing, planning, resource allocation, and timetabling
optimization problems providing optimized business solutions for industry
sectors such as manufacturing, agriculture, defence, healthcare, medicine,
energy, finance, and transportation. Despite the numerous real-life
Combinatorial Optimization Problems found and solved and millions yet to be
discovered and formulated, the number of types of constraints (the building
blocks of a MILP) is relatively small. In the search for a suitable
machine-readable knowledge representation structure for MILPs, we propose an
optimization modelling tree built based upon an MILP model ontology that can be
used as a guide for automated systems to elicit an MILP model from end-users on
their combinatorial business optimization problems. Our ultimate aim is to
develop a machine-readable knowledge representation for MILP that allows us to
map an end-user's natural language description of the business optimization
problem to an MILP formal specification as a first step towards automated
mathematical modelling.",A Knowledge Representation Approach to Automated Mathematical Modelling,2021
"Md. Mushfiqur Rahman, Nahian Muhtasim Zahin, Kazi Raiyan Mahmud, Md. Azmaeen Bin Ansar",AI,2020,"Ill-managed intersections are the primary reasons behind the increasing
traffic problem in urban areas, leading to nonoptimal traffic-flow and
unnecessary deadlocks. In this paper, we propose an automated intersection
management system that extracts data from a well-defined grid of sensors and
optimizes traffic flow by controlling traffic signals. The data extraction
mechanism is independent of the optimization algorithm and this paper primarily
emphasizes the later one. We have used MiniZinc modeling language to define our
system as a constraint satisfaction problem which can be solved using any
off-the-shelf solver. The proposed system performs much better than the systems
currently in use. Our system reduces the mean waiting time and standard
deviation of the waiting time of vehicles and avoids deadlocks.",Automated Intersection Management with MiniZinc,2020
"Qiang Shen, Stefano Teso, Wanyi Zhang, Hao Xu, Fausto Giunchiglia",AI,2020,"Applications like personal assistants need to be aware ofthe user's context,
e.g., where they are, what they are doing, and with whom. Context information
is usually inferred from sensor data, like GPS sensors and accelerometers on
the user's smartphone. This prediction task is known as context recognition. A
well-defined context model is fundamental for successful recognition. Existing
models, however, have two major limitations. First, they focus on few aspects,
like location or activity, meaning that recognition methods based onthem can
only compute and leverage few inter-aspect correlations. Second, existing
models typically assume that context is objective, whereas in most applications
context is best viewed from the user's perspective. Neglecting these factors
limits the usefulness of the context model and hinders recognition. We present
a novel ontological context model that captures five dimensions, namely time,
location, activity, social relations and object. Moreover, our model defines
three levels of description(objective context, machine context and subjective
context) that naturally support subjective annotations and reasoning.An initial
context recognition experiment on real-world data hints at the promise of our
model.",Multi-Modal Subjective Context Modelling and Recognition,2020
"He Zhu, Dianbo Liu",AI,2020,"The concept of disinformation is to use fake messages to confuse people in
order to protect the real information. This strategy can be adapted into data
science to protect valuable private and sensitive data. Huge amount of private
data are being generated from personal devices such as smart phone and wearable
in recent years. Being able to utilize these personal data will bring big
opportunities to design personalized products, conduct precision healthcare and
many other tasks that were impossible in the past. However, due to privacy,
safety and regulation reasons, it is often difficult to transfer or store data
in its original form while keeping them safe. Building a secure data transfer
and storage infrastructure to preserving privacy is costly in most cases and
there is always a concern of data security due to human errors. In this study,
we propose a method, named FakeSafe, to provide human level data protection
using generative adversarial network with cycle consistency and conducted
experiments using both benchmark and real world data sets to illustrate
potential applications of FakeSafe.",FakeSafe: Human Level Data Protection by Disinformation Mapping using Cycle-consistent Adversarial Network,2020
"Giuseppe Fenza, Mariacristina Gallo, Vincenzo Loia, Domenico Marinoand Francesco Orciuoli, Alberto Volpe",AI,2020,"Cyber-Physical Systems (CPS) play a crucial role in the era of the
4thIndustrial Revolution. Recently, the application of the CPS to industrial
manufacturing leads to a specialization of them referred as Cyber-Physical
Production Systems (CPPS). Among other challenges, CPS and CPPS should be able
to address interoperability issues, since one of their intrinsic requirement is
the capability to interface and cooperate with other systems. On the other
hand, to fully realize theIndustry 4.0 vision, it is required to address
horizontal, vertical, and end-to-end integration enabling a complete awareness
through the entire supply chain. In this context, Semantic Web standards and
technologies may have a promising role to represent manufacturing knowledge in
a machine-interpretable way for enabling communications among heterogeneous
Industrial assets. This paper proposes an integration of Semantic Web models
available at state of the art for implementing a5C architecture mainly targeted
to collect and process semantic data stream in a way that would unlock the
potentiality of data yield in a smart manufacturing environment. The analysis
of key industrial ontologies and semantic technologies allows us to instantiate
an example scenario for monitoring Overall Equipment Effectiveness(OEE). The
solution uses the SOSA ontology for representing the semantic datastream. Then,
C-SPARQL queries are defined for periodically carrying out useful KPIs to
address the proposed aim.",Semantic CPPS in Industry 4.0,2020
"Rustem Ozakar, Rafet Efe Gazanfer, Y. Sinan Hanay",AI,2020,"In this work, we analyze the happiness levels of countries using an unbiased
emotion detector, artificial intelligence (AI). To date, researchers proposed
many factors that may affect happiness such as wealth, health and safety. Even
though these factors all seem relevant, there is no clear consensus between
sociologists on how to interpret these, and the models to estimate the cost of
these utilities include some assumptions. Researchers in social sciences have
been working on determination of the happiness levels in society and
exploration of the factors correlated with it through polls and different
statistical methods. In our work, by using artificial intelligence, we
introduce a different and relatively unbiased approach to this problem. By
using AI, we make no assumption about what makes a person happy, and leave the
decision to AI to detect the emotions from the faces of people collected from
publicly available street footages. We analyzed the happiness levels in eight
different cities around the world through available footage on the Internet and
found out that there is no statistically significant difference between
countries in terms of happiness.",Measuring Happiness Around the World Through Artificial Intelligence,2020
"Thomas P. Quinn, Stephan Jacobs, Manisha Senadeera, Vuong Le, Simon Coghlan",AI,2020,"Our title alludes to the three Christmas ghosts encountered by Ebenezer
Scrooge in \textit{A Christmas Carol}, who guide Ebenezer through the past,
present, and future of Christmas holiday events. Similarly, our article will
take readers through a journey of the past, present, and future of medical AI.
In doing so, we focus on the crux of modern machine learning: the reliance on
powerful but intrinsically opaque models. When applied to the healthcare
domain, these models fail to meet the needs for transparency that their
clinician and patient end-users require. We review the implications of this
failure, and argue that opaque models (1) lack quality assurance, (2) fail to
elicit trust, and (3) restrict physician-patient dialogue. We then discuss how
upholding transparency in all aspects of model design and model validation can
help ensure the reliability of medical AI.",The Three Ghosts of Medical AI: Can the Black-Box Present Deliver?,2020
"Ali Yazdizadeh, Bilal Farooq",AI,2020,"Ontology is the explicit and formal representation of the concepts in a
domain and relations among them. Transportation science is a wide domain
dealing with mobility over various complex and interconnected transportation
systems, such as land, aviation, and maritime transport, and can take
considerable advantage from ontology development. While several studies can be
found in the recent literature, there exists a large potential to improve and
develop a comprehensive smart mobility ontology. The current chapter aims to
present different aspects of ontology development in general, such as ontology
development methods, languages, tools, and software. Subsequently, it presents
the currently available mobility-related ontologies developed across different
domains, such as transportation, smart cities, goods mobility, sensors. Current
gaps in the available ontologies are identified, and future directions
regarding ontology development are proposed that can incorporate the
forthcoming autonomous and connected vehicles, mobility as a service (MaaS),
and other disruptive transportation technologies and services.",Smart Mobility Ontology: Current Trends and Future Directions,2020
"Joyjit Chatterjee, Nina Dethlefs",AI,2020,"Condition-based monitoring (CBM) has been widely utilised in the wind
industry for monitoring operational inconsistencies and failures in turbines,
with techniques ranging from signal processing and vibration analysis to
artificial intelligence (AI) models using Supervisory Control & Acquisition
(SCADA) data. However, existing studies do not present a concrete basis to
facilitate explainable decision support in operations and maintenance (O&M),
particularly for automated decision support through recommendation of
appropriate maintenance action reports corresponding to failures predicted by
CBM techniques. Knowledge graph databases (KGs) model a collection of
domain-specific information and have played an intrinsic role for real-world
decision support in domains such as healthcare and finance, but have seen very
limited attention in the wind industry. We propose XAI4Wind, a multimodal
knowledge graph for explainable decision support in real-world operational
turbines and demonstrate through experiments several use-cases of the proposed
KG towards O&M planning through interactive query and reasoning and providing
novel insights using graph data science algorithms. The proposed KG combines
multimodal knowledge like SCADA parameters and alarms with natural language
maintenance actions, images etc. By integrating our KG with an Explainable AI
model for anomaly prediction, we show that it can provide effective
human-intelligible O&M strategies for predicted operational inconsistencies in
various turbine sub-components. This can help instil better trust and
confidence in conventionally black-box AI models. We make our KG publicly
available and envisage that it can serve as the building ground for providing
autonomous decision support in the wind industry.",XAI4Wind: A Multimodal Knowledge Graph Database for Explainable Decision Support in Operations & Maintenance of Wind Turbines,2021
Mohammad Reza Davahli,AI,2020,"Artificial intelligence (AI) has been used to advance different fields, such
as education, healthcare, and finance. However, the application of AI in the
field of project management (PM) has not progressed equally. This paper reports
on a systematic review of the published studies used to investigate the
application of AI in PM. This systematic review identified relevant papers
using Web of Science, Science Direct, and Google Scholar databases. Of the 652
articles found, 58 met the predefined criteria and were included in the review.
Included papers were classified per the following dimensions: PM knowledge
areas, PM processes, and AI techniques. The results indicated that the
application of AI in PM was in its early stages and AI models have not applied
for multiple PM processes especially in processes groups of project stakeholder
management, project procurements management, and project communication
management. However, the most popular PM processes among included papers were
project effort prediction and cost estimation, and the most popular AI
techniques were support vector machines, neural networks, and genetic
algorithms.",The Last State of Artificial Intelligence in Project Management,2020
"Yankai Chen, Yaozu Wu, Shicheng Ma, Irwin King",AI,2021,"With the rapid development of biomedical software and hardware, a large
amount of relational data interlinking genes, proteins, chemical components,
drugs, diseases, and symptoms has been collected for modern biomedical
research. Many graph-based learning methods have been proposed to analyze such
type of data, giving a deeper insight into the topology and knowledge behind
the biomedical data, which greatly benefit to both academic research and
industrial application for human healthcare. However, the main difficulty is
how to handle high dimensionality and sparsity of the biomedical graphs.
Recently, graph embedding methods provide an effective and efficient way to
address the above issues. It converts graph-based data into a low dimensional
vector space where the graph structural properties and knowledge information
are well preserved. In this survey, we conduct a literature review of recent
developments and trends in applying graph embedding methods for biomedical
data. We also introduce important applications and tasks in the biomedical
domain as well as associated public biomedical datasets.",A Literature Review of Recent Graph Embedding Techniques for Biomedical Data,2021
"Honglin Li, Roonak Rezvani, Magdalena Anita Kolanko, David J. Sharp, Maitreyee Wairagkar, Ravi Vaidyanathan, Ramin Nilforooshan, Payam Barnaghi",AI,2021,"Behavioural symptoms and urinary tract infections (UTI) are among the most
common problems faced by people with dementia. One of the key challenges in the
management of these conditions is early detection and timely intervention in
order to reduce distress and avoid unplanned hospital admissions. Using in-home
sensing technologies and machine learning models for sensor data integration
and analysis provides opportunities to detect and predict clinically
significant events and changes in health status. We have developed an
integrated platform to collect in-home sensor data and performed an
observational study to apply machine learning models for agitation and UTI risk
analysis. We collected a large dataset from 88 participants with a mean age of
82 and a standard deviation of 6.5 (47 females and 41 males) to evaluate a new
deep learning model that utilises attention and rational mechanism. The
proposed solution can process a large volume of data over a period of time and
extract significant patterns in a time-series data (i.e. attention) and use the
extracted features and patterns to train risk analysis models (i.e. rational).
The proposed model can explain the predictions by indicating which time-steps
and features are used in a long series of time-series data. The model provides
a recall of 91\% and precision of 83\% in detecting the risk of agitation and
UTIs. This model can be used for early detection of conditions such as UTIs and
managing of neuropsychiatric symptoms such as agitation in association with
initial treatment and early intervention approaches. In our study we have
developed a set of clinical pathways for early interventions using the alerts
generated by the proposed model and a clinical monitoring team has been set up
to use the platform and respond to the alerts according to the created
intervention plans.",An attention model to analyse the risk of agitation and urinary tract infections in people with dementia,2021
"Zijian Zhang, Jaspreet Singh, Ujwal Gadiraju, Avishek Anand",AI,2021,"Complex machine learning models are deployed in several critical domains
including healthcare and autonomous vehicles nowadays, albeit as functional
black boxes. Consequently, there has been a recent surge in interpreting
decisions of such complex models in order to explain their actions to humans.
Models that correspond to human interpretation of a task are more desirable in
certain contexts and can help attribute liability, build trust, expose biases
and in turn build better models. It is, therefore, crucial to understand how
and which models conform to human understanding of tasks. In this paper, we
present a large-scale crowdsourcing study that reveals and quantifies the
dissonance between human and machine understanding, through the lens of an
image classification task. In particular, we seek to answer the following
questions: Which (well-performing) complex ML models are closer to humans in
their use of features to make accurate predictions? How does task difficulty
affect the feature selection capability of machines in comparison to humans?
Are humans consistently better at selecting features that make image
recognition more accurate? Our findings have important implications on
human-machine collaboration, considering that a long term goal in the field of
artificial intelligence is to make machines capable of learning and reasoning
like humans.",Dissonance Between Human and Machine Understanding,2021
"Niels Leadholm, Marcus Lewis, Subutai Ahmad",AI,2021,"Grid cells enable the brain to model the physical space of the world and
navigate effectively via path integration, updating self-position using
information from self-movement. Recent proposals suggest that the brain might
use similar mechanisms to understand the structure of objects in diverse
sensory modalities, including vision. In machine vision, object recognition
given a sequence of sensory samples of an image, such as saccades, is a
challenging problem when the sequence does not follow a consistent, fixed
pattern - yet this is something humans do naturally and effortlessly. We
explore how grid cell-based path integration in a cortical network can support
reliable recognition of objects given an arbitrary sequence of inputs. Our
network (GridCellNet) uses grid cell computations to integrate visual
information and make predictions based on movements. We use local Hebbian
plasticity rules to learn rapidly from a handful of examples (few-shot
learning), and consider the task of recognizing MNIST digits given only a
sequence of image feature patches. We compare GridCellNet to k-Nearest
Neighbour (k-NN) classifiers as well as recurrent neural networks (RNNs), both
of which lack explicit mechanisms for handling arbitrary sequences of input
samples. We show that GridCellNet can reliably perform classification,
generalizing to both unseen examples and completely novel sequence
trajectories. We further show that inference is often successful after sampling
a fraction of the input space, enabling the predictive GridCellNet to
reconstruct the rest of the image given just a few movements. We propose that
dynamically moving agents with active sensors can use grid cell representations
not only for navigation, but also for efficient recognition and feature
prediction of seen objects.",Grid Cell Path Integration For Movement-Based Visual Object Recognition,2021
"Tadahiro Taniguchi, Hiroshi Yamakawa, Takayuki Nagai, Kenji Doya, Masamichi Sakagami, Masahiro Suzuki, Tomoaki Nakamura, Akira Taniguchi",AI,2021,"Building a humanlike integrative artificial cognitive system, that is, an
artificial general intelligence (AGI), is the holy grail of the artificial
intelligence (AI) field. Furthermore, a computational model that enables an
artificial system to achieve cognitive development will be an excellent
reference for brain and cognitive science. This paper describes an approach to
develop a cognitive architecture by integrating elemental cognitive modules to
enable the training of the modules as a whole. This approach is based on two
ideas: (1) brain-inspired AI, learning human brain architecture to build
human-level intelligence, and (2) a probabilistic generative model(PGM)-based
cognitive system to develop a cognitive system for developmental robots by
integrating PGMs. The development framework is called a whole brain PGM
(WB-PGM), which differs fundamentally from existing cognitive architectures in
that it can learn continuously through a system based on sensory-motor
information. In this study, we describe the rationale of WB-PGM, the current
status of PGM-based elemental cognitive modules, their relationship with the
human brain, the approach to the integration of the cognitive modules, and
future challenges. Our findings can serve as a reference for brain studies. As
PGMs describe explicit informational relationships between variables, this
description provides interpretable guidance from computational sciences to
brain science. By providing such information, researchers in neuroscience can
provide feedback to researchers in AI and robotics on what the current models
lack with reference to the brain. Further, it can facilitate collaboration
among researchers in neuro-cognitive sciences as well as AI and robotics.",A Whole Brain Probabilistic Generative Model: Toward Realizing Cognitive Architectures for Developmental Robots,2022
"Avner Hatsek, Irit Hochberg, Deeb Daoud Naccache, Aya Biderman, Yuval Shahar",AI,2021,"We evaluated the DiscovErr system, in which we had previously implemented a
new methodology for assessment of compliance to continuous application of
evidence-based clinical guidelines, based on a bidirectional search from the
guideline objectives to the patient's longitudinal data, and vice versa. We
compared the system comments on 1584 transactions regarding the management,
over a mean of 5.23 years, of 10 randomly selected Type 2 diabetes patients, to
those of two diabetes experts and a senior family practitioner. After providing
their own comments, the experts assessed both the correctness (precision) and
the importance of each of the DiscovErr system comments. The completeness
(recall or coverage) of the system was computed by comparing its comments to
those made by the experts. The system made 279 comments. The experts made 181
unique comments. The completeness of the system was 91% compared to comments
made by at least two experts, and 98% when compared to comments made by all
three. 172 comments were evaluated by the experts for correctness and
importance: All 114 medication-related comments, and a random 35% of the 165
monitoring-related comments. The system's correctness was 81% compared to
comments judged as correct by both diabetes experts, and 91% compared to
comments judged as correct by a diabetes expert and at least as partially
correct by the other. 89% of the comments were judged as important by both
diabetes experts, 8% were judged as important by one expert, 3% were judged as
less important by both experts. The completeness scores of the three experts
(compared to the comments of all experts plus the validated system comments)
were 75%, 60%, and 55%; the experts' correctness scores (compared to their
majority) were respectively 99%, 91%, and 88%. Conclusion: Systems such as
DiscovErr can assess the quality of continuous guideline-based care.","Evaluation of a Bi-Directional Methodology for Automated Assessment of Compliance to Continuous Application of Clinical Guidelines, in the Type 2 Diabetes-Management Domain",2021
"Usha Lokala, Francois Lamy, Triyasha Ghosh Dastidar, Kaushik Roy, Raminta Daniulaityte, Srinivasan Parthasarathy, Amit Sheth",AI,2021,"Opioid and substance misuse is rampant in the United States today, with the
phenomenon known as the opioid crisis. The relationship between substance use
and mental health has been extensively studied, with one possible relationship
being substance misuse causes poor mental health. However, the lack of evidence
on the relationship has resulted in opioids being largely inaccessible through
legal means. This study analyzes the substance misuse posts on social media
with the opioids being sold through crypto market listings. We use the Drug
Abuse Ontology, state-of-the-art deep learning, and BERT-based models to
generate sentiment and emotion for the social media posts to understand user
perception on social media by investigating questions such as, which synthetic
opioids people are optimistic, neutral, or negative about or what kind of drugs
induced fear and sorrow or what kind of drugs people love or thankful about or
which drug people think negatively about or which opioids cause little to no
sentimental reaction. We also perform topic analysis associated with the
generated sentiments and emotions to understand which topics correlate with
people's responses to various drugs. Our findings can help shape policy to help
isolate opioid use cases where timely intervention may be required to prevent
adverse consequences, prevent overdose-related deaths, and worsen the epidemic.",eDarkTrends: Harnessing Social Media Trends in Substance use disorders for Opioid Listings on Cryptomarket,2021
"Özgür Dogan, Oguzhan Sahin, Enis Karaarslan",AI,2021,"The damage and the impact of natural disasters are becoming more destructive
with the increase of urbanization. Today's metropolitan cities are not
sufficiently prepared for the pre and post-disaster situations. Digital Twin
technology can provide a solution. A virtual copy of the physical city could be
created by collecting data from sensors of the Internet of Things (IoT) devices
and stored on the cloud infrastructure. This virtual copy is kept current and
up to date with the continuous flow of the data coming from the sensors. We
propose a disaster management system utilizing machine learning called DT-DMS
is used to support decision-making mechanisms. This study aims to show how to
educate and prepare emergency center staff by simulating potential disaster
situations on the virtual copy. The event of a disaster will be simulated
allowing emergency center staff to make decisions and depicting the potential
outcomes of these decisions. A rescue operation after an earthquake is
simulated. Test results are promising and the simulation scope is planned to be
extended.",Digital Twin Based Disaster Management System Proposal: DT-DMS,2021
"Martin Käppel, Stefan Jablonski, Stefan Schönig",AI,2021,"Predictive business process monitoring is concerned with the prediction how a
running process instance will unfold up to its completion at runtime. Most of
the proposed approaches rely on a wide number of different machine learning
(ML) techniques. In the last years numerous comparative studies, reviews, and
benchmarks of such approaches where published and revealed that they can be
successfully applied for different prediction targets. ML techniques require a
qualitatively and quantitatively sufficient data set. However, there are many
situations in business process management (BPM) where only a quantitatively
insufficient data set is available. The problem of insufficient data in the
context of BPM is still neglected. Hence, none of the comparative studies or
benchmarks investigates the performance of predictive business process
monitoring techniques in environments with small data sets. In this paper an
evaluation framework for comparing existing approaches with regard to their
suitability for small data sets is developed and exemplarily applied to
state-of-the-art approaches in predictive business process monitoring.",Evaluating Predictive Business Process Monitoring Approaches on Small Event Logs,2021
"Jessica Morley, Caroline Morton, Kassandra Karpathakis, Mariarosaria Taddeo, Luciano Floridi",AI,2021,"The potential presented by Artificial Intelligence (AI) for healthcare has
long been recognised by the technical community. More recently, this potential
has been recognised by policymakers, resulting in considerable public and
private investment in the development of AI for healthcare across the globe.
Despite this, excepting limited success stories, real-world implementation of
AI systems into front-line healthcare has been limited. There are numerous
reasons for this, but a main contributory factor is the lack of internationally
accepted, or formalised, regulatory standards to assess AI safety and impact
and effectiveness. This is a well-recognised problem with numerous ongoing
research and policy projects to overcome it. Our intention here is to
contribute to this problem-solving effort by seeking to set out a minimally
viable framework for evaluating the safety, acceptability and efficacy of AI
systems for healthcare. We do this by conducting a systematic search across
Scopus, PubMed and Google Scholar to identify all the relevant literature
published between January 1970 and November 2020 related to the evaluation of:
output performance; efficacy; and real-world use of AI systems, and
synthesising the key themes according to the stages of evaluation: pre-clinical
(theoretical phase); exploratory phase; definitive phase; and post-market
surveillance phase (monitoring). The result is a framework to guide AI system
developers, policymakers, and regulators through a sufficient evaluation of an
AI system designed for use in healthcare.","Towards a framework for evaluating the safety, acceptability and efficacy of AI systems for health: an initial synthesis",2021
"Sola Shirai, Oshani Seneviratne, Deborah L. McGuinness",AI,2021,"Knowledge graphs that encapsulate personal health information, or personal
health knowledge graphs (PHKG), can help enable personalized health care in
knowledge-driven systems. In this paper we provide a short survey of existing
work surrounding the emerging paradigm of PHKGs and highlight the major
challenges that remain. We find that while some preliminary exploration exists
on the topic of personal knowledge graphs, development of PHKGs remains
under-explored. A range of challenges surrounding the collection, linkage, and
maintenance of personal health knowledge remains to be addressed to fully
realize PHKGs.",Applying Personal Knowledge Graphs to Health,2021
"Michael Anis Mihdi Afnan, Cynthia Rudin, Vincent Conitzer, Julian Savulescu, Abhishek Mishra, Yanhe Liu, Masoud Afnan",AI,2021,"AI has the potential to revolutionize many areas of healthcare. Radiology,
dermatology, and ophthalmology are some of the areas most likely to be impacted
in the near future, and they have received significant attention from the
broader research community. But AI techniques are now also starting to be used
in in vitro fertilization (IVF), in particular for selecting which embryos to
transfer to the woman. The contribution of AI to IVF is potentially
significant, but must be done carefully and transparently, as the ethical
issues are significant, in part because this field involves creating new
people. We first give a brief introduction to IVF and review the use of AI for
embryo selection. We discuss concerns with the interpretation of the reported
results from scientific and practical perspectives. We then consider the
broader ethical issues involved. We discuss in detail the problems that result
from the use of black-box methods in this context and advocate strongly for the
use of interpretable models. Importantly, there have been no published trials
of clinical effectiveness, a problem in both the AI and IVF communities, and we
therefore argue that clinical implementation at this point would be premature.
Finally, we discuss ways for the broader AI community to become involved to
ensure scientifically sound and ethically responsible development of AI in IVF.",Ethical Implementation of Artificial Intelligence to Select Embryos in In Vitro Fertilization,2021
"Harish Panneer Selvam, Yan Li, Pengyue Wang, William F. Northrop, Shashi Shekhar",AI,2021,"Given an on-board diagnostics (OBD) dataset and a physics-based emissions
prediction model, this paper aims to develop an accurate and
computational-efficient AI (Artificial Intelligence) method that predicts
vehicle emissions. The problem is of societal importance because vehicular
emissions lead to climate change and impact human health. This problem is
challenging because the OBD data does not contain enough parameters needed by
high-order physics models. Conversely, related work has shown that low-order
physics models have poor predictive accuracy when using available OBD data.
This paper uses a divergent window co-occurrence pattern detection method to
develop a spatiotemporal variability-aware AI model for predicting emission
values from the OBD datasets. We conducted a case study using real-world OBD
data from a local public transportation agency. Results show that the proposed
AI method has approximately 65% improved predictive accuracy than a non-AI
low-order physics model and is approximately 35% more accurate than a baseline
model.",Vehicle Emissions Prediction with Physics-Aware AI Models: Preliminary Results,2021
"Ishita Padhiar, Oshani Seneviratne, Shruthi Chari, Daniel Gruen, Deborah L. McGuinness",AI,2021,"With the increased use of AI methods to provide recommendations in the
health, specifically in the food dietary recommendation space, there is also an
increased need for explainability of those recommendations. Such explanations
would benefit users of recommendation systems by empowering them with
justifications for following the system's suggestions. We present the Food
Explanation Ontology (FEO) that provides a formalism for modeling explanations
to users for food-related recommendations. FEO models food recommendations,
using concepts from the explanation domain to create responses to user
questions about food recommendations they receive from AI systems such as
personalized knowledge base question answering systems. FEO uses a modular,
extensible structure that lends itself to a variety of explanations while still
preserving important semantic details to accurately represent explanations of
food recommendations. In order to evaluate this system, we used a set of
competency questions derived from explanation types present in literature that
are relevant to food recommendations. Our motivation with the use of FEO is to
empower users to make decisions about their health, fully equipped with an
understanding of the AI recommender systems as they relate to user questions,
by providing reasoning behind their recommendations in the form of
explanations.",Semantic Modeling for Food Recommendation Explanations,2021
"Florian Stertz, Juergen Mangler, Stefanie Rinderle-Ma",AI,2021,"Process mining has matured as analysis instrument for process-oriented data
in recent years. Manufacturing is a challenging domain that craves for
process-oriented technologies to address digitalization challenges. We found
that process mining creates high expectations, but its implementation and usage
by manufacturing experts such as process supervisors and shopfloor workers
remain unclear to a certain extent. Reason (1) is that even though
manufacturing allows for well-structured processes, the actual workflow is
rarely captured in a process model. Even if a model is available, a software
for orchestrating and logging the execution is often missing. Reason (2) refers
to the work reality in manufacturing: a process instance is started by a
shopfloor worker who then turns to work on other things. Hence continuous
monitoring of the process instances does not happen, i.e., process monitoring
is merely a secondary task, and the shopfloor worker can only react to
problems/errors that have already occurred. (1) and (2) motivate the goals of
this study that is driven by Technical Action Research (TAR). Based on the
experimental artifact TIDATE -- a lightweight process execution and mining
framework -- it is studied how the correct execution of process instances can
be ensured and how a data set suitable for process mining can be generated at
run time in a real-world setting. Secondly, it is investigated whether and how
process mining supports domain experts during process monitoring as a secondary
task. The findings emphasize the importance of online conformance checking in
manufacturing and show how appropriate data sets can be identified and
generated.",The Role of Time and Data: Online Conformance Checking in the Manufacturing Domain,2021
"Gauthier Chassang, Mogens Thomsen, Pierre Rumeau, Florence Sèdes, Alejandra Delfin",AI,2021,"This paper proposes a comprehensive analysis of existing concepts coming from
different disciplines tackling the notion of intelligence, namely psychology
and engineering, and from disciplines aiming to regulate AI innovations, namely
AI ethics and law. The aim is to identify shared notions or discrepancies to
consider for qualifying AI systems. Relevant concepts are integrated into a
matrix intended to help defining more precisely when and how computing tools
(programs or devices) may be qualified as AI while highlighting critical
features to serve a specific technical, ethical and legal assessment of
challenges in AI development. Some adaptations of existing notions of AI
characteristics are proposed. The matrix is a risk-based conceptual model
designed to allow an empirical, flexible and scalable qualification of AI
technologies in the perspective of benefit-risk assessment practices,
technological monitoring and regulatory compliance: it offers a structured
reflection tool for stakeholders in AI development that are engaged in
responsible research and innovation.Pre-print version (achieved on May 2020)",An interdisciplinary conceptual study of Artificial Intelligence (AI) for helping benefit-risk assessment practices: Towards a comprehensive qualification matrix of AI programs and devices (pre-print 2020),2021
"Yingbo Li, Yucong Duan, Anamaria-Beatrice Spulber, Haoyang Che, Zakaria Maamar, Zhao Li, Chen Yang, Yu lei",AI,2021,"Artificial Intelligence has been a growth catalyst to our society and is
cosidered across all idustries as a fundamental technology. However, its
development has been limited to the signal processing domain that relies on the
generated and collected data from other sensors. In recent research, concepts
of Digital Artificial Intelligence and Physicial Artifical Intelligence have
emerged and this can be considered a big step in the theoretical development of
Artifical Intelligence. In this paper we explore the concept of Physicial
Artifical Intelligence and propose two subdomains: Integrated Physicial
Artifical Intelligence and Distributed Physicial Artifical Intelligence. The
paper will also examine the trend and governance of Physicial Artifical
Intelligence.",Physical Artificial Intelligence: The Concept Expansion of Next-Generation Artificial Intelligence,2021
"Vaishali Mahipal, Mohammad Arif Ul Alam",AI,2021,"Drug overdose has become a public health crisis in the United States with
devastating consequences. However, most of the drug overdose incidences are the
consequence of recitative polysubstance usage over a defined period of time
which can be happened by either the intentional usage of required drug with
other drugs or by accident. Thus, predicting the effects of polysubstance usage
is extremely important for clinicians to decide which combination of drugs
should be prescribed. Recent advancement of structural causal models can
provide ample insights of causal effects from observational data via
identifiable causal directed graphs. In this paper, we propose a system to
estimate heterogeneous concurrent drug usage effects on overdose estimation,
that consists of efficient co-variate selection, sub-group selection and
heterogeneous causal effect estimation. We apply our framework to answer a
critical question, can concurrent usage of benzodiazepines and opioids have
heterogeneous causal effects on the opioid overdose epidemic? Using Truven
MarketScan claim data collected from 2001 to 2013 have shown significant
promise of our proposed framework's efficacy.",Estimating Heterogeneous Causal Effect of Polysubstance Usage on Drug Overdose from Large-Scale Electronic Health Record,2022
"Tejas Gaikwad, Romi Banerjee",AI,2021,"Biological infants are naturally curious and try to comprehend their physical
surroundings by interacting, in myriad multisensory ways, with different
objects - primarily macroscopic solid objects - around them. Through their
various interactions, they build hypotheses and predictions, and eventually
learn, infer and understand the nature of the physical characteristics and
behavior of these objects. Inspired thus, we propose a model for
curiosity-driven learning and inference for real-world AI agents. This model is
based on the arousal of curiosity, deriving from observations along
discontinuities in the fundamental macroscopic solid-body physics parameters,
i.e., shape constancy, spatial-temporal continuity, and object permanence. We
use the term body-budget to represent the perceived fundamental properties of
solid objects. The model aims to support the emulation of learning from scratch
followed by substantiation through experience, irrespective of domain, in
real-world AI agents.",Curiosity-driven Intuitive Physics Learning,2021
"Abderrahim Fathan, Erick Delage",AI,2021,"Optimal stopping is the problem of deciding the right time at which to take a
particular action in a stochastic system, in order to maximize an expected
reward. It has many applications in areas such as finance, healthcare, and
statistics. In this paper, we employ deep Reinforcement Learning (RL) to learn
optimal stopping policies in two financial engineering applications: namely
option pricing, and optimal option exercise. We present for the first time a
comprehensive empirical evaluation of the quality of optimal stopping policies
identified by three state of the art deep RL algorithms: double deep Q-learning
(DDQN), categorical distributional RL (C51), and Implicit Quantile Networks
(IQN). In the case of option pricing, our findings indicate that in a
theoretical Black-Schole environment, IQN successfully identifies nearly
optimal prices. On the other hand, it is slightly outperformed by C51 when
confronted to real stock data movements in a put option exercise problem that
involves assets from the S&P500 index. More importantly, the C51 algorithm is
able to identify an optimal stopping policy that achieves 8% more out-of-sample
returns than the best of four natural benchmark policies. We conclude with a
discussion of our findings which should pave the way for relevant future
research.",Deep Reinforcement Learning for Optimal Stopping with Application in Financial Engineering,2021
"Ethan Lim Ding Feng, Zhi-Wei Neo, Aaron William De Silva, Kellie Sim, Hong-Ray Tan, Thi-Thanh Nguyen, Karen Wei Ling Koh, Wenru Wang, Hoang D. Nguyen",AI,2021,"With the rapid development in artificial intelligence, social computing has
evolved beyond social informatics toward the birth of social intelligence
systems. This paper, therefore, takes initiatives to propose a social behaviour
understanding framework with the use of deep neural networks for social and
behavioural analysis. The integration of information fusion, person and object
detection, social signal understanding, behaviour understanding, and context
understanding plays a harmonious role to elicit social behaviours. Three
systems, including depression detection, activity recognition and cognitive
impairment screening, are developed to evidently demonstrate the importance of
social intelligence. The study considerably contributes to the cumulative
development of social computing and health informatics. It also provides a
number of implications for academic bodies, healthcare practitioners, and
developers of socially intelligent agents.",Social Behaviour Understanding using Deep Neural Networks: Development of Social Intelligence Systems,2021
"Tatiana Tommasi, Silvia Bucci, Barbara Caputo, Pietro Asinari",AI,2021,"Thanks to the great progress of machine learning in the last years, several
Artificial Intelligence (AI) techniques have been increasingly moving from the
controlled research laboratory settings to our everyday life. AI is clearly
supportive in many decision-making scenarios, but when it comes to sensitive
areas such as health care, hiring policies, education, banking or justice, with
major impact on individuals and society, it becomes crucial to establish
guidelines on how to design, develop, deploy and monitor this technology.
Indeed the decision rules elaborated by machine learning models are data-driven
and there are multiple ways in which discriminatory biases can seep into data.
Algorithms trained on those data incur the risk of amplifying prejudices and
societal stereotypes by over associating protected attributes such as gender,
ethnicity or disabilities with the prediction task. Starting from the extensive
experience of the National Metrology Institute on measurement standards and
certification roadmaps, and of Politecnico di Torino on machine learning as
well as methods for domain bias evaluation and mastering, we propose a first
joint effort to define the operational steps needed for AI fairness
certification. Specifically we will overview the criteria that should be met by
an AI system before coming into official service and the conformity assessment
procedures useful to monitor its functioning for fair decisions.",Towards Fairness Certification in Artificial Intelligence,2021
"Izack Cohen, Avigdor Gal",AI,2021,"Motivated by the abundance of uncertain event data from multiple sources
including physical devices and sensors, this paper presents the task of
relating a stochastic process observation to a process model that can be
rendered from a dataset. In contrast to previous research that suggested to
transform a stochastically known event log into a less informative uncertain
log with upper and lower bounds on activity frequencies, we consider the
challenge of accommodating the probabilistic knowledge into conformance
checking techniques. Based on a taxonomy that captures the spectrum of
conformance checking cases under stochastic process observations, we present
three types of challenging cases. The first includes conformance checking of a
stochastically known log with respect to a given process model. The second case
extends the first to classify a stochastically known log into one of several
process models. The third case extends the two previous ones into settings in
which process models are only stochastically known. The suggested problem
captures the increasingly growing number of applications in which sensors
provide probabilistic process information.",Uncertain Process Data with Probabilistic Knowledge: Problem Characterization and Challenges,2021
Niklas Muennighoff,AI,2021,"Artificial Intelligence will significantly impact the work environment of
radiologists. I suggest that up to 50% of a radiologists work in 2021 will be
performed by AI-models in 2025. However, it won't increase beyond that 50%
level, as radiologists remain key for human-centered aspects of their job. I
project that few to no radiologists will be laid off in China due to the
existing supply shortage of radiology services in 2021. The application of AI
in radiology could contribute 1.7 billion USD to China's GDP in 2025. It will
further allow radiologists to start productive work up to four years earlier.
AI in radiology will positively impact the health of patients and radiologists
themselves.",Diagnosing the Impact of AI on Radiology in China,2021
"Ricardo Ferreira, Carolina Lopes, Ricardo Gonçalves, Matthias Knorr, Ludwig Krippahl, João Leite",AI,2021,"The amount of information produced, whether by newspapers, blogs and social
networks, or by monitoring systems, is increasing rapidly. Processing all this
data in real-time, while taking into consideration advanced knowledge about the
problem domain, is challenging, but required in scenarios where assessing
potential risks in a timely fashion is critical. C-SPARQL, a language for
continuous queries over streams of RDF data, is one of the more prominent
approaches in stream reasoning that provides such continuous inference
capabilities over dynamic data that go beyond mere stream processing. However,
it has been shown that, in the presence of huge amounts of data, C-SPARQL may
not be able to answer queries in time, in particular when the frequency of
incoming data is higher than the time required for reasoning with that data. In
this paper, we investigate whether reasoning with C-SPARQL can be approximated
using Recurrent Neural Networks and Convolutional Neural Networks, two neural
network architectures that have been shown to be well-suited for time series
forecasting and time series classification, to leverage on their higher
processing speed once the network has been trained. We consider a variety of
different kinds of queries and obtain overall positive results with high
accuracies while improving processing time often by several orders of
magnitude.",Deep Neural Networks for Approximating Stream Reasoning with C-SPARQL,2021
"Haile Misgna, Moges Ahmed, Anubhav Kumar",AI,2021,"The solution to prevent maternal complications are known and preventable by
trained health professionals. But in countries like Ethiopia where the patient
to physician ratio is 1 doctor to 1000 patients, maternal mortality and
morbidity rate is high. To fill the gap of highly trained health professionals,
Ethiopia introduced health extension programs. Task shifting to health
extension workers (HEWs) contributed in decreasing mortality and morbidity rate
in Ethiopia. Knowledge-gap has been one of the major challenges to HEWs. The
reasons are trainings are not given in regular manner, there is no midwife,
gynecologists or doctors around for consultation, and all guidelines are
paper-based which are easily exposed to damage. In this paper, we describe the
design and implementation of a web-based expert system for maternal care. We
only targeted the major 10 diseases and complication of maternal health issues
seen in Sub-Saharan Africa. The expert system can be accessed through the use
of web browsers from computers as well as smart phones. Forward chaining
rule-based expert system is used in order to give suggestions and create a new
knowledge from the knowledge-base. This expert system can be used to train HEWs
in the field of maternal health.
  Keywords: expert system, maternal care, forward-chaining, rule-based expert
system, PHLIPS",MatES: Web-based Forward Chaining Expert System for Maternal Care,2021
"Hans Weytjens, Jochen De Weerdt",AI,2021,"Advances in AI, and especially machine learning, are increasingly drawing
research interest and efforts towards predictive process monitoring, the
subfield of process mining (PM) that concerns predicting next events, process
outcomes and remaining execution times. Unfortunately, researchers use a
variety of datasets and ways to split them into training and test sets. The
documentation of these preprocessing steps is not always complete.
Consequently, research results are hard or even impossible to reproduce and to
compare between papers. At times, the use of non-public domain knowledge
further hampers the fair competition of ideas. Often the training and test sets
are not completely separated, a data leakage problem particular to predictive
process monitoring. Moreover, test sets usually suffer from bias in terms of
both the mix of case durations and the number of running cases. These obstacles
pose a challenge to the field's progress. The contribution of this paper is to
identify and demonstrate the importance of these obstacles and to propose
preprocessing steps to arrive at unbiased benchmark datasets in a principled
way, thus creating representative test sets without data leakage with the aim
of levelling the playing field, promoting open science and contributing to more
rapid progress in predictive process monitoring.",Creating Unbiased Public Benchmark Datasets with Data Leakage Prevention for Predictive Process Monitoring,2021
"Fariba Afrin Irany, Arnav Iyer, Rubenia Borge Flores, Armin R. Mikler",AI,2021,"The delivery of Medical Countermeasures(MCMs) for mass prophylaxis in the
case of a bio-terrorist attack is an active research topic that has interested
the research community over the past decades. The objective of this study is to
design an efficient algorithm for the Receive Reload and Store Problem(RSS) in
which we aim to find feasible routes to deliver MCMs to a target population
considering time, physical, and human resources, and capacity limitations. For
doing this, we adapt the p-median problem to the POD-based emergency response
planning procedures and propose an efficient algorithm solution to perform the
p-median in reasonable computational time. We present RE-PLAN, the Response
PLan Analyzer system that contains some RSS solutions developed at The Center
for Computational Epidemiology and Response Analysis (CeCERA) at the University
of North Texas. Finally, we analyze a study case where we show how the
computational performance of the algorithm can impact the process of decision
making and emergency planning in the short and long terms.",The Multi-phase spatial meta-heuristic algorithm for public health emergency transportation,2021
"Sriram Gopalakrishnan, Utkarsh Soni, Tung Thai, Panagiotis Lymperopoulos, Matthias Scheutz, Subbarao Kambhampati",AI,2021,"The game of monopoly is an adversarial multi-agent domain where there is no
fixed goal other than to be the last player solvent, There are useful subgoals
like monopolizing sets of properties, and developing them. There is also a lot
of randomness from dice rolls, card-draws, and adversaries' strategies. This
unpredictability is made worse when unknown novelties are added during
gameplay. Given these challenges, Monopoly was one of the test beds chosen for
the DARPA-SAILON program which aims to create agents that can detect and
accommodate novelties. To handle the game complexities, we developed an agent
that eschews complete plans, and adapts it's policy online as the game evolves.
In the most recent independent evaluation in the SAILON program, our agent was
the best performing agent on most measures. We herein present our approach and
results.","Integrating Planning, Execution and Monitoring in the presence of Open World Novelties: Case Study of an Open World Monopoly Solver",2021
"Francesco Massari, Martin Biehl, Lisa Meeden, Ryota Kanai",AI,2021,"Reinforcement Learning (RL) is known to be often unsuccessful in environments
with sparse extrinsic rewards. A possible countermeasure is to endow RL agents
with an intrinsic reward function, or 'intrinsic motivation', which rewards the
agent based on certain features of the current sensor state. An intrinsic
reward function based on the principle of empowerment assigns rewards
proportional to the amount of control the agent has over its own sensors. We
implemented a variation on a recently proposed intrinsically motivated agent,
which we refer to as the 'curious' agent, and an empowerment-inspired agent.
The former leverages sensor state encoding with a variational autoencoder,
while the latter predicts the next sensor state via a variational information
bottleneck. We compared the performance of both agents to that of an advantage
actor-critic baseline in four sparse reward grid worlds. Both the empowerment
agent and its curious competitor seem to benefit to similar extents from their
intrinsic rewards. This provides some experimental support to the conjecture
that empowerment can be used to drive exploration.",Experimental Evidence that Empowerment May Drive Exploration in Sparse-Reward Environments,2021
"Xueping Peng, Guodong Long, Sen Wang, Jing Jiang, Allison Clarke, Clement Schlegel, Chengqi Zhang",AI,2021,"Healthcare representation learning on the Electronic Health Records is
crucial for downstream medical prediction tasks in health informatics. Many NLP
techniques, such as RNN and self-attention, have been adapted to learn medical
representations from hierarchical and time-stamped EHRs data, but fail when
they lack either general or task-specific data. Hence, some recent works train
healthcare representations by incorporating medical ontology, by
self-supervised tasks like diagnosis prediction, but (1) the small-scale,
monotonous ontology is insufficient for robust learning, and (2) critical
contexts or dependencies underlying patient journeys are barely exploited to
enhance ontology learning. To address the challenges, we propose a
Transformer-based representation learning approach: Mutual Integration of
Patient journey and medical Ontology (MIPO), which is a robust end-to-end
framework. Specifically, the proposed method focuses on task-specific
representation learning by a sequential diagnoses predictive task, which is
also beneficial to the ontology-based disease typing task. To integrate
information in the patient's visiting records, we further introduce a
graph-embedding module, which can mitigate the challenge of data insufficiency
in healthcare. In this way, MIPO creates a mutual integration to benefit both
healthcare representation learning and medical ontology embedding. Such an
effective integration is guaranteed by joint training over fused embeddings of
the two modules, targeting both task-specific prediction and ontology-based
disease typing tasks simultaneously. Extensive experiments conducted on two
real-world benchmark datasets have shown MIPO consistently achieves better
performance than state-of-the-art methods no matter whether the training data
is sufficient or not. Also, MIPO derives more interpretable diagnose embedding
results compared to its counterparts.",MIPO: Mutual Integration of Patient Journey and Medical Ontology for Healthcare Representation Learning,2022
"Michael Timothy Bennett, Yoshihiro Maruyama",AI,2021,"In order to construct an ethical artificial intelligence (AI) two complex
problems must be overcome. Firstly, humans do not consistently agree on what is
or is not ethical. Second, contemporary AI and machine learning methods tend to
be blunt instruments which either search for solutions within the bounds of
predefined rules, or mimic behaviour. An ethical AI must be capable of
inferring unspoken rules, interpreting nuance and context, possess and be able
to infer intent, and explain not just its actions but its intent. Using
enactivism, semiotics, perceptual symbol systems and symbol emergence, we
specify an agent that learns not just arbitrary relations between signs but
their meaning in terms of the perceptual states of its sensorimotor system.
Subsequently it can learn what is meant by a sentence and infer the intent of
others in terms of its own experiences. It has malleable intent because the
meaning of symbols changes as it learns, and its intent is represented
symbolically as a goal. As such it may learn a concept of what is most likely
to be considered ethical by the majority within a population of humans, which
may then be used as a goal. The meaning of abstract symbols is expressed using
perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with
Ockham's Razor) necessary and sufficient concept, an intensional definition
learned from an ostensive definition, from which the extensional definition or
category of all ethical decisions may be obtained. Because these abstract
symbols are the same for both situation and response, the same symbol is used
when either performing or observing an action. This is akin to mirror neurons
in the human brain. Mirror symbols may allow the agent to empathise, because
its own experiences are associated with the symbol, which is also associated
with the observation of another agent experiencing something that symbol
represents.",Philosophical Specification of Empathetic Ethical Artificial Intelligence,2021
"Lingwei Wei, Dou Hu, Wei Zhou, Zhaojuan Yue, Songlin Hu",AI,2021,"Detecting rumors on social media is a very critical task with significant
implications to the economy, public health, etc. Previous works generally
capture effective features from texts and the propagation structure. However,
the uncertainty caused by unreliable relations in the propagation structure is
common and inevitable due to wily rumor producers and the limited collection of
spread data. Most approaches neglect it and may seriously limit the learning of
features. Towards this issue, this paper makes the first attempt to explore
propagation uncertainty for rumor detection. Specifically, we propose a novel
Edge-enhanced Bayesian Graph Convolutional Network (EBGCN) to capture robust
structural features. The model adaptively rethinks the reliability of latent
relations by adopting a Bayesian approach. Besides, we design a new edge-wise
consistency training framework to optimize the model by enforcing consistency
on relations. Experiments on three public benchmark datasets demonstrate that
the proposed model achieves better performance than baseline methods on both
rumor detection and early rumor detection tasks.",Towards Propagation Uncertainty: Edge-enhanced Bayesian Graph Convolutional Networks for Rumor Detection,2021
"Vince I. Madai, David C. Higgins",AI,2021,"Artificial intelligence (AI) in healthcare is a potentially revolutionary
tool to achieve improved healthcare outcomes while reducing overall health
costs. While many exploratory results hit the headlines in recent years there
are only few certified and even fewer clinically validated products available
in the clinical setting. This is a clear indication of failing translation due
to shortcomings of the current approach to AI in healthcare. In this work, we
highlight the major areas, where we observe current challenges for translation
in AI in healthcare, namely precision medicine, reproducible science, data
issues and algorithms, causality, and product development. For each field, we
outline possible solutions for these challenges. Our work will lead to improved
translation of AI in healthcare products into the clinical setting",Artificial Intelligence in Healthcare: Lost In Translation?,2021
"Jörg Stork, Philip Wenzel, Severin Landwein, Maria-Elena Algorri, Martin Zaefferer, Wolfgang Kusch, Martin Staubach, Thomas Bartz-Beielstein, Hartmut Köhn, Hermann Dejager, Christian Wolf",AI,2021,"We have built a novel system for the surveillance of drinking water
reservoirs using underwater sensor networks. We implement an innovative
AI-based approach to detect, classify and localize underwater events. In this
paper, we describe the technology and cognitive AI architecture of the system
based on one of the sensor networks, the hydrophone network. We discuss the
challenges of installing and using the hydrophone network in a water reservoir
where traffic, visitors, and variable water conditions create a complex,
varying environment. Our AI solution uses an autoencoder for unsupervised
learning of latent encodings for classification and anomaly detection, and time
delay estimates for sound localization. Finally, we present the results of
experiments carried out in a laboratory pool and the water reservoir and
discuss the system's potential.",Underwater Acoustic Networks for Security Risk Assessment in Public Drinking Water Reservoirs,2021
"Swarnamugi. M, Chinnaiyan. R",AI,2021,"The emergence of Internet of Things technology and recent advancement in
sensor networks enabled transportation systems to a new dimension called
Intelligent Transportation System. Due to increased usage of vehicles and
communication among entities in road traffic scenarios, the amount of raw data
generation in Intelligent Transportation System is huge. This raw data are to
be processed to infer contextual information and provide new services related
to different modes of road transport such as traffic signal management,
accident prediction, object detection etc. To understand the importance of
context, this article aims to study context awareness in the Intelligent
Transportation System. We present a review on prominent applications developed
in the literature concerning context awareness in the intelligent
transportation system. The objective of this research paper is to highlight
context and its features in ITS and to address the applicability of modelling
techniques and reasoning approaches in Intelligent Transportation System. Also
to shed light on impact of Internet of Things and machine learning in
Intelligent Transportation System development.",Modelling and Reasoning Techniques for Context Aware Computing in Intelligent Transportation System,2021
Samira Ghodratnama,AI,2021,"The ubiquitous availability of computing devices and the widespread use of
the internet have generated a large amount of data continuously. Therefore, the
amount of available information on any given topic is far beyond humans'
processing capacity to properly process, causing what is known as information
overload. To efficiently cope with large amounts of information and generate
content with significant value to users, we require identifying, merging and
summarising information. Data summaries can help gather related information and
collect it into a shorter format that enables answering complicated questions,
gaining new insight and discovering conceptual boundaries.
  This thesis focuses on three main challenges to alleviate information
overload using novel summarisation techniques. It further intends to facilitate
the analysis of documents to support personalised information extraction. This
thesis separates the research issues into four areas, covering (i) feature
engineering in document summarisation, (ii) traditional static and inflexible
summaries, (iii) traditional generic summarisation approaches, and (iv) the
need for reference summaries. We propose novel approaches to tackle these
challenges, by: i)enabling automatic intelligent feature engineering, ii)
enabling flexible and interactive summarisation, iii) utilising intelligent and
personalised summarisation approaches. The experimental results prove the
efficiency of the proposed approaches compared to other state-of-the-art
models. We further propose solutions to the information overload problem in
different domains through summarisation, covering network traffic data, health
data and business process data.",Towards Personalized and Human-in-the-Loop Document Summarization,2021
"Ignacio Vellido, Carlos Núñez-Molina, Vladislav Nikolov, Juan Fdez-Olivares",AI,2021,"This project proposes a methodology for the automatic generation of action
models from video game dynamics descriptions, as well as its integration with a
planning agent for the execution and monitoring of the plans. Planners use
these action models to get the deliberative behaviour for an agent in many
different video games and, combined with a reactive module, solve deterministic
and no-deterministic levels. Experimental results validate the methodology and
prove that the effort put by a knowledge engineer can be greatly reduced in the
definition of such complex domains. Furthermore, benchmarks of the domains has
been produced that can be of interest to the international planning community
to evaluate planners in international planning competitions.",Planning from video game descriptions,2021
"Thomas P Quinn, Simon Coghlan",AI,2021,"Medical students will almost inevitably encounter powerful medical AI systems
early in their careers. Yet, contemporary medical education does not adequately
equip students with the basic clinical proficiency in medical AI needed to use
these tools safely and effectively. Education reform is urgently needed, but
not easily implemented, largely due to an already jam-packed medical curricula.
In this article, we propose an education reform framework as an effective and
efficient solution, which we call the Embedded AI Ethics Education Framework.
Unlike other calls for education reform to accommodate AI teaching that are
more radical in scope, our framework is modest and incremental. It leverages
existing bioethics or medical ethics curricula to develop and deliver content
on the ethical issues associated with medical AI, especially the harms of
technology misuse, disuse, and abuse that affect the risk-benefit analyses at
the heart of healthcare. In doing so, the framework provides a simple tool for
going beyond the ""What?"" and the ""Why?"" of medical AI ethics education, to
answer the ""How?"", giving universities, course directors, and/or professors a
broad road-map for equipping their students with the necessary clinical
proficiency in medical AI.",Readying Medical Students for Medical AI: The Need to Embed AI Ethics Education,2021
"Ninareh Mehrabi, Umang Gupta, Fred Morstatter, Greg Ver Steeg, Aram Galstyan",AI,2021,"The widespread use of Artificial Intelligence (AI) in consequential domains,
such as healthcare and parole decision-making systems, has drawn intense
scrutiny on the fairness of these methods. However, ensuring fairness is often
insufficient as the rationale for a contentious decision needs to be audited,
understood, and defended. We propose that the attention mechanism can be used
to ensure fair outcomes while simultaneously providing feature attributions to
account for how a decision was made. Toward this goal, we design an
attention-based model that can be leveraged as an attribution framework. It can
identify features responsible for both performance and fairness of the model
through attention interventions and attention weight manipulation. Using this
attribution framework, we then design a post-processing bias mitigation
strategy and compare it with a suite of baselines. We demonstrate the
versatility of our approach by conducting experiments on two distinct data
types, tabular and textual.",Attributing Fair Decisions with Attention Interventions,2021
"Paul Festor, Giulia Luise, Matthieu Komorowski, A. Aldo Faisal",AI,2021,"Reinforcement Learning (RL) is emerging as tool for tackling complex control
and decision-making problems. However, in high-risk environments such as
healthcare, manufacturing, automotive or aerospace, it is often challenging to
bridge the gap between an apparently optimal policy learnt by an agent and its
real-world deployment, due to the uncertainties and risk associated with it.
Broadly speaking RL agents face two kinds of uncertainty, 1. aleatoric
uncertainty, which reflects randomness or noise in the dynamics of the world,
and 2. epistemic uncertainty, which reflects the bounded knowledge of the agent
due to model limitations and finite amount of information/data the agent has
acquired about the world. These two types of uncertainty carry fundamentally
different implications for the evaluation of performance and the level of risk
or trust. Yet these aleatoric and epistemic uncertainties are generally
confounded as standard and even distributional RL is agnostic to this
difference. Here we propose how a distributional approach (UA-DQN) can be
recast to render uncertainties by decomposing the net effects of each
uncertainty. We demonstrate the operation of this method in grid world examples
to build intuition and then show a proof of concept application for an RL agent
operating as a clinical decision support system in critical care",Enabling risk-aware Reinforcement Learning for medical interventions through uncertainty decomposition,2022
"Yuta Saito, Takuma Udagawa, Kei Tateno",AI,2021,"Off-policy evaluation (OPE) is the method that attempts to estimate the
performance of decision making policies using historical data generated by
different policies without conducting costly online A/B tests. Accurate OPE is
essential in domains such as healthcare, marketing or recommender systems to
avoid deploying poor performing policies, as such policies may hart human lives
or destroy the user experience. Thus, many OPE methods with theoretical
backgrounds have been proposed. One emerging challenge with this trend is that
a suitable estimator can be different for each application setting. It is often
unknown for practitioners which estimator to use for their specific
applications and purposes. To find out a suitable estimator among many
candidates, we use a data-driven estimator selection procedure for off-policy
policy performance estimators as a practical solution. As proof of concept, we
use our procedure to select the best estimator to evaluate coupon treatment
policies on a real-world online content delivery service. In the experiment, we
first observe that a suitable estimator might change with different definitions
of the outcome variable, and thus the accurate estimator selection is critical
in real-world applications of OPE. Then, we demonstrate that, by utilizing the
estimator selection procedure, we can easily find out suitable estimators for
each purpose.",Data-Driven Off-Policy Estimator Selection: An Application in User Marketing on An Online Content Delivery Service,2021
"Youssef Achenchabe, Alexis Bondu, Antoine Cornuéjols, Vincent Lemaire",AI,2021,"Many approaches have been proposed for early classification of time series in
light of itssignificance in a wide range of applications including healthcare,
transportation and fi-nance. Until now, the early classification problem has
been dealt with by considering onlyirrevocable decisions. This paper introduces
a new problem calledearly and revocabletimeseries classification, where the
decision maker can revoke its earlier decisions based on thenew available
measurements. In order to formalize and tackle this problem, we propose anew
cost-based framework and derive two new approaches from it. The first approach
doesnot consider explicitly the cost of changing decision, while the second one
does. Exten-sive experiments are conducted to evaluate these approaches on a
large benchmark of realdatasets. The empirical results obtained convincingly
show (i) that the ability of revok-ing decisions significantly improves
performance over the irrevocable regime, and (ii) thattaking into account the
cost of changing decision brings even better results in
general.Keywords:revocable decisions, cost estimation, online decision making",Early and Revocable Time Series Classification,2021
"Raul Sena Ferreira, Jean Arlat, Jeremie Guiochet, Hélène Waeselynck",AI,2021,"High-accurate machine learning (ML) image classifiers cannot guarantee that
they will not fail at operation. Thus, their deployment in safety-critical
applications such as autonomous vehicles is still an open issue. The use of
fault tolerance mechanisms such as safety monitors is a promising direction to
keep the system in a safe state despite errors of the ML classifier. As the
prediction from the ML is the core information directly impacting safety, many
works are focusing on monitoring the ML model itself. Checking the efficiency
of such monitors in the context of safety-critical applications is thus a
significant challenge. Therefore, this paper aims at establishing a baseline
framework for benchmarking monitors for ML image classifiers. Furthermore, we
propose a framework covering the entire pipeline, from data generation to
evaluation. Our approach measures monitor performance with a broader set of
metrics than usually proposed in the literature. Moreover, we benchmark three
different monitor approaches in 79 benchmark datasets containing five
categories of out-of-distribution data for image classifiers: class novelty,
noise, anomalies, distributional shifts, and adversarial attacks. Our results
indicate that these monitors are no more accurate than a random monitor. We
also release the code of all experiments for reproducibility.",Benchmarking Safety Monitors for Image Classifiers with Machine Learning,2021
"Ilaria Angela Amantea, Livio Robaldo, Emilio Sulis, Guido Boella, Guido Governatori",AI,2021,"One of the main issues of every business process is to be compliant with
legal rules. This work presents a methodology to check in a semi-automated way
the regulatory compliance of a business process. We analyse an e-Health
hospital service in particular: the Hospital at Home (HaH) service. The paper
shows, at first, the analysis of the hospital business using the Business
Process Management and Notation (BPMN) standard language, then, the
formalization in Defeasible Deontic Logic (DDL) of some rules of the European
General Data Protection Regulation (GDPR). The aim is to show how to combine a
set of tasks of a business with a set of rules to be compliant with, using a
tool.",Semi-automated checking for regulatory compliance in e-Health,2021
"Nguyen Tan Viet Tuyen, Oya Celiktutan",AI,2021,"We are approaching a future where social robots will progressively become
widespread in many aspects of our daily lives, including education, healthcare,
work, and personal use. All of such practical applications require that humans
and robots collaborate in human environments, where social interaction is
unavoidable. Along with verbal communication, successful social interaction is
closely coupled with the interplay between nonverbal perception and action
mechanisms, such as observation of gaze behaviour and following their
attention, coordinating the form and function of hand gestures. Humans perform
nonverbal communication in an instinctive and adaptive manner, with no effort.
For robots to be successful in our social landscape, they should therefore
engage in social interactions in a humanlike way, with increasing levels of
autonomy. In particular, nonverbal gestures are expected to endow social robots
with the capability of emphasizing their speech, or showing their intentions.
Motivated by this, our research sheds a light on modeling human behaviors in
social interactions, specifically, forecasting human nonverbal social signals
during dyadic interactions, with an overarching goal of developing robotic
interfaces that can learn to imitate human dyadic interactions. Such an
approach will ensure the messages encoded in the robot gestures could be
perceived by interacting partners in a facile and transparent manner, which
could help improve the interacting partner perception and makes the social
interaction outcomes enhanced.",Forecasting Nonverbal Social Signals during Dyadic Interactions with Generative Adversarial Neural Networks,2021
"Donsuk Lee, Samantha M. W. Wood, Justin N. Wood",AI,2021,"Collective behavior is widespread across the animal kingdom. To date,
however, the developmental and mechanistic foundations of collective behavior
have not been formally established. What learning mechanisms drive the
development of collective behavior in newborn animals? Here, we used deep
reinforcement learning and curiosity-driven learning -- two learning mechanisms
deeply rooted in psychological and neuroscientific research -- to build newborn
artificial agents that develop collective behavior. Like newborn animals, our
agents learn collective behavior from raw sensory inputs in naturalistic
environments. Our agents also learn collective behavior without external
rewards, using only intrinsic motivation (curiosity) to drive learning.
Specifically, when we raise our artificial agents in natural visual
environments with groupmates, the agents spontaneously develop ego-motion,
object recognition, and a preference for groupmates, rapidly learning all of
the core skills required for collective behavior. This work bridges the divide
between high-dimensional sensory inputs and collective action, resulting in a
pixels-to-actions model of collective animal behavior. More generally, we show
that two generic learning mechanisms -- deep reinforcement learning and
curiosity-driven learning -- are sufficient to learn collective behavior from
unsupervised natural experience.",Development of collective behavior in newborn artificial agents,2021
"Yanou Ramon, Sandra C. Matz, R. A. Farrokhnia, David Martens",AI,2021,"Every step we take in the digital world leaves behind a record of our
behavior; a digital footprint. Research has suggested that algorithms can
translate these digital footprints into accurate estimates of psychological
characteristics, including personality traits, mental health or intelligence.
The mechanisms by which AI generates these insights, however, often remain
opaque. In this paper, we show how Explainable AI (XAI) can help domain experts
and data subjects validate, question, and improve models that classify
psychological traits from digital footprints. We elaborate on two popular XAI
methods (rule extraction and counterfactual explanations) in the context of Big
Five personality predictions (traits and facets) from financial transactions
data (N = 6,408). First, we demonstrate how global rule extraction sheds light
on the spending patterns identified by the model as most predictive for
personality, and discuss how these rules can be used to explain, validate, and
improve the model. Second, we implement local rule extraction to show that
individuals are assigned to personality classes because of their unique
financial behavior, and that there exists a positive link between the model's
prediction confidence and the number of features that contributed to the
prediction. Our experiments highlight the importance of both global and local
XAI methods. By better understanding how predictive models work in general as
well as how they derive an outcome for a particular person, XAI promotes
accountability in a world in which AI impacts the lives of billions of people
around the world.",Explainable AI for Psychological Profiling from Digital Footprints: A Case Study of Big Five Personality Predictions from Spending Data,2021
"Sekou L Remy, Aisha Walcott-Bryant, Nelson K Bore, Charles M Wachira, Julian Kuenhert",AI,2021,"In popular usage, Data Gravity refers to the ability of a body of data to
attract applications, services and other data. In this work we introduce a
broader concept, ""Digital Gravity"" which includes not just data, but other
elements of the AI/ML workflow. This concept is born out of our recent
experiences in developing and deploying an AI-based decision support platform
intended for use in a public health context. In addition to data, examples of
additional considerations are compute (infrastructure and software), DevSecOps
(personnel and practices), algorithms/programs, control planes, middleware
(considered separately from programs), and even companies/service providers. We
discuss the impact of Digital Gravity on the pathway to adoption and suggest
preliminary approaches to conceptualize and mitigate the friction caused by it.",Overcoming Digital Gravity when using AI in Public Health Decisions,2021
"Anti Alman, Fabrizio Maria Maggi, Marco Montali, Fabio Patrizi, Andrey Rivkin",AI,2021,"Business process monitoring approaches have thus far mainly focused on
monitoring the execution of a process with respect to a single process model.
However, in some cases it is necessary to consider multiple process
specifications simultaneously. In addition, these specifications can be
procedural, declarative, or a combination of both. For example, in the medical
domain, a clinical guideline describing the treatment of a specific disease
cannot account for all possible co-factors that can coexist for a specific
patient and therefore additional constraints may need to be considered. In some
cases, these constraints may be incompatible with clinical guidelines,
therefore requiring the violation of either the guidelines or the constraints.
In this paper, we propose a solution for monitoring the interplay of hybrid
process specifications expressed as a combination of (data-aware) Petri nets
and temporal logic rules. During the process execution, if these specifications
are in conflict with each other, it is possible to violate some of them. The
monitoring system is equipped with a violation cost model according to which
the system can recommend the next course of actions in a way that would either
avoid possible violations or minimize the total cost of violations.",Monitoring Hybrid Process Specifications with Conflict Management: The Automata-theoretic Approach,2021
"Aditya Kanade, Mansi Sharma, M. Manivannan",AI,2021,"Technology has an important role to play in the field of Rehabilitation,
improving patient outcomes and reducing healthcare costs. However, existing
approaches lack clinical validation, robustness and ease of use. We propose
Tele-EvalNet, a novel system consisting of two components: a live feedback
model and an overall performance evaluation model. The live feedback model
demonstrates feedback on exercise correctness with easy to understand
instructions highlighted using color markers. The overall performance
evaluation model learns a mapping of joint data to scores, given to the
performance by clinicians. The model does this by extracting clinically
approved features from joint data. Further, these features are encoded to a
lower dimensional space with an autoencoder. A novel multi-scale CNN-LSTM
network is proposed to learn a mapping of performance data to the scores by
leveraging features extracted at multiple scales. The proposed system shows a
high degree of improvement in score predictions and outperforms the
state-of-the-art rehabilitation models.","Tele-EvalNet: A Low-cost, Teleconsultation System for Home based Rehabilitation of Stroke Survivors using Multiscale CNN-LSTM Architecture",2021
"Mohammadhossein Ghahramani, Mengchu Zhou, Anna Molter, Francesco Pilla",AI,2022,"The Internet of Things (IoT) is a paradigm characterized by a network of
embedded sensors and services. These sensors are incorporated to collect
various information, track physical conditions, e.g., waste bins' status, and
exchange data with different centralized platforms. The need for such sensors
is increasing; however, proliferation of technologies comes with various
challenges. For example, how can IoT and its associated data be used to enhance
waste management? In smart cities, an efficient waste management system is
crucial. Artificial Intelligence (AI) and IoT-enabled approaches can empower
cities to manage the waste collection. This work proposes an intelligent
approach to route recommendation in an IoT-enabled waste management system
given spatial constraints. It performs a thorough analysis based on AI-based
methods and compares their corresponding results. Our solution is based on a
multiple-level decision-making process in which bins' status and coordinates
are taken into account to address the routing problem. Such AI-based models can
help engineers design a sustainable infrastructure system.",IoT-based Route Recommendation for an Intelligent Waste Management System,2022
"Rahma Dandan, Sylvie Despres, Karima Sedki",AI,2022,"Our food preferences guide our food choices and in turn affect our personal
health and our social life. In this paper, we adopt an approach using a domain
ontology expressed in OWL2 to support the acquisition and representation of
preferences in formalism CP-Net. Specifically, we present the construction of
the domain ontology and questionnaire design to acquire and represent the
preferences. The acquisition and representation of preferences are implemented
in the field of university canteen. Our main contribution in this preliminary
work is to acquire preferences and enrich the model preferably with domain
knowledge represented in the ontology.",Acquisition and Representation of User Preferences Guided by an Ontology,2022
"Dominique Verdejo, Eunika Mercier-Laurent",AI,2022,"This paper describes the evolution of our research from video analytics to a
global security system with focus on the video surveillance component. Indeed
video surveillance has evolved from a commodity security tool up to the most
efficient way of tracking perpetrators when terrorism hits our modern urban
centers. As number of cameras soars, one could expect the system to leverage
the huge amount of data carried through the video streams to provide fast
access to video evidences, actionable intelligence for monitoring real-time
events and enabling predictive capacities to assist operators in their
surveillance tasks. This research explores a hybrid platform for video
intelligence capture, automated data extraction, supervised Machine Learning
for intelligently assisted urban video surveillance; Extension to other
components of a global security system are discussed. Applying Knowledge
Management principles in this research helps with deep problem understanding
and facilitates the implementation of efficient information and experience
sharing decision support systems providing assistance to people on the field as
well as in operations centers. The originality of this work is also the
creation of ""common"" human-machine and machine to machine language and a
security ontology.",Video Intelligence as a component of a Global Security system,2022
"Shuai Niu, Qing Yin, Yunya Song, Yike Guo, Xian Yang",AI,2022,"Disease risk prediction has attracted increasing attention in the field of
modern healthcare, especially with the latest advances in artificial
intelligence (AI). Electronic health records (EHRs), which contain
heterogeneous patient information, are widely used in disease risk prediction
tasks. One challenge of applying AI models for risk prediction lies in
generating interpretable evidence to support the prediction results while
retaining the prediction ability. In order to address this problem, we propose
the method of jointly embedding words and labels whereby attention modules
learn the weights of words from medical notes according to their relevance to
the names of risk prediction labels. This approach boosts interpretability by
employing an attention mechanism and including the names of prediction tasks in
the model. However, its application is only limited to the handling of textual
inputs such as medical notes. In this paper, we propose a label dependent
attention model LDAM to 1) improve the interpretability by exploiting
Clinical-BERT (a biomedical language model pre-trained on a large clinical
corpus) to encode biomedically meaningful features and labels jointly; 2)
extend the idea of joint embedding to the processing of time-series data, and
develop a multi-modal learning framework for integrating heterogeneous
information from medical notes and time-series health status indicators. To
demonstrate our method, we apply LDAM to the MIMIC-III dataset to predict
different disease risks. We evaluate our method both quantitatively and
qualitatively. Specifically, the predictive power of LDAM will be shown, and
case studies will be carried out to illustrate its interpretability.",Label Dependent Attention Model for Disease Risk Prediction Using Multimodal Electronic Health Records,2022
"Shuai Niu, Yunya Song, Qing Yin, Yike Guo, Xian Yang",AI,2022,"Electronic health records (EHRs) contain patients' heterogeneous data that
are collected from medical providers involved in the patient's care, including
medical notes, clinical events, laboratory test results, symptoms, and
diagnoses. In the field of modern healthcare, predicting whether patients would
experience any risks based on their EHRs has emerged as a promising research
area, in which artificial intelligence (AI) plays a key role. To make AI models
practically applicable, it is required that the prediction results should be
both accurate and interpretable. To achieve this goal, this paper proposed a
label-dependent and event-guided risk prediction model (LERP) to predict the
presence of multiple disease risks by mainly extracting information from
unstructured medical notes. Our model is featured in the following aspects.
First, we adopt a label-dependent mechanism that gives greater attention to
words from medical notes that are semantically similar to the names of risk
labels. Secondly, as the clinical events (e.g., treatments and drugs) can also
indicate the health status of patients, our model utilizes the information from
events and uses them to generate an event-guided representation of medical
notes. Thirdly, both label-dependent and event-guided representations are
integrated to make a robust prediction, in which the interpretability is
enabled by the attention weights over words from medical notes. To demonstrate
the applicability of the proposed method, we apply it to the MIMIC-III dataset,
which contains real-world EHRs collected from hospitals. Our method is
evaluated in both quantitative and qualitative ways.",Label-dependent and event-guided interpretable disease risk prediction using EHRs,2022
"Kathrin Blagec, Jakob Kraiger, Wolfgang Frühwirt, Matthias Samwald",AI,2022,"Publicly accessible benchmarks that allow for assessing and comparing model
performances are important drivers of progress in artificial intelligence (AI).
While recent advances in AI capabilities hold the potential to transform
medical practice by assisting and augmenting the cognitive processes of
healthcare professionals, the coverage of clinically relevant tasks by AI
benchmarks is largely unclear. Furthermore, there is a lack of systematized
meta-information that allows clinical AI researchers to quickly determine
accessibility, scope, content and other characteristics of datasets and
benchmark datasets relevant to the clinical domain.
  To address these issues, we curated and released a comprehensive catalogue of
datasets and benchmarks pertaining to the broad domain of clinical and
biomedical natural language processing (NLP), based on a systematic review of
literature and online resources. A total of 450 NLP datasets were manually
systematized and annotated with rich metadata, such as targeted tasks, clinical
applicability, data types, performance metrics, accessibility and licensing
information, and availability of data splits. We then compared tasks covered by
AI benchmark datasets with relevant tasks that medical practitioners reported
as highly desirable targets for automation in a previous empirical study.
  Our analysis indicates that AI benchmarks of direct clinical relevance are
scarce and fail to cover most work activities that clinicians want to see
addressed. In particular, tasks associated with routine documentation and
patient data administration workflows are not represented despite significant
associated workloads. Thus, currently available AI benchmarks are improperly
aligned with desired targets for AI automation in clinical settings, and novel
benchmarks should be created to fill these gaps.",Benchmark datasets driving artificial intelligence development fail to capture the needs of medical professionals,2022
"Kamil Faber, Roberto Corizzo, Bartlomiej Sniezynski, Michael Baron, Nathalie Japkowicz",AI,2022,"Detecting relevant changes in dynamic time series data in a timely manner is
crucially important for many data analysis tasks in real-world settings. Change
point detection methods have the ability to discover changes in an unsupervised
fashion, which represents a desirable property in the analysis of unbounded and
unlabeled data streams. However, one limitation of most of the existing
approaches is represented by their limited ability to handle multivariate and
high-dimensional data, which is frequently observed in modern applications such
as traffic flow prediction, human activity recognition, and smart grids
monitoring. In this paper, we attempt to fill this gap by proposing WATCH, a
novel Wasserstein distance-based change point detection approach that models an
initial distribution and monitors its behavior while processing new data
points, providing accurate and robust detection of change points in dynamic
high-dimensional data. An extensive experimental evaluation involving a large
number of benchmark datasets shows that WATCH is capable of accurately
identifying change points and outperforming state-of-the-art methods.",WATCH: Wasserstein Change Point Detection for High-Dimensional Time Series Data,2022
"Timon Felske, Stefan Lüdtke, Sebastian Bader, Thomas Kirste",AI,2022,"We study sensor-based human activity recognition in manual work processes
like assembly tasks. In such processes, the system states often have a rich
structure, involving object properties and relations. Thus, estimating the
hidden system state from sensor observations by recursive Bayesian filtering
can be very challenging, due to the combinatorial explosion in the number of
system states. To alleviate this problem, we propose an efficient Bayesian
filtering model for such processes. In our approach, system states are
represented by multi-hypergraphs, and the system dynamics is modeled by graph
rewriting rules. We show a preliminary concept that allows to represent
distributions over multi-hypergraphs more compactly than by full enumeration,
and present an inference algorithm that works directly on this compact
representation. We demonstrate the applicability of the algorithm on a real
dataset.",Activity Recognition in Assembly Tasks by Bayesian Filtering in Multi-Hypergraphs,2022
"Shivam Gupta, Auriol Degbelo",AI,2022,"Artificial Intelligence (AI) presents opportunities to develop tools and
techniques for addressing some of the major global challenges and deliver
solutions with significant social and economic impacts. The application of AI
has far-reaching implications for the 17 Sustainable Development Goals (SDGs)
in general, and sustainable urban development in particular. However, existing
attempts to understand and use the opportunities offered by AI for SDG 11 have
been explored sparsely, and the shortage of empirical evidence about the
practical application of AI remains. In this chapter, we analyze the
contribution of AI to support the progress of SDG 11 (Sustainable Cities and
Communities). We address the knowledge gap by empirically analyzing the AI
systems (N = 29) from the AIxSDG database and the Community Research and
Development Information Service (CORDIS) database. Our analysis revealed that
AI systems have indeed contributed to advancing sustainable cities in several
ways (e.g., waste management, air quality monitoring, disaster response
management, transportation management), but many projects are still working for
citizens and not with them. This snapshot of AI's impact on SDG11 is inherently
partial, yet useful to advance our understanding as we move towards more mature
systems and research on the impact of AI systems for social good.",An Empirical Analysis of AI Contributions to Sustainable Cities (SDG11),2022
"Kayalvizhi S, Thenmozhi D",AI,2022,"Depression is a common mental illness that has to be detected and treated at
an early stage to avoid serious consequences. There are many methods and
modalities for detecting depression that involves physical examination of the
individual. However, diagnosing mental health using their social media data is
more effective as it avoids such physical examinations. Also, people express
their emotions well in social media, it is desirable to diagnose their mental
health using social media data. Though there are many existing systems that
detects mental illness of a person by analysing their social media data,
detecting the level of depression is also important for further treatment.
Thus, in this research, we developed a gold standard data set that detects the
levels of depression as `not depressed', `moderately depressed' and `severely
depressed' from the social media postings. Traditional learning algorithms were
employed on this data set and an empirical analysis was presented in this
paper. Data augmentation technique was applied to overcome the data imbalance.
Among the several variations that are implemented, the model with Word2Vec
vectorizer and Random Forest classifier on augmented data outperforms the other
variations with a score of 0.877 for both accuracy and F1 measure.",Data set creation and empirical analysis for detecting signs of depression from social media postings,2022
Mark Dukes,AI,2022,"We present a method for calculating and analyzing stakeholder utilities of
processes that arise in, but are not limited to, the social sciences. These
areas include business process analysis, healthcare workflow analysis and
policy process analysis. This method is quite general and applicable to any
situation in which declarative-type constraints of a modal and/or temporal
nature play a part.
  A declarative process is a process in which activities may freely happen
while respecting a set of constraints. For such a process, anything may happen
so long as it is not explicitly forbidden. Declarative processes have been used
and studied as models of business and healthcare workflows by several authors.
In considering a declarative process as a model of some system it is natural to
consider how the process behaves with respect to stakeholders. We derive a
measure for stakeholder utility that can be applied in a very general setting.
This derivation is achieved by listing a collection a properties which we argue
such a stakeholder utility function ought to satisfy, and then using these to
show a very specific form must hold for such a utility. The utility measure
depends on the set of unique traces of the declarative process, and calculating
this set requires a combinatorial analysis of the declarative graph that
represents the process.
  This builds on previous work of the author wherein the combinatorial
diversity metrics for declarative processes were derived for use in policy
process analysis. The collection of stakeholder utilities can themselves then
be used to form a metric with which we can compare different declarative
processes to one another. These are illustrated using several examples of
declarative processes that already exist in the literature.",Stakeholder utility measures for declarative processes and their use in process comparisons,2022
"Williams Rizzi, Marco Comuzzi, Chiara Di Francescomarino, Chiara Ghidini, Suhwan Lee, Fabrizio Maria Maggi, Alexander Nolte",AI,2022,"Explainability is motivated by the lack of transparency of black-box Machine
Learning approaches, which do not foster trust and acceptance of Machine
Learning algorithms. This also happens in the Predictive Process Monitoring
field, where predictions, obtained by applying Machine Learning techniques,
need to be explained to users, so as to gain their trust and acceptance. In
this work, we carry on a user evaluation on explanation approaches for
Predictive Process Monitoring aiming at investigating whether and how the
explanations provided (i) are understandable; (ii) are useful in decision
making tasks;(iii) can be further improved for process analysts, with different
Machine Learning expertise levels. The results of the user evaluation show
that, although explanation plots are overall understandable and useful for
decision making tasks for Business Process Management users -- with and without
experience in Machine Learning -- differences exist in the comprehension and
usage of different plots, as well as in the way users with different Machine
Learning expertise understand and use them.",Explainable Predictive Process Monitoring: A User Evaluation,2022
"Fedor Scholz, Christian Gumbsch, Sebastian Otte, Martin V. Butz",AI,2022,"Flexible, goal-directed behavior is a fundamental aspect of human life. Based
on the free energy minimization principle, the theory of active inference
formalizes the generation of such behavior from a computational neuroscience
perspective. Based on the theory, we introduce an output-probabilistic,
temporally predictive, modular artificial neural network architecture, which
processes sensorimotor information, infers behavior-relevant aspects of its
world, and invokes highly flexible, goal-directed behavior. We show that our
architecture, which is trained end-to-end to minimize an approximation of free
energy, develops latent states that can be interpreted as affordance maps. That
is, the emerging latent states signal which actions lead to which effects
dependent on the local context. In combination with active inference, we show
that flexible, goal-directed behavior can be invoked, incorporating the
emerging affordance maps. As a result, our simulated agent flexibly steers
through continuous spaces, avoids collisions with obstacles, and prefers
pathways that lead to the goal with high certainty. Additionally, we show that
the learned agent is highly suitable for zero-shot generalization across
environments: After training the agent in a handful of fixed environments with
obstacles and other terrains affecting its behavior, it performs similarly well
in procedurally generated environments containing different amounts of
obstacles and terrains of various sizes at different locations.",Inference of Affordances and Active Motor Control in Simulated Agents,2022
Stefan Bosse,AI,2022,"Most traffic flow control algorithms address switching cycle adaptation of
traffic signals and lights. This work addresses traffic flow optimisation by
self-organising micro-level control combining Reinforcement Learning and
rule-based agents for action selection performing long-range navigation in
urban environments. I.e., vehicles represented by agents adapt their decision
making for re-routing based on local environmental sensors. Agent-based
modelling and simulation is used to study emergence effects on urban city
traffic flows. An unified agent programming model enables simulation and
distributed data processing with possible incorporation of crowd sensing tasks
used as an additional sensor data base. Results from an agent-based simulation
of an artificial urban area show that the deployment of micro-level vehicle
navigation control just by learned individual decision making and re-routing
based on local environmental sensors can increase the efficiency of mobility in
terms of path length and travelling time.",Self-organising Urban Traffic control on micro-level using Reinforcement Learning and Agent-based Modelling,2022
"Ayman Alahmar, Ola Alkhatib",AI,2022,"Clinical Pathways (CP) are medical management plans developed to standardize
patient treatment activities, optimize resource usage, reduce expenses, and
improve the quality of healthcare services. Most CPs currently in use are
paper-based documents (i.e., not computerized). CP computerization has been an
active research topic since the inception of CP use in hospitals. This
literature review research aims to examine studies that focused on CP
computerization and offers recommendations for future research in this
important research area. Some critical research suggestions include
centralizing computerized CPs in Healthcare Information Systems (HIS), CP term
standardization using international medical terminology systems, developing a
global CP-specific digital coding system, creating a unified CP meta-ontology,
developing independent Clinical Pathway Management Systems (CPMS), and
supporting CPMSs with machine learning sub-systems.",Computerization of Clinical Pathways: A Literature Review and Directions for Future Research,2022
"Isabelle Hupont, Emilia Gomez, Songul Tolan, Lorenzo Porcaro, Ana Freire",AI,2022,"DivinAI is an open and collaborative initiative promoted by the European
Commission's Joint Research Centre to measure and monitor diversity indicators
related to AI conferences, with special focus on gender balance, geographical
representation, and presence of academia vs companies. This paper summarizes
the main achievements and lessons learnt during the first year of life of the
DivinAI project, and proposes a set of recommendations for its further
development and maintenance by the AI community.",Monitoring Diversity of AI Conferences: Lessons Learnt and Future Challenges in the DivinAI Project,2022
"Pervaiz Iqbal Khan, Shoaib Ahmed Siddiqui, Imran Razzak, Andreas Dengel, Sheraz Ahmed",AI,2022,"Health mentioning classification (HMC) classifies an input text as health
mention or not. Figurative and non-health mention of disease words makes the
classification task challenging. Learning the context of the input text is the
key to this problem. The idea is to learn word representation by its
surrounding words and utilize emojis in the text to help improve the
classification results. In this paper, we improve the word representation of
the input text using adversarial training that acts as a regularizer during
fine-tuning of the model. We generate adversarial examples by perturbing the
embeddings of the model and then train the model on a pair of clean and
adversarial examples. Additionally, we utilize contrastive loss that pushes a
pair of clean and perturbed examples close to each other and other examples
away in the representation space. We train and evaluate the method on an
extended version of the publicly available PHM2017 dataset. Experiments show an
improvement of 1.0% over BERT-Large baseline and 0.6% over RoBERTa-Large
baseline, whereas 5.8% over the state-of-the-art in terms of F1 score.
Furthermore, we provide a brief analysis of the results by utilizing the power
of explainable AI.",Improving Health Mentioning Classification of Tweets using Contrastive Adversarial Training,2022
"Eli Bogdanov, Izack Cohen, Avigdor Gal",AI,2022,"With the growing number of devices, sensors and digital systems, data logs
may become uncertain due to, e.g., sensor reading inaccuracies or incorrect
interpretation of readings by processing programs. At times, such uncertainties
can be captured stochastically, especially when using probabilistic data
classification models. In this work we focus on conformance checking, which
compares a process model with an event log, when event logs are stochastically
known. Building on existing alignment-based conformance checking fundamentals,
we mathematically define a stochastic trace model, a stochastic synchronous
product, and a cost function that reflects the uncertainty of events in a log.
Then, we search for an optimal alignment over the reachability graph of the
stochastic synchronous product for finding an optimal alignment between a model
and a stochastic process observation. Via structured experiments with two
well-known process mining benchmarks, we explore the behavior of the suggested
stochastic conformance checking approach and compare it to a standard
alignment-based approach as well as to an approach that creates a lower bound
on performance. We envision the proposed stochastic conformance checking
approach as a viable process mining component for future analysis of stochastic
event logs.",Conformance Checking Over Stochastically Known Logs,2022
"Kefan Jin, Xingyao Han",AI,2022,"Environmental disturbances, such as sensor data noises, various lighting
conditions, challenging weathers and external adversarial perturbations, are
inevitable in real self-driving applications. Existing researches and testings
have shown that they can severely influence the vehicles perception ability and
performance, one of the main issue is the false positive detection, i.e., the
ghost object which is not real existed or occurs in the wrong position (such as
a non-existent vehicle). Traditional navigation methods tend to avoid every
detected objects for safety, however, avoiding a ghost object may lead the
vehicle into a even more dangerous situation, such as a sudden break on the
highway. Considering the various disturbance types, it is difficult to address
this issue at the perceptual aspect. A potential solution is to detect the
ghost through relation learning among the whole scenario and develop an
integrated end-to-end navigation system. Our underlying logic is that the
behavior of all vehicles in the scene is influenced by their neighbors, and
normal vehicles behave in a logical way, while ghost vehicles do not. By
learning the spatio-temporal relation among surrounding vehicles, an
information reliability representation is learned for each detected vehicle and
then a robot navigation network is developed. In contrast to existing works, we
encourage the network to learn how to represent the reliability and how to
aggregate all the information with uncertainties by itself, thus increasing the
efficiency and generalizability. To the best of the authors knowledge, this
paper provides the first work on using graph relation learning to achieve
end-to-end robust navigation in the presence of ghost vehicles. Simulation
results in the CARLA platform demonstrate the feasibility and effectiveness of
the proposed method in various scenarios.",Conquering Ghosts: Relation Learning for Information Reliability Representation and End-to-End Robust Navigation,2023
"Xiao Liu, Bonan Gao, Basem Suleiman, Han You, Zisu Ma, Yu Liu, Ali Anaissi",AI,2022,"Recommender systems have been successfully used in many domains with the help
of machine learning algorithms. However, such applications tend to use
multi-dimensional user data, which has raised widespread concerns about the
breach of users privacy. Meanwhile, wearable technologies have enabled users to
collect fitness-related data through embedded sensors to monitor their
conditions or achieve personalized fitness goals. In this paper, we propose a
novel privacy-aware personalized fitness recommender system. We introduce a
multi-level deep learning framework that learns important features from a
large-scale real fitness dataset that is collected from wearable IoT devices to
derive intelligent fitness recommendations. Unlike most existing approaches,
our approach achieves personalization by inferring the fitness characteristics
of users from sensory data and thus minimizing the need for explicitly
collecting user identity or biometric information, such as name, age, height,
weight. In particular, our proposed models and algorithms predict (a)
personalized exercise distance recommendations to help users to achieve target
calories, (b) personalized speed sequence recommendations to adjust exercise
speed given the nature of the exercise and the chosen route, and (c)
personalized heart rate sequence to guide the user of the potential health
status for future exercises. Our experimental evaluation on a real-world Fitbit
dataset demonstrated high accuracy in predicting exercise distance, speed
sequence, and heart rate sequence compared to similar studies. Furthermore, our
approach is novel compared to existing studies as it does not require
collecting and using users sensitive information, and thus it preserves the
users privacy.",Privacy-Preserving Personalized Fitness Recommender System (P3FitRec): A Multi-level Deep Learning Approach,2022
"Vishal Pallagani, Priyadharsini Ramamurthy, Vedant Khandelwal, Revathy Venkataramanan, Kausik Lakkaraju, Sathyanarayanan N. Aakur, Biplav Srivastava",AI,2022,"Food is not only a basic human necessity but also a key factor driving a
society's health and economic well-being. As a result, the cooking domain is a
popular use-case to demonstrate decision-support (AI) capabilities in service
of benefits like precision health with tools ranging from information retrieval
interfaces to task-oriented chatbots. An AI here should understand concepts in
the food domain (e.g., recipes, ingredients), be tolerant to failures
encountered while cooking (e.g., browning of butter), handle allergy-based
substitutions, and work with multiple data modalities (e.g. text and images).
However, the recipes today are handled as textual documents which makes it
difficult for machines to read, reason and handle ambiguity. This demands a
need for better representation of the recipes, overcoming the ambiguity and
sparseness that exists in the current textual documents. In this paper, we
discuss the construction of a machine-understandable rich recipe representation
(R3), in the form of plans, from the recipes available in natural language. R3
is infused with additional knowledge such as information about allergens and
images of ingredients, possible failures and tips for each atomic cooking step.
To show the benefits of R3, we also present TREAT, a tool for recipe retrieval
which uses R3 to perform multi-modal reasoning on the recipe's content (plan
objects - ingredients and cooking tools), food preparation process (plan
actions and time), and media type (image, text). R3 leads to improved retrieval
efficiency and new capabilities that were hither-to not possible in textual
representation.",A Rich Recipe Representation as Plan to Support Expressive Multi Modal Queries on Recipe Content and Preparation Process,2022
"Joyjit Chatterjee, Nina Dethlefs",AI,2022,"Wind energy has emerged as a highly promising source of renewable energy in
recent times. However, wind turbines regularly suffer from operational
inconsistencies, leading to significant costs and challenges in operations and
maintenance (O&M). Condition-based monitoring (CBM) and performance
assessment/analysis of turbines are vital aspects for ensuring efficient O&M
planning and cost minimisation. Data-driven decision making techniques have
witnessed rapid evolution in the wind industry for such O&M tasks during the
last decade, from applying signal processing methods in early 2010 to
artificial intelligence (AI) techniques, especially deep learning in 2020. In
this article, we utilise statistical computing to present a scientometric
review of the conceptual and thematic evolution of AI in the wind energy
sector, providing evidence-based insights into present strengths and
limitations of data-driven decision making in the wind industry. We provide a
perspective into the future and on current key challenges in data availability
and quality, lack of transparency in black box-natured AI models, and
prevailing issues in deploying models for real-time decision support, along
with possible strategies to overcome these problems. We hope that a systematic
analysis of the past, present and future of CBM and performance assessment can
encourage more organisations to adopt data-driven decision making techniques in
O&M towards making wind energy sources more reliable, contributing to the
global efforts of tackling climate change.","Scientometric Review of Artificial Intelligence for Operations & Maintenance of Wind Turbines: The Past, Present and Future",2022
"Till Hofmann, Vaishak Belle",AI,2022,"Abstraction is a commonly used process to represent some low-level system by
a more coarse specification with the goal to omit unnecessary details while
preserving important aspects. While recent work on abstraction in the situation
calculus has focused on non-probabilistic domains, we describe an approach to
abstraction of probabilistic and dynamic systems. Based on a variant of the
situation calculus with probabilistic belief, we define a notion of
bisimulation that allows to abstract a detailed probabilistic basic action
theory with noisy actuators and sensors by a possibly non-stochastic basic
action theory. By doing so, we obtain abstract Golog programs that omit
unnecessary details and which can be translated back to a detailed program for
actual execution. This simplifies the implementation of noisy robot programs,
opens up the possibility of using non-stochastic reasoning methods (e.g.,
planning) on probabilistic problems, and provides domain descriptions that are
more easily understandable and explainable.",Abstracting Noisy Robot Programs,2023
"Selene Baez Santamaria, Emmanouil Manousogiannis, Guusje Boomgaard, Linh P. Tran, Zoltan Szlavik, Robert-Jan Sips",AI,2022,"Background: Access to medical care is strongly dependent on resource
allocation, such as the geographical distribution of medical facilities.
Nevertheless, this data is usually restricted to country official
documentation, not available to the public. While some medical facilities' data
is accessible as semantic resources on the Web, it is not consistent in its
modeling and has yet to be integrated into a complete, open, and specialized
repository. This work focuses on generating a comprehensive semantic dataset of
medical facilities worldwide containing extensive information about such
facilities' geo-location.
  Results: For this purpose, we collect, align, and link various open-source
databases where medical facilities' information may be present. This work
allows us to evaluate each data source along various dimensions, such as
completeness, correctness, and interlinking with other sources, all critical
aspects of current knowledge representation technologies.
  Conclusions: Our contributions directly benefit stakeholders in the
biomedical and health domain (patients, healthcare professionals, companies,
regulatory authorities, and researchers), who will now have a better overview
of the access to and distribution of medical facilities.",Access to care: analysis of the geographical distribution of healthcare using Linked Open Data,2022
"Jacqueline K. Kueper, Jennifer Rayner, Daniel J. Lizotte",AI,2022,"Electronic health records (EHRs) include simple features like patient age
together with more complex data like care history that are informative but not
easily represented as individual features. To better harness such data, we
developed an interpretable hybrid feature- and similarity-based model for
supervised learning that combines feature and kernel learning for prediction
and for investigation of causal relationships. We fit our hybrid models by
convex optimization with a sparsity-inducing penalty on the kernel. Depending
on the desired model interpretation, the feature and kernel coefficients can be
learned sequentially or simultaneously. The hybrid models showed comparable or
better predictive performance than solely feature- or similarity-based
approaches in a simulation study and in a case study to predict two-year risk
of loneliness or social isolation with EHR data from a complex primary health
care population. Using the case study we also present new kernels for
high-dimensional indicator-coded EHR data that are based on deviations from
population-level expectations, and we identify considerations for causal
interpretations.",Hybrid Feature- and Similarity-Based Models for Joint Prediction and Interpretation,2023
"Gyunam Park, Jan Niklas Adams, Wil. M. P. van der Aalst",AI,2022,"Performance analysis in process mining aims to provide insights on the
performance of a business process by using a process model as a formal
representation of the process. Such insights are reliably interpreted by
process analysts in the context of a model with formal semantics. Existing
techniques for performance analysis assume that a single case notion exists in
a business process (e.g., a patient in healthcare process). However, in
reality, different objects might interact (e.g., order, item, delivery, and
invoice in an O2C process). In such a setting, traditional techniques may yield
misleading or even incorrect insights on performance metrics such as waiting
time. More importantly, by considering the interaction between objects, we can
define object-centric performance metrics such as synchronization time, pooling
time, and lagging time. In this work, we propose a novel approach to
performance analysis considering multiple case notions by using object-centric
Petri nets as formal representations of business processes. The proposed
approach correctly computes existing performance metrics, while supporting the
derivation of newly-introduced object-centric performance metrics. We have
implemented the approach as a web application and conducted a case study based
on a real-life loan application process.",OPerA: Object-Centric Performance Analysis,2022
"Jayetri Bardhan, Anthony Colas, Kirk Roberts, Daisy Zhe Wang",AI,2022,"This paper develops the first question answering dataset (DrugEHRQA)
containing question-answer pairs from both structured tables and unstructured
notes from a publicly available Electronic Health Record (EHR). EHRs contain
patient records, stored in structured tables and unstructured clinical notes.
The information in structured and unstructured EHRs is not strictly disjoint:
information may be duplicated, contradictory, or provide additional context
between these sources. Our dataset has medication-related queries, containing
over 70,000 question-answer pairs. To provide a baseline model and help analyze
the dataset, we have used a simple model (MultimodalEHRQA) which uses the
predictions of a modality selection network to choose between EHR tables and
clinical notes to answer the questions. This is used to direct the questions to
the table-based or text-based state-of-the-art QA model. In order to address
the problem arising from complex, nested queries, this is the first time
Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers (RAT-SQL)
has been used to test the structure of query templates in EHR data. Our goal is
to provide a benchmark dataset for multi-modal QA systems, and to open up new
avenues of research in improving question answering over EHR structured data by
using context from unstructured clinical data.",DrugEHRQA: A Question Answering Dataset on Structured and Unstructured Electronic Health Records For Medicine Related Queries,2022
Annet Onnes,AI,2022,"Knowledge-based systems have been used to monitor machines and processes in
the real world. In this paper we propose the use of knowledge-based systems to
monitor other AI systems in operation. We motivate and provide a problem
analysis of this novel setting and subsequently propose a framework that allows
for structuring future research related to this setting. Several directions for
further research are also discussed.","Monitoring AI systems: A Problem Analysis, Framework and Outlook",2022
"Hammaad Adam, Ming Ying Yang, Kenrick Cato, Ioana Baldini, Charles Senteio, Leo Anthony Celi, Jiaming Zeng, Moninder Singh, Marzyeh Ghassemi",AI,2022,"Clinical notes are becoming an increasingly important data source for machine
learning (ML) applications in healthcare. Prior research has shown that
deploying ML models can perpetuate existing biases against racial minorities,
as bias can be implicitly embedded in data. In this study, we investigate the
level of implicit race information available to ML models and human experts and
the implications of model-detectable differences in clinical notes. Our work
makes three key contributions. First, we find that models can identify patient
self-reported race from clinical notes even when the notes are stripped of
explicit indicators of race. Second, we determine that human experts are not
able to accurately predict patient race from the same redacted clinical notes.
Finally, we demonstrate the potential harm of this implicit information in a
simulation study, and show that models trained on these race-redacted clinical
notes can still perpetuate existing biases in clinical treatment decisions.",Write It Like You See It: Detectable Differences in Clinical Notes By Race Lead To Differential Model Recommendations,2022
"Marko Tesic, Ulrike Hahn",AI,2022,"Counterfactual (CF) explanations have been employed as one of the modes of
explainability in explainable AI-both to increase the transparency of AI
systems and to provide recourse. Cognitive science and psychology, however,
have pointed out that people regularly use CFs to express causal relationships.
Most AI systems are only able to capture associations or correlations in data
so interpreting them as casual would not be justified. In this paper, we
present two experiment (total N = 364) exploring the effects of CF explanations
of AI system's predictions on lay people's causal beliefs about the real world.
In Experiment 1 we found that providing CF explanations of an AI system's
predictions does indeed (unjustifiably) affect people's causal beliefs
regarding factors/features the AI uses and that people are more likely to view
them as causal factors in the real world. Inspired by the literature on
misinformation and health warning messaging, Experiment 2 tested whether we can
correct for the unjustified change in causal beliefs. We found that pointing
out that AI systems capture correlations and not necessarily causal
relationships can attenuate the effects of CF explanations on people's causal
beliefs.","Can counterfactual explanations of AI systems' predictions skew lay users' causal intuitions about the world? If so, can we correct for that?",2022
"Corina Catarau-Cotutiu, Esther Mondragon, Eduardo Alonso",AI,2022,"Inspired by cognitive theories of creativity, this paper introduces a
computational model (AIGenC) that lays down the necessary components to enable
artificial agents to learn, use and generate transferable representations.
Unlike machine representation learning, which relies exclusively on raw sensory
data, biological representations incorporate relational and associative
information that embeds rich and structured concept spaces. The AIGenC model
poses a hierarchical graph architecture with various levels and types of
representations procured by different components. The first component, Concept
Processing, extracts objects and affordances from sensory input and encodes
them into a concept space. The resulting representations are stored in a dual
memory system and enriched with goal-directed and temporal information acquired
through reinforcement learning, creating a higher-level of abstraction. Two
additional components work in parallel to detect and recover relevant concepts
and create new ones, respectively, in a process akin to cognitive Reflective
Reasoning and Blending. The Reflective Reasoning unit detects and recovers from
memory concepts relevant to the task by means of a matching process that
calculates a similarity value between the current state and memory graph
structures. Once the matching interaction ends, rewards and temporal
information are added to the graph, building further abstractions. If the
reflective reasoning processing fails to offer a suitable solution, a blending
operation comes into place, creating new concepts by combining past
information. We discuss the model's capability to yield better
out-of-distribution generalisation in artificial agents, thus advancing toward
Artificial General Intelligence.",AIGenC: An AI generalisation model via creativity,2023
"Md Sazzad Hossain, Pritom Saha, Townim Faisal Chowdhury, Shafin Rahman, Fuad Rahman, Nabeel Mohammed",AI,2022,"It is common to have continuous streams of new data that need to be
introduced in the system in real-world applications. The model needs to learn
newly added capabilities (future tasks) while retaining the old knowledge (past
tasks). Incremental learning has recently become increasingly appealing for
this problem. Task-incremental learning is a kind of incremental learning where
task identity of newly included task (a set of classes) remains known during
inference. A common goal of task-incremental methods is to design a network
that can operate on minimal size, maintaining decent performance. To manage the
stability-plasticity dilemma, different methods utilize replay memory of past
tasks, specialized hardware, regularization monitoring etc. However, these
methods are still less memory efficient in terms of architecture growth or
input data costs. In this study, we present a simple yet effective adjustment
network (SAN) for task incremental learning that achieves near state-of-the-art
performance while using minimal architectural size without using memory
instances compared to previous state-of-the-art approaches. We investigate this
approach on both 3D point cloud object (ModelNet40) and 2D image (CIFAR10,
CIFAR100, MiniImageNet, MNIST, PermutedMNIST, notMNIST, SVHN, and FashionMNIST)
recognition tasks and establish a strong baseline result for a fair comparison
with existing methods. On both 2D and 3D domains, we also observe that SAN is
primarily unaffected by different task orders in a task-incremental setting.",Rethinking Task-Incremental Learning Baselines,2022
"Robert K. Helmeczi, Can Kavaklioglu, Mucahit Cevik, Davood Pirayesh Neghab",AI,2022,"Breast cancer is a common and deadly disease, but it is often curable when
diagnosed early. While most countries have large-scale screening programs,
there is no consensus on a single globally accepted guideline for breast cancer
screening. The complex nature of the disease; the limited availability of
screening methods such as mammography, magnetic resonance imaging (MRI), and
ultrasound; and public health policies all factor into the development of
screening policies. Resource availability concerns necessitate the design of
policies which conform to a budget, a problem which can be modelled as a
constrained partially observable Markov decision process (CPOMDP). In this
study, we propose a multi-objective CPOMDP model for breast cancer screening
which allows for supplemental screening methods to accompany mammography. The
model has two objectives: maximize the quality-adjusted life years (QALYs) and
minimize lifetime breast cancer mortality risk (LBCMR). We identify the Pareto
frontier of optimal solutions for average and high-risk patients at different
budget levels, which can be used by decision-makers to set policies in
practice. We find that the policies obtained by using a weighted objective are
able to generate well-balanced QALYs and LBCMR values. In contrast, the
single-objective models generally sacrifice a substantial amount in terms of
QALYs/LBCMR for a minimal gain in LBCMR/QALYs. Additionally, our results show
that, with the baseline cost values for supplemental screenings as well as the
additional disutility that they incur, they are rarely recommended in CPOMDP
policies, especially in a budget-constrained setting. A sensitivity analysis
reveals the thresholds on cost and disutility values at which supplemental
screenings become advantageous to prescribe.",A multi-objective constrained POMDP model for breast cancer screening,2023
"Gyunam Park, Janik-Vasily Benzin, Wil M. P. van der Aalst",AI,2022,"A deviation detection aims to detect deviating process instances, e.g.,
patients in the healthcare process and products in the manufacturing process. A
business process of an organization is executed in various contextual
situations, e.g., a COVID-19 pandemic in the case of hospitals and a lack of
semiconductor chip shortage in the case of automobile companies. Thus,
context-aware deviation detection is essential to provide relevant insights.
However, existing work 1) does not provide a systematic way of incorporating
various contexts, 2) is tailored to a specific approach without using an
extensive pool of existing deviation detection techniques, and 3) does not
distinguish positive and negative contexts that justify and refute deviation,
respectively. In this work, we provide a framework to bridge the aforementioned
gaps. We have implemented the proposed framework as a web service that can be
extended to various contexts and deviation detection methods. We have evaluated
the effectiveness of the proposed framework by conducting experiments using 255
different contextual scenarios.",Detecting Context-Aware Deviations in Process Executions,2022
"Wang Lu, Jindong Wang, Yiqiang Chen, Sinno Jialin Pan, Chunyu Hu, Xin Qin",AI,2022,"It is expensive and time-consuming to collect sufficient labeled data to
build human activity recognition (HAR) models. Training on existing data often
makes the model biased towards the distribution of the training data, thus the
model might perform terribly on test data with different distributions.
Although existing efforts on transfer learning and domain adaptation try to
solve the above problem, they still need access to unlabeled data on the target
domain, which may not be possible in real scenarios. Few works pay attention to
training a model that can generalize well to unseen target domains for HAR. In
this paper, we propose a novel method called Semantic-Discriminative Mixup
(SDMix) for generalizable cross-domain HAR. Firstly, we introduce
semantic-aware Mixup that considers the activity semantic ranges to overcome
the semantic inconsistency brought by domain differences. Secondly, we
introduce the large margin loss to enhance the discrimination of Mixup to
prevent misclassification brought by noisy virtual labels. Comprehensive
generalization experiments on five public datasets demonstrate that our SDMix
substantially outperforms the state-of-the-art approaches with 6% average
accuracy improvement on cross-person, cross-dataset, and cross-position HAR.",Semantic-Discriminative Mixup for Generalizable Sensor-based Cross-domain Activity Recognition,2022
"Ryan Nguyen, Shubhendu Kumar Singh, Rahul Rai",AI,2022,"Prognostics aid in the longevity of fielded systems or products. Quantifying
the system's current health enable prognosis to enhance the operator's
decision-making to preserve the system's health. Creating a prognosis for a
system can be difficult due to (a) unknown physical relationships and/or (b)
irregularities in data appearing well beyond the initiation of a problem.
Traditionally, three different modeling paradigms have been used to develop a
prognostics model: physics-based (PbM), data-driven (DDM), and hybrid modeling.
Recently, the hybrid modeling approach that combines the strength of both PbM
and DDM based approaches and alleviates their limitations is gaining traction
in the prognostics domain. In this paper, a novel hybrid modeling approach for
prognostics applications based on combining concepts from fuzzy logic and
generative adversarial networks (GANs) is outlined. The FuzzyGAN based method
embeds a physics-based model in the aggregation of the fuzzy implications. This
technique constrains the output of the learning method to a realistic solution.
Results on a bearing problem showcases the efficacy of adding a physics-based
aggregation in a fuzzy logic model to improve GAN's ability to model health and
give a more accurate system prognosis.",Physics-Infused Fuzzy Generative Adversarial Network for Robust Failure Prognosis,2022
"Shreyas Gawde, Shruti Patil, Satish Kumar, Pooja Kamat, Ketan Kotecha, Ajith Abraham",AI,2022,"Industry 4.0 is an era of smart manufacturing. Manufacturing is impossible
without the use of machinery. Majority of these machines comprise rotating
components and are called rotating machines. The engineers' top priority is to
maintain these critical machines to reduce the unplanned shutdown and increase
the useful life of machinery. Predictive maintenance (PDM) is the current trend
of smart maintenance. The challenging task in PDM is to diagnose the type of
fault. With Artificial Intelligence (AI) advancement, data-driven approach for
predictive maintenance is taking a new flight towards smart manufacturing.
Several researchers have published work related to fault diagnosis in rotating
machines, mainly exploring a single type of fault. However, a consolidated
review of literature that focuses more on multi-fault diagnosis of rotating
machines is lacking. There is a need to systematically cover all the aspects
right from sensor selection, data acquisition, feature extraction, multi-sensor
data fusion to the systematic review of AI techniques employed in multi-fault
diagnosis. In this regard, this paper attempts to achieve the same by
implementing a systematic literature review on a Data-driven approach for
multi-fault diagnosis of Industrial Rotating Machines using Preferred Reporting
Items for Systematic Reviews and Meta-Analysis (PRISMA) method. The PRISMA
method is a collection of guidelines for the composition and structure of
systematic reviews and other meta-analyses. This paper identifies the
foundational work done in the field and gives a comparative study of different
aspects related to multi-fault diagnosis of industrial rotating machines. The
paper also identifies the major challenges, research gap. It gives solutions
using recent advancements in AI in implementing multi-fault diagnosis, giving a
strong base for future research in this field.",Multi-Fault Diagnosis Of Industrial Rotating Machines Using Data-Driven Approach: A Review Of Two Decades Of Research,2022
"Quoc Hung Ngo, Tahar Kechadi, Nhien-An Le-Khac",AI,2022,"In recent years, data science has evolved significantly. Data analysis and
mining processes become routines in all sectors of the economy where datasets
are available. Vast data repositories have been collected, curated, stored, and
used for extracting knowledge. And this is becoming commonplace. Subsequently,
we extract a large amount of knowledge, either directly from the data or
through experts in the given domain. The challenge now is how to exploit all
this large amount of knowledge that is previously known for efficient
decision-making processes. Until recently, much of the knowledge gained through
a number of years of research is stored in static knowledge bases or
ontologies, while more diverse and dynamic knowledge acquired from data mining
studies is not centrally and consistently managed. In this research, we propose
a novel model called ontology-based knowledge map to represent and store the
results (knowledge) of data mining in crop farming to build, maintain, and
enrich the process of knowledge discovery. The proposed model consists of six
main sets: concepts, attributes, relations, transformations, instances, and
states. This model is dynamic and facilitates the access, updates, and
exploitation of the knowledge at any time. This paper also proposes an
architecture for handling this knowledge-based model. The system architecture
includes knowledge modelling, extraction, assessment, publishing, and
exploitation. This system has been implemented and used in agriculture for crop
management and monitoring. It is proven to be very effective and promising for
its extension to other domains.",Knowledge Representation in Digital Agriculture: A Step Towards Standardised Model,2022
"Tim Franzmeyer, Stephen McAleer, João F. Henriques, Jakob N. Foerster, Philip H. S. Torr, Adel Bibi, Christian Schroeder de Witt",AI,2022,"Autonomous agents deployed in the real world need to be robust against
adversarial attacks on sensory inputs. Robustifying agent policies requires
anticipating the strongest attacks possible. We demonstrate that existing
observation-space attacks on reinforcement learning agents have a common
weakness: while effective, their lack of temporal consistency makes them
detectable using automated means or human inspection. Detectability is
undesirable to adversaries as it may trigger security escalations. We introduce
perfect illusory attacks, a novel form of adversarial attack on sequential
decision-makers that is both effective and provably statistically undetectable.
We then propose the more versatile R-attacks, which result in observation
transitions that are consistent with the state-transition function of the
adversary-free environment and can be learned end-to-end. Compared to existing
attacks, we empirically find R-attacks to be significantly harder to detect
with automated methods, and a small study with human subjects suggests they are
similarly harder to detect for humans. We propose that undetectability should
be a central concern in the study of adversarial attacks on mixed-autonomy
settings.",Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers,2023
"Till Hofmann, Vaishak Belle",AI,2022,"A robot's actions are inherently stochastic, as its sensors are noisy and its
actions do not always have the intended effects. For this reason, the agent
language Golog has been extended to models with degrees of belief and
stochastic actions. While this allows more precise robot models, the resulting
programs are much harder to comprehend, because they need to deal with the
noise, e.g., by looping until some desired state has been reached with
certainty, and because the resulting action traces consist of a large number of
actions cluttered with sensor noise. To alleviate these issues, we propose to
use abstraction. We define a high-level and nonstochastic model of the robot
and then map the high-level model into the lower-level stochastic model. The
resulting programs are much easier to understand, often do not require belief
operators or loops, and produce much shorter action traces.",Using Abstraction for Interpretable Robot Programs in Stochastic Domains,2022
Matteo Cardellini,AI,2022,"Avoiding congestion and controlling traffic in urban scenarios is becoming
nowadays of paramount importance due to the rapid growth of our cities'
population and vehicles. The effective control of urban traffic as a means to
mitigate congestion can be beneficial in an economic, environmental and health
way. In this paper, a framework which allows to efficiently simulate and
optimize traffic flow in a large roads' network with hundreds of vehicles is
presented. The framework leverages on an Answer Set Programming (ASP) encoding
to formally describe the movements of vehicles inside a network. Taking
advantage of the ability to specify optimization constraints in ASP and the
off-the-shelf solver Clingo, it is then possible to optimize the routes of
vehicles inside the network to reduce a range of relevant metrics (e.g., travel
times or emissions). Finally, an analysis on real-world traffic data is
performed, utilizing the state-of-the-art Urban Mobility Simulator (SUMO) to
keep track of the state of the network, test the correctness of the solution
and to prove the efficiency and capabilities of the presented solution.",An ASP Framework for Efficient Urban Traffic Optimization,2022
"Haixiao Chi, Dawei Wang, Gaojie Cui, Feng Mao, Beishui Liao",AI,2022,"Interpretability has become an essential topic for artificial intelligence in
some high-risk domains such as healthcare, bank and security. For commonly-used
tabular data, traditional methods trained end-to-end machine learning models
with numerical and categorical data only, and did not leverage human
understandable knowledge such as data descriptions. Yet mining human-level
knowledge from tabular data and using it for prediction remain a challenge.
Therefore, we propose a concept and argumentation based model (CAM) that
includes the following two components: a novel concept mining method to obtain
human understandable concepts and their relations from both descriptions of
features and the underlying data, and a quantitative argumentation-based method
to do knowledge representation and reasoning. As a result of it, CAM provides
decisions that are based on human-level knowledge and the reasoning process is
intrinsically interpretable. Finally, to visualize the purposed interpretable
model, we provide a dialogical explanation that contain dominated reasoning
path within CAM. Experimental results on both open source benchmark dataset and
real-word business dataset show that (1) CAM is transparent and interpretable,
and the knowledge inside the CAM is coherent with human understanding; (2) Our
interpretable approach can reach competitive results comparing with other
state-of-art models.",A Concept and Argumentation based Interpretable Model in High Risk Domains,2022
"Martin Glauer, Robert West, Susan Michie, Janna Hastings",AI,2022,"We describe a novel approach to explainable prediction of a continuous
variable based on learning fuzzy weighted rules. Our model trains a set of
weighted rules to maximise prediction accuracy and minimise an ontology-based
'semantic loss' function including user-specified constraints on the rules that
should be learned in order to maximise the explainability of the resulting rule
set from a user perspective. This system fuses quantitative sub-symbolic
learning with symbolic learning and constraints based on domain knowledge. We
illustrate our system on a case study in predicting the outcomes of behavioural
interventions for smoking cessation, and show that it outperforms other
interpretable approaches, achieving performance close to that of a deep
learning model, while offering transparent explainability that is an essential
requirement for decision-makers in the health domain.","ESC-Rules: Explainable, Semantically Constrained Rule Sets",2022
Sophia Sun,AI,2022,"Machine learning methods are increasingly widely used in high-risk settings
such as healthcare, transportation, and finance. In these settings, it is
important that a model produces calibrated uncertainty to reflect its own
confidence and avoid failures. In this paper we survey recent works on
uncertainty quantification (UQ) for deep learning, in particular
distribution-free Conformal Prediction method for its mathematical properties
and wide applicability. We will cover the theoretical guarantees of conformal
methods, introduce techniques that improve calibration and efficiency for UQ in
the context of spatiotemporal data, and discuss the role of UQ in the context
of safe decision making.",Conformal Methods for Quantifying Uncertainty in Spatiotemporal Data: A Survey,2022
"Rajeswari Chengoden, Nancy Victor, Thien Huynh-The, Gokul Yenduri, Rutvij H. Jhaveri, Mamoun Alazab, Sweta Bhattacharya, Pawan Hegde, Praveen Kumar Reddy Maddikunta, Thippa Reddy Gadekallu",AI,2022,"The rapid progress in digitalization and automation have led to an
accelerated growth in healthcare, generating novel models that are creating new
channels for rendering treatment with reduced cost. The Metaverse is an
emerging technology in the digital space which has huge potential in
healthcare, enabling realistic experiences to the patients as well as the
medical practitioners. The Metaverse is a confluence of multiple enabling
technologies such as artificial intelligence, virtual reality, augmented
reality, internet of medical devices, robotics, quantum computing, etc. through
which new directions for providing quality healthcare treatment and services
can be explored. The amalgamation of these technologies ensures immersive,
intimate and personalized patient care. It also provides adaptive intelligent
solutions that eliminates the barriers between healthcare providers and
receivers. This article provides a comprehensive review of the Metaverse for
healthcare, emphasizing on the state of the art, the enabling technologies for
adopting the Metaverse for healthcare, the potential applications and the
related projects. The issues in the adaptation of the Metaverse for healthcare
applications are also identified and the plausible solutions are highlighted as
part of future research directions.","Metaverse for Healthcare: A Survey on Potential Applications, Challenges and Future Directions",2022
"Jiawei Zheng, Petros Papapanagiotou, Jacques D. Fleuriot",AI,2022,"Conformance checking techniques allow us to evaluate how well some exhibited
behaviour, represented by a trace of monitored events, conforms to a specified
process model. Modern monitoring and activity recognition technologies, such as
those relying on sensors, the IoT, statistics and AI, can produce a wealth of
relevant event data. However, this data is typically characterised by noise and
uncertainty, in contrast to the assumption of a deterministic event log
required by conformance checking algorithms. In this paper, we extend
alignment-based conformance checking to function under a probabilistic event
log. We introduce a weighted trace model and weighted alignment cost function,
and a custom threshold parameter that controls the level of confidence on the
event data vs. the process model. The resulting algorithm considers activities
of lower but sufficiently high probability that better align with the process
model. We explain the algorithm and its motivation both from formal and
intuitive perspectives, and demonstrate its functionality in comparison with
deterministic alignment using real-life datasets.",Alignment-based conformance checking over probabilistic events,2023
"David Piorkowski, Michael Hind, John Richards",AI,2022,"Although AI-based systems are increasingly being leveraged to provide value
to organizations, individuals, and society, significant attendant risks have
been identified. These risks have led to proposed regulations, litigation, and
general societal concerns.
  As with any promising technology, organizations want to benefit from the
positive capabilities of AI technology while reducing the risks. The best way
to reduce risks is to implement comprehensive AI lifecycle governance where
policies and procedures are described and enforced during the design,
development, deployment, and monitoring of an AI system. While support for
comprehensive governance is beginning to emerge, organizations often need to
identify the risks of deploying an already-built model without knowledge of how
it was constructed or access to its original developers.
  Such an assessment will quantitatively assess the risks of an existing model
in a manner analogous to how a home inspector might assess the energy
efficiency of an already-built home or a physician might assess overall patient
health based on a battery of tests. This paper explores the concept of a
quantitative AI Risk Assessment, exploring the opportunities, challenges, and
potential impacts of such an approach, and discussing how it might improve AI
regulations.",Quantitative AI Risk Assessments: Opportunities and Challenges,2023
"Maria Krantz, Alexander Windmann, Rene Heesch, Lukas Moddemann, Oliver Niggemann",AI,2022,"The increasing complexity of Cyber-Physical Systems (CPS) makes industrial
automation challenging. Large amounts of data recorded by sensors need to be
processed to adequately perform tasks such as diagnosis in case of fault. A
promising approach to deal with this complexity is the concept of causality.
However, most research on causality has focused on inferring causal relations
between parts of an unknown system. Engineering uses causality in a
fundamentally different way: complex systems are constructed by combining
components with known, controllable behavior. As CPS are constructed by the
second approach, most data-based causality models are not suited for industrial
automation. To bridge this gap, a Uniform Causality Model for various
application areas of industrial automation is proposed, which will allow better
communication and better data usage across disciplines. The resulting model
describes the behavior of CPS mathematically and, as the model is evaluated on
the unique requirements of the application areas, it is shown that the Uniform
Causality Model can work as a basis for the application of new approaches in
industrial automation that focus on machine learning.",On a Uniform Causality Model for Industrial Automation,2022
"Gideon Vos, Kelly Trinh, Zoltan Sarnyai, Mostafa Rahimi Azghadi",AI,2022,"Introduction. The stress response has both subjective, psychological and
objectively measurable, biological components. Both of them can be expressed
differently from person to person, complicating the development of a generic
stress measurement model. This is further compounded by the lack of large,
labeled datasets that can be utilized to build machine learning models for
accurately detecting periods and levels of stress. The aim of this review is to
provide an overview of the current state of stress detection and monitoring
using wearable devices, and where applicable, machine learning techniques
utilized.
  Methods. This study reviewed published works contributing and/or using
datasets designed for detecting stress and their associated machine learning
methods, with a systematic review and meta-analysis of those that utilized
wearable sensor data as stress biomarkers. The electronic databases of Google
Scholar, Crossref, DOAJ and PubMed were searched for relevant articles and a
total of 24 articles were identified and included in the final analysis. The
reviewed works were synthesized into three categories of publicly available
stress datasets, machine learning, and future research directions.
  Results. A wide variety of study-specific test and measurement protocols were
noted in the literature. A number of public datasets were identified that are
labeled for stress detection. In addition, we discuss that previous works show
shortcomings in areas such as their labeling protocols, lack of statistical
power, validity of stress biomarkers, and generalization ability.
  Conclusion. Generalization of existing machine learning models still require
further study, and research in this area will continue to provide improvements
as newer and more substantial datasets become available for study.",Generalizable machine learning for stress monitoring from wearable devices: A systematic literature review,2023
"Richard G. Freedman, Joseph B. Mueller, Jack Ladwig, Steven Johnston, David McDonald, Helen Wauck, Ruta Wheelock, Hayley Borck",AI,2022,"Robots that interact with humans in a physical space or application need to
think about the person's posture, which typically comes from visual sensors
like cameras and infra-red. Artificial intelligence and machine learning
algorithms use information from these sensors either directly or after some
level of symbolic abstraction, and the latter usually partitions the range of
observed values to discretize the continuous signal data. Although these
representations have been effective in a variety of algorithms with respect to
accuracy and task completion, the underlying models are rarely interpretable,
which also makes their outputs more difficult to explain to people who request
them. Instead of focusing on the possible sensor values that are familiar to a
machine, we introduce a qualitative spatial reasoning approach that describes
the human posture in terms that are more familiar to people. This paper
explores the derivation of our symbolic representation at two levels of detail
and its preliminary use as features for interpretable activity recognition.",A Symbolic Representation of Human Posture for Interpretable Learning and Reasoning,2022
"Chun-Kit Ngan, Alexander Brodsky",AI,2022,"We propose a Web-Mashup Application Service Framework for Multivariate Time
Series Analytics (MTSA) that supports the services of model definitions,
querying, parameter learning, model evaluations, data monitoring, decision
recommendations, and web portals. This framework maintains the advantage of
combining the strengths of both the domain-knowledge-based and the
formal-learning-based approaches and is designed for a more general class of
problems over multivariate time series. More specifically, we identify a
general-hybrid-based model, MTSA-Parameter Estimation, to solve this class of
problems in which the objective function is maximized or minimized from the
optimal decision parameters regardless of particular time points. This model
also allows domain experts to include multiple types of constraints, e.g.,
global constraints and monitoring constraints. We further extend the MTSA data
model and query language to support this class of problems for the services of
learning, monitoring, and recommendation. At the end, we conduct an
experimental case study for a university campus microgrid as a practical
example to demonstrate our proposed framework, models, and language.",Optimal Event Monitoring through Internet Mashup over Multivariate Time Series,2022
"Gyunam Park, Wil. M. P. van der Aalst",AI,2022,"Constraint monitoring aims to monitor the violation of constraints in
business processes, e.g., an invoice should be cleared within 48 hours after
the corresponding goods receipt, by analyzing event data. Existing techniques
for constraint monitoring assume that a single case notion exists in a business
process, e.g., a patient in a healthcare process, and each event is associated
with the case notion. However, in reality, business processes are
object-centric, i.e., multiple case notions (objects) exist, and an event may
be associated with multiple objects. For instance, an Order-To-Cash (O2C)
process involves order, item, delivery, etc., and they interact when executing
an event, e.g., packing multiple items together for a delivery. The existing
techniques produce misleading insights when applied to such object-centric
business processes. In this work, we propose an approach to monitoring
constraints in object-centric business processes. To this end, we introduce
Object-Centric Constraint Graphs (OCCGs) to represent constraints that consider
the interaction of objects. Next, we evaluate the constraints represented by
OCCGs by analyzing Object-Centric Event Logs (OCELs) that store the interaction
of different objects in events. We have implemented a web application to
support the proposed approach and conducted two case studies using a real-life
SAP ERP system.",Monitoring Constraints in Business Processes Using Object-Centric Constraint Graphs,2022
"Liang Peng, Boqi Li, Wenhao Yu, Kai Yang, Wenbo Shao, Hong Wang",AI,2022,"Autonomous driving confronts great challenges in complex traffic scenarios,
where the risk of Safety of the Intended Functionality (SOTIF) can be triggered
by the dynamic operational environment and system insufficiencies. The SOTIF
risk is reflected not only intuitively in the collision risk with objects
outside the autonomous vehicles (AVs), but also inherently in the performance
limitation risk of the implemented algorithms themselves. How to minimize the
SOTIF risk for autonomous driving is currently a critical, difficult, and
unresolved issue. Therefore, this paper proposes the ""Self-Surveillance and
Self-Adaption System"" as a systematic approach to online minimize the SOTIF
risk, which aims to provide a systematic solution for monitoring,
quantification, and mitigation of inherent and external risks. The core of this
system is the risk monitoring of the implemented artificial intelligence
algorithms within the AV. As a demonstration of the Self-Surveillance and
Self-Adaption System, the risk monitoring of the perception algorithm, i.e.,
YOLOv5 is highlighted. Moreover, the inherent perception algorithm risk and
external collision risk are jointly quantified via SOTIF entropy, which is then
propagated downstream to the decision-making module and mitigated. Finally,
several challenging scenarios are demonstrated, and the Hardware-in-the-Loop
experiments are conducted to verify the efficiency and effectiveness of the
system. The results demonstrate that the Self-Surveillance and Self-Adaption
System enables dependable online monitoring, quantification, and mitigation of
SOTIF risk in real-time critical traffic environments.",SOTIF Entropy: Online SOTIF Risk Quantification and Mitigation for Autonomous Driving,2022
"Sergey Zeltyn, Segev Shlomov, Avi Yaeli, Alon Oved",AI,2022,"Business processes that involve AI-powered automation have been gaining
importance and market share in recent years. These business processes combine
the characteristics of classical business process management, goal-driven
chatbots, conversational recommendation systems, and robotic process
automation. In the new context, prescriptive process monitoring demands
innovative approaches. Unfortunately, data logs from these new processes are
still not available in the public domain. We describe the main challenges in
this new domain and introduce a synthesized dataset that is based on an actual
use case of intelligent process automation with chatbot orchestration. Using
this dataset, we demonstrate crowd-wisdom and goal-driven approaches to
prescriptive process monitoring.",Prescriptive Process Monitoring in Intelligent Process Automation with Chatbot Orchestration,2022
"Abdulaziz Ahmed, Mohammed Al-Maamari, Mohammad Firouz, Dursun Delen",AI,2022,"Patient triage at emergency departments (EDs) is necessary to prioritize care
for patients with critical and time-sensitive conditions. Different tools are
used for patient triage and one of the most common ones is the emergency
severity index (ESI), which has a scale of five levels, where level 1 is the
most urgent and level 5 is the least urgent. This paper proposes a framework
for utilizing machine learning to develop an e-triage tool that can be used at
EDs. A large retrospective dataset of ED patient visits is obtained from the
electronic health record of a healthcare provider in the Midwest of the US for
three years. However, the main challenge of using machine learning algorithms
is that most of them have many parameters and without optimizing these
parameters, developing a high-performance model is not possible. This paper
proposes an approach to optimize the hyperparameters of machine learning. The
metaheuristic optimization algorithms simulated annealing (SA) and adaptive
simulated annealing (ASA) are proposed to optimize the parameters of extreme
gradient boosting (XGB) and categorical boosting (CaB). The newly proposed
algorithms are SA-XGB, ASA-XGB, SA-CaB, ASA-CaB. Grid search (GS), which is a
traditional approach used for machine learning fine-tunning is also used to
fine-tune the parameters of XGB and CaB, which are named GS-XGB and GS-CaB. The
six algorithms are trained and tested using eight data groups obtained from the
feature selection phase. The results show ASA-CaB outperformed all the proposed
algorithms with accuracy, precision, recall, and f1 of 83.3%, 83.2%, 83.3%,
83.2%, respectively.",An Adaptive Simulated Annealing-Based Machine Learning Approach for Developing an E-Triage Tool for Hospital Emergency Operations,2022
"Ritesh Chandra, Sadhana Tiwari, Sonali Agarwal, Navjot Singh",AI,2023,"Vector-borne diseases (VBDs) are a kind of infection caused through the
transmission of vectors generated by the bites of infected parasites, bacteria,
and viruses, such as ticks, mosquitoes, triatomine bugs, blackflies, and
sandflies. If these diseases are not properly treated within a reasonable time
frame, the mortality rate may rise. In this work, we propose a set of
ontologies that will help in the diagnosis and treatment of vector-borne
diseases. For developing VBD's ontology, electronic health records taken from
the Indian Health Records website, text data generated from Indian government
medical mobile applications, and doctors' prescribed handwritten notes of
patients are used as input. This data is then converted into correct text using
Optical Character Recognition (OCR) and a spelling checker after
pre-processing. Natural Language Processing (NLP) is applied for entity
extraction from text data for making Resource Description Framework (RDF)
medical data with the help of the Patient Clinical Data (PCD) ontology.
Afterwards, Basic Formal Ontology (BFO), National Vector Borne Disease Control
Program (NVBDCP) guidelines, and RDF medical data are used to develop
ontologies for VBDs, and Semantic Web Rule Language (SWRL) rules are applied
for diagnosis and treatment. The developed ontology helps in the construction
of decision support systems (DSS) for the NVBDCP to control these diseases.",Semantic rule Web-based Diagnosis and Treatment of Vector-Borne Diseases using SWRL rules,2023
"Ignacio Vellido, Juan Fdez-Olivares, Raúl Pérez",AI,2023,"World wide transport authorities are imposing complex Hours of Service
regulations to drivers, which constraint the amount of working, driving and
resting time when delivering a service. As a consequence, transport companies
are responsible not only of scheduling driving plans aligned with laws that
define the legal behaviour of a driver, but also of monitoring and identifying
as soon as possible problematic patterns that can incur in costs due to
sanctions. Transport experts are frequently in charge of many drivers and lack
time to analyse the vast amount of data recorded by the onboard sensors, and
companies have grown accustomed to pay sanctions rather than predict and
forestall wrongdoings. This paper exposes an application for summarising raw
driver activity logs according to these regulations and for explaining driver
behaviour in a human readable format. The system employs planning, constraint,
and clustering techniques to extract and describe what the driver has been
doing while identifying infractions and the activities that originate them.
Furthermore, it groups drivers based on similar driving patterns. An
experimentation in real world data indicates that recurring driving patterns
can be clustered from short basic driving sequences to whole drivers working
days.",Discovering and Explaining Driver Behaviour under HoS Regulations,2023
"Paritosh Verma, Shresth Verma, Aditya Mate, Aparna Taneja, Milind Tambe",AI,2023,"Restless multi-arm bandits (RMABs) is a popular decision-theoretic framework
that has been used to model real-world sequential decision making problems in
public health, wildlife conservation, communication systems, and beyond.
Deployed RMAB systems typically operate in two stages: the first predicts the
unknown parameters defining the RMAB instance, and the second employs an
optimization algorithm to solve the constructed RMAB instance.
  In this work we provide and analyze the results from a first-of-its-kind
deployment of an RMAB system in public health domain, aimed at improving
maternal and child health. Our analysis is focused towards understanding the
relationship between prediction accuracy and overall performance of deployed
RMAB systems. This is crucial for determining the value of investing in
improving predictive accuracy towards improving the final system performance,
and is useful for diagnosing, monitoring deployed RMAB systems.
  Using real-world data from our deployed RMAB system, we demonstrate that an
improvement in overall prediction accuracy may even be accompanied by a
degradation in the performance of RMAB system -- a broad investment of
resources to improve overall prediction accuracy may not yield expected
results. Following this, we develop decision-focused evaluation metrics to
evaluate the predictive component and show that it is better at explaining
(both empirically and theoretically) the overall performance of a deployed RMAB
system.",Decision-Focused Evaluation: Analyzing Performance of Deployed Restless Multi-Arm Bandits,2023
Hiroyuki Kido,AI,2023,"This paper gives a simple theory of inference to logically reason symbolic
knowledge fully from data over time. We take a Bayesian approach to model how
data causes symbolic knowledge. Probabilistic reasoning with symbolic knowledge
is modelled as a process of going the causality forwards and backwards. The
forward and backward processes correspond to an interpretation and inverse
interpretation of formal logic, respectively. The theory is applied to a
localisation problem to show a robot with broken or noisy sensors can
efficiently solve the problem in a fully data-driven fashion.",Generative Logic with Time: Beyond Logical Consistency and Statistical Possibility,2023
"Roberto Verdecchia, June Sallou, Luís Cruz",AI,2023,"With the ever-growing adoption of AI-based systems, the carbon footprint of
AI is no longer negligible. AI researchers and practitioners are therefore
urged to hold themselves accountable for the carbon emissions of the AI models
they design and use. This led in recent years to the appearance of researches
tackling AI environmental sustainability, a field referred to as Green AI.
Despite the rapid growth of interest in the topic, a comprehensive overview of
Green AI research is to date still missing. To address this gap, in this paper,
we present a systematic review of the Green AI literature. From the analysis of
98 primary studies, different patterns emerge. The topic experienced a
considerable growth from 2020 onward. Most studies consider monitoring AI model
footprint, tuning hyperparameters to improve model sustainability, or
benchmarking models. A mix of position papers, observational studies, and
solution papers are present. Most papers focus on the training phase, are
algorithm-agnostic or study neural networks, and use image data. Laboratory
experiments are the most common research strategy. Reported Green AI energy
savings go up to 115%, with savings over 50% being rather common. Industrial
parties are involved in Green AI studies, albeit most target academic readers.
Green AI tool provisioning is scarce. As a conclusion, the Green AI research
field results to have reached a considerable level of maturity. Therefore, from
this review emerges that the time is suitable to adopt other Green AI research
strategies, and port the numerous promising academic results to industrial
practice.",A Systematic Review of Green AI,2023
"Zhaoyang Chen, Lina Siltala-Li, Mikko Lassila, Pekka Malo, Eeva Vilkkumaa, Tarja Saaresranta, Arho Veli Virkki",AI,2023,"Background: Obstructive sleep apnea (OSA) is growing increasingly prevalent
in many countries as obesity rises. Sufficient, effective treatment of OSA
entails high social and financial costs for healthcare. Objective: For
treatment purposes, predicting OSA patients' visit expenses for the coming year
is crucial. Reliable estimates enable healthcare decision-makers to perform
careful fiscal management and budget well for effective distribution of
resources to hospitals. The challenges created by scarcity of high-quality
patient data are exacerbated by the fact that just a third of those data from
OSA patients can be used to train analytics models: only OSA patients with more
than 365 days of follow-up are relevant for predicting a year's expenditures.
Methods and procedures: The authors propose a method applying two Transformer
models, one for augmenting the input via data from shorter visit histories and
the other predicting the costs by considering both the material thus enriched
and cases with more than a year's follow-up. Results: The two-model solution
permits putting the limited body of OSA patient data to productive use.
Relative to a single-Transformer solution using only a third of the
high-quality patient data, the solution with two models improved the prediction
performance's $R^{2}$ from 88.8% to 97.5%. Even using baseline models with the
model-augmented data improved the $R^{2}$ considerably, from 61.6% to 81.9%.
Conclusion: The proposed method makes prediction with the most of the available
high-quality data by carefully exploiting details, which are not directly
relevant for answering the question of the next year's likely expenditure.",Predicting Visit Cost of Obstructive Sleep Apnea using Electronic Healthcare Records with Transformer,2023
"Seunghyun Lee, Da Young Lee, Sujeong Im, Nan Hee Kim, Sung-Min Park",AI,2023,"With recent achievements in tasks requiring context awareness, foundation
models have been adopted to treat large-scale data from electronic health
record (EHR) systems. However, previous clinical recommender systems based on
foundation models have a limited purpose of imitating clinicians' behavior and
do not directly consider a problem of missing values. In this paper, we propose
Clinical Decision Transformer (CDT), a recommender system that generates a
sequence of medications to reach a desired range of clinical states given as
goal prompts. For this, we conducted goal-conditioned sequencing, which
generated a subsequence of treatment history with prepended future goal state,
and trained the CDT to model sequential medications required to reach that goal
state. For contextual embedding over intra-admission and inter-admissions, we
adopted a GPT-based architecture with an admission-wise attention mask and
column embedding. In an experiment, we extracted a diabetes dataset from an EHR
system, which contained treatment histories of 4788 patients. We observed that
the CDT achieved the intended treatment effect according to goal prompt ranges
(e.g., NormalA1c, LowerA1c, and HigherA1c), contrary to the case with behavior
cloning. To the best of our knowledge, this is the first study to explore
clinical recommendations from the perspective of goal prompting. See
https://clinical-decision-transformer.github.io for code and additional
information.",Clinical Decision Transformer: Intended Treatment Recommendation through Goal Prompting,2023
"Ismail Nejjar, Fabian Geissmann, Mengjie Zhao, Cees Taal, Olga Fink",AI,2023,"Effective Prognostics and Health Management (PHM) relies on accurate
prediction of the Remaining Useful Life (RUL). Data-driven RUL prediction
techniques rely heavily on the representativeness of the available
time-to-failure trajectories. Therefore, these methods may not perform well
when applied to data from new units of a fleet that follow different operating
conditions than those they were trained on. This is also known as domain
shifts. Domain adaptation (DA) methods aim to address the domain shift problem
by extracting domain invariant features. However, DA methods do not distinguish
between the different phases of operation, such as steady states or transient
phases. This can result in misalignment due to under- or over-representation of
different operation phases. This paper proposes two novel DA approaches for RUL
prediction based on an adversarial domain adaptation framework that considers
the different phases of the operation profiles separately. The proposed
methodologies align the marginal distributions of each phase of the operation
profile in the source domain with its counterpart in the target domain. The
effectiveness of the proposed methods is evaluated using the New Commercial
Modular Aero-Propulsion System (N-CMAPSS) dataset, where sub-fleets of turbofan
engines operating in one of the three different flight classes (short, medium,
and long) are treated as separate domains. The experimental results show that
the proposed methods improve the accuracy of RUL predictions compared to
current state-of-the-art DA methods.",Domain Adaptation via Alignment of Operation Profile for Remaining Useful Lifetime Prediction,2023
"Seyed Mohammad Sadegh Dashti, Seyedeh Fatemeh Dashti",AI,2023,"Objective: Until now, traditional invasive approaches have been the only
means being leveraged to diagnose spinal disorders. Traditional manual
diagnostics require a high workload, and diagnostic errors are likely to occur
due to the prolonged work of physicians. In this research, we develop an expert
system based on a hybrid inference algorithm and comprehensive integrated
knowledge for assisting the experts in the fast and high-quality diagnosis of
spinal disorders.
  Methods: First, for each spinal anomaly, the accurate and integrated
knowledge was acquired from related experts and resources. Second, based on
probability distributions and dependencies between symptoms of each anomaly, a
unique numerical value known as certainty effect value was assigned to each
symptom. Third, a new hybrid inference algorithm was designed to obtain
excellent performance, which was an incorporation of the Backward Chaining
Inference and Theory of Uncertainty.
  Results: The proposed expert system was evaluated in two different phases,
real-world samples, and medical records evaluation. Evaluations show that in
terms of real-world samples analysis, the system achieved excellent accuracy.
Application of the system on the sample with anomalies revealed the degree of
severity of disorders and the risk of development of abnormalities in unhealthy
and healthy patients. In the case of medical records analysis, our expert
system proved to have promising performance, which was very close to those of
experts.
  Conclusion: Evaluations suggest that the proposed expert system provides
promising performance, helping specialists to validate the accuracy and
integrity of their diagnosis. It can also serve as an intelligent educational
software for medical students to gain familiarity with spinal disorder
diagnosis process, and related symptoms.",An Expert System to Diagnose Spinal Disorders,2023
"Chen Peng, Zhengqi Dai, Guangping Xia, Yajie Niu, Yihui Lei",AI,2023,"Machine learning systems have been extensively used as auxiliary tools in
domains that require critical decision-making, such as healthcare and criminal
justice. The explainability of decisions is crucial for users to develop trust
on these systems. In recent years, the globally-consistent rule-based
summary-explanation and its max-support (MS) problem have been proposed, which
can provide explanations for particular decisions along with useful statistics
of the dataset. However, globally-consistent summary-explanations with limited
complexity typically have small supports, if there are any. In this paper, we
propose a relaxed version of summary-explanation, i.e., the $q$-consistent
summary-explanation, which aims to achieve greater support at the cost of
slightly lower consistency. The challenge is that the max-support problem of
$q$-consistent summary-explanation (MSqC) is much more complex than the
original MS problem, resulting in over-extended solution time using standard
branch-and-bound solvers. To improve the solution time efficiency, this paper
proposes the weighted column sampling~(WCS) method based on solving smaller
problems by sampling variables according to their simplified increase support
(SIS) values. Experiments verify that solving MSqC with the proposed SIS-based
WCS method is not only more scalable in efficiency, but also yields solutions
with greater support and better global extrapolation effectiveness.",Explaining with Greater Support: Weighted Column Sampling Optimization for q-Consistent Summary-Explanations,2023
Jeffrey W. Johnston,AI,2023,"AI constructivism as inspired by Jean Piaget, described and surveyed by Frank
Guerin, and representatively implemented by Gary Drescher seeks to create
algorithms and knowledge structures that enable agents to acquire, maintain,
and apply a deep understanding of the environment through sensorimotor
interactions. This paper aims to increase awareness of constructivist AI
implementations to encourage greater progress toward enabling lifelong learning
by machines. It builds on Guerin's 2008 ""Learning Like a Baby: A Survey of AI
approaches."" After briefly recapitulating that survey, it summarizes subsequent
progress by the Guerin referents, numerous works not covered by Guerin (or
found in other surveys), and relevant efforts in related areas. The focus is on
knowledge representations and learning algorithms that have been used in
practice viewed through lenses of Piaget's schemas, adaptation processes, and
staged development. The paper concludes with a preview of a simple framework
for constructive AI being developed by the author that parses concepts from
sensory input and stores them in a semantic memory network linked to episodic
data. Extensive references are provided.",The Construction of Reality in an AI: A Review,2023
"Jiawen Deng, Hao Sun, Zhexin Zhang, Jiale Cheng, Minlie Huang",AI,2023,"With the development of artificial intelligence, dialogue systems have been
endowed with amazing chit-chat capabilities, and there is widespread interest
and discussion about whether the generated contents are socially beneficial. In
this paper, we present a new perspective of research scope towards building a
safe, responsible, and modal dialogue system, including 1) abusive and toxic
contents, 2) unfairness and discrimination, 3) ethics and morality issues, and
4) risk of misleading and privacy information. Besides, we review the
mainstream methods for evaluating the safety of large models from the
perspectives of exposure and detection of safety issues. The recent advances in
methodologies for the safety improvement of both end-to-end dialogue systems
and pipeline-based models are further introduced. Finally, we discussed six
existing challenges towards responsible AI: explainable safety monitoring,
continuous learning of safety issues, robustness against malicious attacks,
multimodal information processing, unified research framework, and
multidisciplinary theory integration. We hope this survey will inspire further
research toward safer dialogue systems.","Recent Advances towards Safe, Responsible, and Moral Dialogue Systems: A Survey",2023
"Zirong Chen, Issa Li, Haoxiang Zhang, Sarah Preum, John A. Stankovic, Meiyi Ma",AI,2023,"An increasing number of monitoring systems have been developed in smart
cities to ensure that the real-time operations of a city satisfy safety and
performance requirements. However, many existing city requirements are written
in English with missing, inaccurate, or ambiguous information. There is a high
demand for assisting city policymakers in converting human-specified
requirements to machine-understandable formal specifications for monitoring
systems. To tackle this limitation, we build CitySpec, the first intelligent
assistant system for requirement specification in smart cities. To create
CitySpec, we first collect over 1,500 real-world city requirements across
different domains (e.g., transportation and energy) from over 100 cities and
extract city-specific knowledge to generate a dataset of city vocabulary with
3,061 words. We also build a translation model and enhance it through
requirement synthesis and develop a novel online learning framework with
shielded validation. The evaluation results on real-world city requirements
show that CitySpec increases the sentence-level accuracy of requirement
specification from 59.02% to 86.64%, and has strong adaptability to a new city
and a new domain (e.g., the F1 score for requirements in Seattle increases from
77.6% to 93.75% with online learning). After the enhancement from the shield
function, CitySpec is now immune to most known textual adversarial inputs
(e.g., the attack success rate of DeepWordBug after the shield function is
reduced to 0% from 82.73%). We test the CitySpec with 18 participants from
different domains. CitySpec shows its strong usability and adaptability to
different domains, and also its robustness to malicious inputs.",CitySpec with Shield: A Secure Intelligent Assistant for Requirement Formalization,2023
"Yuya Takada, Tsuyoshi Kato",AI,2023,"Monitoring microbiological behaviors in water is crucial to manage public
health risk from waterborne pathogens, although quantifying the concentrations
of microbiological organisms in water is still challenging because
concentrations of many pathogens in water samples may often be below the
quantification limit, producing censoring data. To enable statistical analysis
based on quantitative values, the true values of non-detected measurements are
required to be estimated with high precision. Tobit model is a well-known
linear regression model for analyzing censored data. One drawback of the Tobit
model is that only the target variable is allowed to be censored. In this
study, we devised a novel extension of the classical Tobit model, called the
\emph{multi-target Tobit model}, to handle multiple censored variables
simultaneously by introducing multiple target variables. For fitting the new
model, a numerical stable optimization algorithm was developed based on
elaborate theories. Experiments conducted using several real-world water
quality datasets provided an evidence that estimating multiple columns jointly
gains a great advantage over estimating them separately.",Multi-Target Tobit Models for Completing Water Quality Data,2023
"Yiqi Zhao, Ziyan An, Xuqing Gao, Ayan Mukhopadhyay, Meiyi Ma",AI,2023,"Smart cities operate on computational predictive frameworks that collect,
aggregate, and utilize data from large-scale sensor networks. However, these
frameworks are prone to multiple sources of data and algorithmic bias, which
often lead to unfair prediction results. In this work, we first demonstrate
that bias persists at a micro-level both temporally and spatially by studying
real city data from Chattanooga, TN. To alleviate the issue of such bias, we
introduce Fairguard, a micro-level temporal logic-based approach for fair smart
city policy adjustment and generation in complex temporal-spatial domains. The
Fairguard framework consists of two phases: first, we develop a static
generator that is able to reduce data bias based on temporal logic conditions
by minimizing correlations between selected attributes. Then, to ensure
fairness in predictive algorithms, we design a dynamic component to regulate
prediction results and generate future fair predictions by harnessing logic
rules. Evaluations show that logic-enabled static Fairguard can effectively
reduce the biased correlations while dynamic Fairguard can guarantee fairness
on protected groups at run-time with minimal impact on overall performance.",Fairguard: Harness Logic-based Fairness Rules in Smart Cities,2023
"Mianxin Liu, Jingyang Zhang, Yao Wang, Yan Zhou, Fang Xie, Qihao Guo, Feng Shi, Han Zhang, Qian Wang, Dinggang Shen",AI,2023,"Brain disorders in the early and late life of humans potentially share
pathological alterations in brain functions. However, the key evidence from
neuroimaging data for pathological commonness remains unrevealed. To explore
this hypothesis, we build a deep learning model, using multi-site functional
magnetic resonance imaging data (N=4,410, 6 sites), for classifying 5 different
brain disorders from healthy controls, with a set of common features. Our model
achieves 62.6(1.9)% overall classification accuracy on data from the 6
investigated sites and detects a set of commonly affected functional
subnetworks at different spatial scales, including default mode, executive
control, visual, and limbic networks. In the deep-layer feature representation
for individual data, we observe young and aging patients with disorders are
continuously distributed, which is in line with the clinical concept of the
""spectrum of disorders"". The revealed spectrum underlying early- and late-life
brain disorders promotes the understanding of disorder comorbidities in the
lifespan.",Deep learning reveals the common spectrum underlying multiple brain disorders in youth and elders from brain functional networks,2023
"Zolzaya Dashdorj, Stanislav Grigorev, Munguntsatsral Dovdondash",AI,2023,"In the field of health-care and bio-medical research, understanding the
relationship between the symptoms of diseases is crucial for early diagnosis
and determining hidden relationships between diseases. The study aimed to
understand the extent of symptom types in disease prediction tasks. In this
research, we analyze a pre-generated symptom-based human disease dataset and
demonstrate the degree of predictability for each disease based on the
Convolutional Neural Network and the Support Vector Machine. Ambiguity of
disease is studied using the K-Means and the Principal Component Analysis. Our
results indicate that machine learning can potentially diagnose diseases with
the 98-100% accuracy in the early stage, taking the characteristics of symptoms
into account. Our result highlights that types of unusual symptoms are a good
proxy for disease early identification accurately. We also highlight that
unusual symptoms increase the accuracy of the disease prediction task.",Explorative analysis of human disease-symptoms relations using the Convolutional Neural Network,2023
"Liang Wang, Zhuangkun Wei, Weisi Guo",AI,2023,"Internet-of-Things (IoT) devices are often used to transmit physical sensor
data over digital wireless channels. Traditional Physical Layer Security
(PLS)-based cryptography approaches rely on accurate channel estimation and
information exchange for key generation, which irrevocably ties key quality
with digital channel estimation quality. Recently, we proposed a new concept
called Graph Layer Security (GLS), where digital keys are derived from physical
sensor readings. The sensor readings between legitimate users are correlated
through a common background infrastructure environment (e.g., a common water
distribution network or electric grid). The challenge for GLS has been how to
achieve distributed key generation. This paper presents a Federated multi-agent
Deep reinforcement learning-assisted Distributed Key generation scheme (FD2K),
which fully exploits the common features of physical dynamics to establish
secret key between legitimate users. We present for the first time initial
experimental results of GLS with federated learning, achieving considerable
security performance in terms of key agreement rate (KAR), and key randomness.",Securing IoT Communication using Physical Sensor Data -- Graph Layer Security with Federated Multi-Agent Deep Reinforcement Learning,2023
"Shreevignesh Suriyanarayanan, Praveen Paruchuri, Girish Varma",AI,2023,"A significant cause of air pollution in urban areas worldwide is the high
volume of road traffic. Long-term exposure to severe pollution can cause
serious health issues. One approach towards tackling this problem is to design
a pollution-aware traffic routing policy that balances multiple objectives of
i) avoiding extreme pollution in any area ii) enabling short transit times, and
iii) making effective use of the road capacities. We propose a novel
sampling-based approach for this problem. We provide the first construction of
a Markov Chain that can sample integer max flow solutions of a planar graph,
with theoretical guarantees that the probabilities depend on the aggregate
transit length. We designed a traffic policy using diverse samples and
simulated traffic on real-world road maps using the SUMO traffic simulator. We
observe a considerable decrease in areas with severe pollution when
experimented with maps of large cities across the world compared to other
approaches.",City-scale Pollution Aware Traffic Routing by Sampling Max Flows using MCMC,2023
"Chen Chen, Jie Fu, Lingjuan Lyu",AI,2023,"AI Generated Content (AIGC) has received tremendous attention within the past
few years, with content ranging from image, text, to audio, video, etc.
Meanwhile, AIGC has become a double-edged sword and recently received much
criticism regarding its responsible usage. In this vision paper, we focus on
three main concerns that may hinder the healthy development and deployment of
AIGC in practice, including risks from privacy, bias, toxicity, misinformation,
and intellectual property (IP). By documenting known and potential risks, as
well as any possible misuse scenarios of AIGC, the aim is to draw attention to
potential risks and misuse, help society to eliminate obstacles, and promote
the more ethical and secure deployment of AIGC. Additionally, we provide
insights into the promising directions for tackling these risks while
constructing generative models, enabling AIGC to be used responsibly to benefit
society.",A Pathway Towards Responsible AI Generated Content,2023
"Beate Scheibel, Stefanie Rinderle-Ma",AI,2023,"Decision mining enables the discovery of decision rules from event logs or
streams, and constitutes an important part of in-depth analysis and
optimisation of business processes. So far, decision mining has been merely
applied in an ex-post way resulting in a snapshot of decision rules for the
given chunk of log data. Online decision mining, by contrast, enables
continuous monitoring of decision rule evolution and decision drift. Hence this
paper presents an end-to-end approach for the discovery as well as monitoring
of decision points and the corresponding decision rules during runtime,
bridging the gap between online control flow discovery and decision mining. The
approach provides automatic decision support for process-aware information
systems with efficient decision drift discovery and monitoring. For monitoring,
not only the performance, in terms of accuracy, of decision rules is taken into
account, but also the occurrence of data elements and changes in branching
frequency. The paper provides two algorithms, which are evaluated on four
synthetic and one real-life data set, showing feasibility and applicability of
the approach. Overall, the approach fosters the understanding of decisions in
business processes and hence contributes to an improved human-process
interaction.",An End-to-End Approach for Online Decision Mining and Decision Drift Analysis in Process-Aware Information Systems: Extended Version,2023
"Subhash Nerella, Ziyuan Guan, Scott Siegel, Jiaqing Zhang, Kia Khezeli, Azra Bihorac, Parisa Rashidi",AI,2023,"The intensive care unit (ICU) is a specialized hospital space where
critically ill patients receive intensive care and monitoring. Comprehensive
monitoring is imperative in assessing patients conditions, in particular
acuity, and ultimately the quality of care. However, the extent of patient
monitoring in the ICU is limited due to time constraints and the workload on
healthcare providers. Currently, visual assessments for acuity, including fine
details such as facial expressions, posture, and mobility, are sporadically
captured, or not captured at all. These manual observations are subjective to
the individual, prone to documentation errors, and overburden care providers
with the additional workload. Artificial Intelligence (AI) enabled systems has
the potential to augment the patient visual monitoring and assessment due to
their exceptional learning capabilities. Such systems require robust annotated
data to train. To this end, we have developed pervasive sensing and data
processing system which collects data from multiple modalities depth images,
color RGB images, accelerometry, electromyography, sound pressure, and light
levels in ICU for developing intelligent monitoring systems for continuous and
granular acuity, delirium risk, pain, and mobility assessment. This paper
presents the Intelligent Intensive Care Unit (I2CU) system architecture we
developed for real-time patient monitoring and visual assessment.",AI-Enhanced Intensive Care Unit: Revolutionizing Patient Care with Pervasive Sensing,2023
"Tim Puphal, Malte Probst, Julian Eggert",AI,2023,"Risk assessment is a central element for the development and validation of
Autonomous Vehicles (AV). It comprises a combination of occurrence probability
and severity of future critical events. Time Headway (TH) as well as
Time-To-Contact (TTC) are commonly used risk metrics and have qualitative
relations to occurrence probability. However, they lack theoretical derivations
and additionally they are designed to only cover special types of traffic
scenarios (e.g. following between single car pairs). In this paper, we present
a probabilistic situation risk model based on survival analysis considerations
and extend it to naturally incorporate sensory, temporal and behavioral
uncertainties as they arise in real-world scenarios. The resulting Risk Spot
Detector (RSD) is applied and tested on naturalistic driving data of a
multi-lane boulevard with several intersections, enabling the visualization of
road criticality maps. Compared to TH and TTC, our approach is more selective
and specific in predicting risk. RSD concentrates on driving sections of high
vehicle density where large accelerations and decelerations or approaches with
high velocity occur.",Probabilistic Uncertainty-Aware Risk Spot Detector for Naturalistic Driving,2023
"Stefano Branchi, Andrei Buliga, Chiara Di Francescomarino, Chiara Ghidini, Francesca Meneghello, Massimiliano Ronzani",AI,2023,"Prescriptive Process Monitoring is a prominent problem in Process Mining,
which consists in identifying a set of actions to be recommended with the goal
of optimising a target measure of interest or Key Performance Indicator (KPI).
One challenge that makes this problem difficult is the need to provide
Prescriptive Process Monitoring techniques only based on temporally annotated
(process) execution data, stored in, so-called execution logs, due to the lack
of well crafted and human validated explicit models. In this paper we aim at
proposing an AI based approach that learns, by means of Reinforcement Learning
(RL), an optimal policy (almost) only from the observation of past executions
and recommends the best activities to carry on for optimizing a KPI of
interest. This is achieved first by learning a Markov Decision Process for the
specific KPIs from data, and then by using RL training to learn the optimal
policy. The approach is validated on real and synthetic datasets and compared
with off-policy Deep RL approaches. The ability of our approach to compare
with, and often overcome, Deep RL approaches provides a contribution towards
the exploitation of white box RL techniques in scenarios where only temporal
execution data are available.",Recommending the optimal policy by learning to act from temporal data,2023
"Zahra Sadeghi, Roohallah Alizadehsani, Mehmet Akif Cifci, Samina Kausar, Rizwan Rehman, Priyakshi Mahanta, Pranjal Kumar Bora, Ammar Almasri, Rami S. Alkhawaldeh, Sadiq Hussain, Bilal Alatas, Afshin Shoeibi, Hossein Moosaei, Milan Hladik, Saeid Nahavandi, Panos M. Pardalos",AI,2023,"XAI refers to the techniques and methods for building AI applications which
assist end users to interpret output and predictions of AI models. Black box AI
applications in high-stakes decision-making situations, such as medical domain
have increased the demand for transparency and explainability since wrong
predictions may have severe consequences. Model explainability and
interpretability are vital successful deployment of AI models in healthcare
practices. AI applications' underlying reasoning needs to be transparent to
clinicians in order to gain their trust. This paper presents a systematic
review of XAI aspects and challenges in the healthcare domain. The primary
goals of this study are to review various XAI methods, their challenges, and
related machine learning models in healthcare. The methods are discussed under
six categories: Features-oriented methods, global methods, concept models,
surrogate models, local pixel-based methods, and human-centric methods. Most
importantly, the paper explores XAI role in healthcare problems to clarify its
necessity in safety-critical applications. The paper intends to establish a
comprehensive understanding of XAI-related applications in the healthcare field
by reviewing the related experimental results. To facilitate future research
for filling research gaps, the importance of XAI models from different
viewpoints and their limitations are investigated.",A Brief Review of Explainable Artificial Intelligence in Healthcare,2023
"Tiago Andres Vaz, José Miguel Silva Dora, Luís da Cunha Lamb, Suzi Alves Camey",AI,2023,"This article details the creation of a novel domain ontology at the
intersection of epidemiology, medicine, statistics, and computer science. Using
the terminology defined by current legislation, the article outlines a
systematic approach to handling hospital data anonymously in preparation for
its use in Artificial Intelligence (AI) applications in healthcare. The
development process consisted of 7 pragmatic steps, including defining scope,
selecting knowledge, reviewing important terms, constructing classes that
describe designs used in epidemiological studies, machine learning paradigms,
types of data and attributes, risks that anonymized data may be exposed to,
privacy attacks, techniques to mitigate re-identification, privacy models, and
metrics for measuring the effects of anonymization. The article concludes by
demonstrating the practical implementation of this ontology in hospital
settings for the development and validation of AI.",Ontology for Healthcare Artificial Intelligence Privacy in Brazil,2023
"Yue Hu, Yuhang Zhang, Yanbing Wang, Daniel Work",AI,2023,"With the rapid development of Internet of Things technologies, the next
generation traffic monitoring infrastructures are connected via the web, to aid
traffic data collection and intelligent traffic management. One of the most
important tasks in traffic is anomaly detection, since abnormal drivers can
reduce traffic efficiency and cause safety issues. This work focuses on
detecting abnormal driving behaviors from trajectories produced by highway
video surveillance systems. Most of the current abnormal driving behavior
detection methods focus on a limited category of abnormal behaviors that deal
with a single vehicle without considering vehicular interactions. In this work,
we consider the problem of detecting a variety of socially abnormal driving
behaviors, i.e., behaviors that do not conform to the behavior of other nearby
drivers. This task is complicated by the variety of vehicular interactions and
the spatial-temporal varying nature of highway traffic. To solve this problem,
we propose an autoencoder with a Recurrent Graph Attention Network that can
capture the highway driving behaviors contextualized on the surrounding cars,
and detect anomalies that deviate from learned patterns. Our model is scalable
to large freeways with thousands of cars. Experiments on data generated from
traffic simulation software show that our model is the only one that can spot
the exact vehicle conducting socially abnormal behaviors, among the
state-of-the-art anomaly detection models. We further show the performance on
real world HighD traffic dataset, where our model detects vehicles that violate
the local driving norms.",Detecting Socially Abnormal Highway Driving Behaviors via Recurrent Graph Attention Networks,2023
"Debesh Jha, Ashish Rauniyar, Abhiskek Srivastava, Desta Haileselassie Hagos, Nikhil Kumar Tomar, Vanshali Sharma, Elif Keles, Zheyuan Zhang, Ugur Demir, Ahmet Topcu, Anis Yazidi, Jan Erik Håakegård, Ulas Bagci",AI,2023,"Artificial intelligence (AI) methods have great potential to revolutionize
numerous medical care by enhancing the experience of medical experts and
patients. AI based computer-assisted diagnosis tools can have a tremendous
benefit if they can outperform or perform similarly to the level of a clinical
expert. As a result, advanced healthcare services can be affordable in
developing nations, and the problem of a lack of expert medical practitioners
can be addressed. AI based tools can save time, resources, and overall cost for
patient treatment. Furthermore, in contrast to humans, AI can uncover complex
relations in the data from a large set of inputs and even lead to new
evidence-based knowledge in medicine. However, integrating AI in healthcare
raises several ethical and philosophical concerns, such as bias, transparency,
autonomy, responsibility and accountability, which must be addressed before
integrating such tools into clinical settings. In this article, we emphasize
recent advances in AI-assisted medical image analysis, existing standards, and
the significance of comprehending ethical issues and best practices for the
applications of AI in clinical settings. We cover the technical and ethical
challenges of AI and the implications of deploying AI in hospitals and public
organizations. We also discuss promising key measures and techniques to address
the ethical challenges, data scarcity, racial bias, lack of transparency, and
algorithmic bias. Finally, we provide our recommendation and future directions
for addressing the ethical challenges associated with AI in healthcare
applications, with the goal of deploying AI into the clinical settings to make
the workflow more efficient, accurate, accessible, transparent, and reliable
for the patient worldwide.",Ensuring Trustworthy Medical Artificial Intelligence through Ethical and Philosophical Principles,2023
"Subhrangshu Adhikary, Sonam Chaturvedi, Sudhir Kumar Chaturvedi, Saikat Banerjee",AI,2023,"The COVID-19 pandemic is considered as the most alarming global health
calamity of this century. COVID-19 has been confirmed to be mutated from
coronavirus family. As stated by the records of The World Health Organization
(WHO at April 18 2020), the present epidemic of COVID-19, has influenced more
than 2,164,111 persons and killed more than 146,198 folks in over 200 countries
across the globe and billions had confronted impacts in lifestyle because of
this virus outbreak. The ongoing overall outbreak of the COVID-19 opened up new
difficulties to the research sectors. Artificial intelligence (AI) driven
strategies can be valuable to predict the parameters, hazards, and impacts of
such an epidemic in a cost-efficient manner. The fundamental difficulties of AI
in this situation is the limited availability of information and the uncertain
nature of the disease. Here in this article, we have tried to integrate AI to
predict the infection outbreak and along with this, we have also tried to test
whether AI with help deep learning can recognize COVID-19 infected chest X-Rays
or not. The global outbreak of the virus posed enormous economic, ecological
and societal challenges into the human population and with help of this paper,
we have tried to give a message that AI can help us to identify certain
features of the disease outbreak that could prove to be essential to protect
the humanity from this deadly disease.",COVID-19 Spreading Prediction and Impact Analysis by Using Artificial Intelligence for Sustainable Global Health Assessment,2023
"Jun Wu, Xuesong Ye, Chengjie Mou, Weinan Dai",AI,2023,"Monitoring the health status of patients in the Intensive Care Unit (ICU) is
a critical aspect of providing superior care and treatment. The availability of
large-scale electronic health records (EHR) provides machine learning models
with an abundance of clinical text and vital sign data, enabling them to make
highly accurate predictions. Despite the emergence of advanced Natural Language
Processing (NLP) algorithms for clinical note analysis, the complex textual
structure and noise present in raw clinical data have posed significant
challenges. Coarse embedding approaches without domain-specific refinement have
limited the accuracy of these algorithms. To address this issue, we propose
FINEEHR, a system that utilizes two representation learning techniques, namely
metric learning and fine-tuning, to refine clinical note embeddings, while
leveraging the intrinsic correlations among different health statuses and note
categories. We evaluate the performance of FINEEHR using two metrics, namely
Area Under the Curve (AUC) and AUC-PR, on a real-world MIMIC III dataset. Our
experimental results demonstrate that both refinement approaches improve
prediction accuracy, and their combination yields the best results. Moreover,
our proposed method outperforms prior works, with an AUC improvement of over
10%, achieving an average AUC of 96.04% and an average AUC-PR of 96.48% across
various classifiers.",FineEHR: Refine Clinical Note Representations to Improve Mortality Prediction,2023
"Alexis Roger, Esma Aïmeur, Irina Rish",AI,2023,"The impact of artificial intelligence systems on our society is increasing at
an unprecedented speed. For instance, ChatGPT is being tested in mental health
treatment applications such as Koko, Stable Diffusion generates pieces of art
competitive with (or outperforming) human artists, and so on. Ethical concerns
regarding the behavior and applications of generative AI systems have been
increasing over the past years, and the field of AI alignment - steering the
behavior of AI systems towards being aligned with human values - is a rapidly
growing subfield of modern AI. In this paper, we address the challenges
involved in ethical evaluation of a multimodal artificial intelligence system.
The multimodal systems we focus on take both text and an image as input and
output text, completing the sentence or answering the question asked as input.
We perform the evaluation of these models in two steps: we first discus the
creation of a multimodal ethical database and then use this database to
construct morality-evaluating algorithms. The creation of the multimodal
ethical database is done interactively through human feedback. Users are
presented with multiple examples and votes on whether they are ethical or not.
Once these answers have been aggregated into a dataset, we built and tested
different algorithms to automatically evaluate the morality of multimodal
systems. These algorithms aim to classify the answers as ethical or not. The
models we tested are a RoBERTa-large classifier and a multilayer perceptron
classifier.",Towards ethical multimodal systems,2023
"Ioannis Papantonis, Vaishak Belle",AI,2023,"AI and ML models have already found many applications in critical domains,
such as healthcare and criminal justice. However, fully automating such
high-stakes applications can raise ethical or fairness concerns. Instead, in
such cases, humans should be assisted by automated systems so that the two
parties reach a joint decision, stemming out of their interaction. In this work
we conduct an empirical study to identify how uncertainty estimates and model
explanations affect users' reliance, understanding, and trust towards a model,
looking for potential benefits of bringing the two together. Moreover, we seek
to assess how users' behaviour is affected by their own self-confidence in
their abilities to perform a certain task, while we also discuss how the latter
may distort the outcome of an analysis based on agreement and switching
percentages.","Why not both? Complementing explanations with uncertainty, and the role of self-confidence in Human-AI collaboration",2023
"Jiaqi Wang, Enze Shi, Sigang Yu, Zihao Wu, Chong Ma, Haixing Dai, Qiushi Yang, Yanqing Kang, Jinru Wu, Huawen Hu, Chenxi Yue, Haiyang Zhang, Yiheng Liu, Xiang Li, Bao Ge, Dajiang Zhu, Yixuan Yuan, Dinggang Shen, Tianming Liu, Shu Zhang",AI,2023,"This review will introduce the latest advances in prompt engineering in the
field of natural language processing (NLP) for the medical domain. First, we
will provide a brief overview of the development of prompt engineering and
emphasize its significant contributions to healthcare NLP applications such as
question-answering systems, text summarization, and machine translation. With
the continuous improvement of general large language models, the importance of
prompt engineering in the healthcare domain is becoming increasingly prominent.
The aim of this article is to provide useful resources and bridges for
healthcare NLP researchers to better explore the application of prompt
engineering in this field. We hope that this review can provide new ideas and
inspire ample possibilities for research and application in medical NLP.",Prompt Engineering for Healthcare: Methodologies and Applications,2023
"Amit Sheth, Kaushik Roy, Manas Gaur",AI,2023,"Humans interact with the environment using a combination of perception -
transforming sensory inputs from their environment into symbols, and cognition
- mapping symbols to knowledge about the environment for supporting
abstraction, reasoning by analogy, and long-term planning. Human
perception-inspired machine perception, in the context of AI, refers to
large-scale pattern recognition from raw data using neural networks trained
using self-supervised learning objectives such as next-word prediction or
object recognition. On the other hand, machine cognition encompasses more
complex computations, such as using knowledge of the environment to guide
reasoning, analogy, and long-term planning. Humans can also control and explain
their cognitive functions. This seems to require the retention of symbolic
mappings from perception outputs to knowledge about their environment. For
example, humans can follow and explain the guidelines and safety constraints
driving their decision-making in safety-critical applications such as
healthcare, criminal justice, and autonomous driving. This article introduces
the rapidly emerging paradigm of Neurosymbolic AI combines neural networks and
knowledge-guided symbolic approaches to create more capable and flexible AI
systems. These systems have immense potential to advance both algorithm-level
(e.g., abstraction, analogy, reasoning) and application-level (e.g.,
explainable and safety-constrained decision-making) capabilities of AI systems.","Neurosymbolic AI - Why, What, and How",2023
"Ayomide Owoyemi, Emmanuel Nnaemeka, Temitope O. Benson, Ronald Ikpe, Blessing Nwachukwu, Temitope Isedowo",AI,2023,"The uptake of health insurance has been poor in Nigeria, a significant step
to improving this includes improved awareness, access to information and tools
to support decision making. Artificial intelligence (AI) based recommender
systems have gained popularity in helping individuals find movies, books,
music, and different types of products on the internet including diverse
applications in healthcare. The content-based methodology (item-based approach)
was employed in the recommender system. We applied both the K-Nearest Neighbor
(KNN) and Cosine similarity algorithm. We chose the Cosine similarity as our
chosen algorithm after several evaluations based of their outcomes in
comparison with domain knowledge. The recommender system takes into
consideration the choices entered by the user, filters the health management
organization (HMO) data by location and chosen prices. It then recommends the
top 3 HMOs with closest similarity in services offered. A recommendation tool
to help people find and select the best health insurance plan for them is
useful in reducing the barrier of accessing health insurance. Users are
empowered to easily find appropriate information on available plans, reduce
cognitive overload in dealing with over 100 options available in the market and
easily see what matches their financial capacity.",Machine Learning Recommendation System For Health Insurance Decision Making In Nigeria,2023
"Amos Okomayin, Tosin Ige",AI,2023,"Today, we have a mixture of young and older individuals, people with special
needs, and people who can care for themselves. Over 1 billion people are
estimated to be disabled; this figure corresponds to about 15% of the world's
population, with 3.8% (approximately 190 million people) accounting for people
aged 15 and up (Organization, 2011). The number of people with disabilities is
upward due to the increase in chronic health conditions and many other things.
These and other factors have made the need for proper care facilities urgent in
today's society. Several care facilities are built to help people with
disabilities live their everyday lives and not be left out of the community.",Ambient Technology & Intelligence,2023
"Joshua McGraw, Donsuk Lee, Justin Wood",AI,2023,"What are the computational foundations of social grouping? Traditional
approaches to this question have focused on verbal reasoning or simple
(low-dimensional) quantitative models. In the real world, however, social
preferences emerge when high-dimensional learning systems (brains and bodies)
interact with high-dimensional sensory inputs during an animal's embodied
interactions with the world. A deep understanding of social grouping will
therefore require embodied models that learn directly from sensory inputs using
high-dimensional learning mechanisms. To this end, we built artificial neural
networks (ANNs), embodied those ANNs in virtual fish bodies, and raised the
artificial fish in virtual fish tanks that mimicked the rearing conditions of
real fish. We then compared the social preferences that emerged in real fish
versus artificial fish. We found that when artificial fish had two core
learning mechanisms (reinforcement learning and curiosity-driven learning),
artificial fish developed fish-like social preferences. Like real fish, the
artificial fish spontaneously learned to prefer members of their own group over
members of other groups. The artificial fish also spontaneously learned to
self-segregate with their in-group, akin to self-segregation behavior seen in
nature. Our results suggest that social grouping can emerge from three
ingredients: (1) reinforcement learning, (2) intrinsic motivation, and (3)
early social experiences with in-group members. This approach lays a foundation
for reverse engineering animal-like social behavior with image-computable
models, bridging the divide between high-dimensional sensory inputs and social
preferences.",Parallel development of social preferences in fish and machines,2023
"Jun Wen, Jue Hou, Clara-Lea Bonzel, Yihan Zhao, Victor M. Castro, Vivian S. Gainer, Dana Weisenfeld, Tianrun Cai, Yuk-Lam Ho, Vidul A. Panickan, Lauren Costa, Chuan Hong, J. Michael Gaziano, Katherine P. Liao, Junwei Lu, Kelly Cho, Tianxi Cai",AI,2023,"Electronic health record (EHR) data are increasingly used to support
real-world evidence (RWE) studies. Yet its ability to generate reliable RWE is
limited by the lack of readily available precise information on the timing of
clinical events such as the onset time of heart failure. We propose a
LAbel-efficienT incidenT phEnotyping (LATTE) algorithm to accurately annotate
the timing of clinical events from longitudinal EHR data. By leveraging the
pre-trained semantic embedding vectors from large-scale EHR data as prior
knowledge, LATTE selects predictive EHR features in a concept re-weighting
module by mining their relationship to the target event and compresses their
information into longitudinal visit embeddings through a visit attention
learning network. LATTE employs a recurrent neural network to capture the
sequential dependency between the target event and visit embeddings
before/after it. To improve label efficiency, LATTE constructs highly
informative longitudinal silver-standard labels from large-scale unlabeled
patients to perform unsupervised pre-training and semi-supervised joint
training. Finally, LATTE enhances cross-site portability via contrastive
representation learning. LATTE is evaluated on three analyses: the onset of
type-2 diabetes, heart failure, and the onset and relapses of multiple
sclerosis. We use various evaluation metrics present in the literature
including the $ABC_{gain}$, the proportion of reduction in the area between the
observed event indicator and the predicted cumulative incidences in reference
to the prediction per incident prevalence. LATTE consistently achieves
substantial improvement over benchmark methods such as SAMGEP and RETAIN in all
settings.",LATTE: Label-efficient Incident Phenotyping from Longitudinal Electronic Health Records,2023
"Vedran Galetić, Alistair Nottle",AI,2023,"Trustworthiness of artificially intelligent agents is vital for the
acceptance of human-machine teaming in industrial manufacturing environments.
Predictable behaviours and explainable (and understandable) rationale allow
humans collaborating with (and building) these agents to understand their
motivations and therefore validate decisions that are made. To that aim, we
make use of G\""ardenfors's cognitively inspired Conceptual Space framework to
represent the agent's knowledge using concepts as convex regions in a space
spanned by inherently comprehensible quality dimensions. A simple typicality
quantification model is built on top of it to determine fuzzy category
membership and classify instances interpretably. We apply it on a use case from
the manufacturing domain, using objects' physical properties obtained from
cobots' onboard sensors and utilisation properties from crowdsourced
commonsense knowledge available at public knowledge bases. Such flexible
knowledge representation based on property decomposition allows for
data-efficient representation learning of typically highly specialist or
specific manufacturing artefacts. In such a setting, traditional data-driven
(e.g., computer vision-based) classification approaches would struggle due to
training data scarcity. This allows for comprehensibility of an AI agent's
acquired knowledge by the human collaborator thus contributing to
trustworthiness. We situate our approach within an existing explainability
framework specifying explanation desiderata. We provide arguments for our
system's applicability and appropriateness for different roles of human agents
collaborating with the AI system throughout its design, validation, and
operation.",Flexible and Inherently Comprehensible Knowledge Representation for Data-Efficient Learning and Trustworthy Human-Machine Teaming in Manufacturing Environments,2023
"Deshendran Moodley, Christopher Seebregts",AI,2023,"In this paper, we discuss and explore the potential and relevance of recent
developments in artificial intelligence (AI) and digital twins for health and
well-being in low-resource African countries. Using an AI systems perspective,
we review emerging trends in AI systems and digital twins and propose an
initial augmented AI system architecture to illustrate how an AI system can
work in conjunction with a 3D digital twin. We highlight scientific knowledge
discovery, continual learning, pragmatic interoperability, and interactive
explanation and decision-making as important research challenges for AI systems
and digital twins.",Re-imagining health and well-being in low resource African settings using an augmented AI system and a 3D digital twin,2023
"David Marasco, Ilya Tyagin, Justin Sybrandt, James H. Spencer, Ilya Safro",AI,2023,"This project demonstrates how medical corpus hypothesis generation, a
knowledge discovery field of AI, can be used to derive new research angles for
landscape and urban planners. The hypothesis generation approach herein
consists of a combination of deep learning with topic modeling, a probabilistic
approach to natural language analysis that scans aggregated research databases
for words that can be grouped together based on their subject matter
commonalities; the word groups accordingly form topics that can provide
implicit connections between two general research terms. The hypothesis
generation system AGATHA was used to identify likely conceptual relationships
between emerging infectious diseases (EIDs) and deforestation, with the
objective of providing landscape planners guidelines for productive research
directions to help them formulate research hypotheses centered on deforestation
and EIDs that will contribute to the broader health field that asserts causal
roles of landscape-level issues. This research also serves as a partial
proof-of-concept for the application of medical database hypothesis generation
to medicine-adjacent hypothesis discovery.",Literature-based Discovery for Landscape Planning,2023
"Juan Sebastian Canas, Francisco Gomez, Omar Costilla-Reyes",AI,2023,"Clinical practice in psychiatry is burdened with the increased demand for
healthcare services and the scarce resources available. New paradigms of health
data powered with machine learning techniques could open the possibility to
improve clinical workflow in critical stages of clinical assessment and
treatment in psychiatry. In this work, we propose a machine learning system
capable of predicting, detecting, and explaining individual changes in symptoms
of patients with Schizophrenia by using behavioral digital phenotyping data. We
forecast symptoms of patients with an error rate below 10%. The system detects
decreases in symptoms using changepoint algorithms and uses counterfactual
explanations as a recourse in a simulated continuous monitoring scenario in
healthcare. Overall, this study offers valuable insights into the performance
and potential of counterfactual explanations, predictive models, and
change-point detection within a simulated clinical workflow. These findings lay
the foundation for further research to explore additional facets of the
workflow, aiming to enhance its effectiveness and applicability in real-world
healthcare settings. By leveraging these components, the goal is to develop an
actionable, interpretable, and trustworthy integrative decision support system
that combines real-time clinical assessments with sensor-based inputs.",Counterfactual Explanations and Predictive Models to Enhance Clinical Decision-Making in Schizophrenia using Digital Phenotyping,2023
"Mbithe Nzomo, Deshendran Moodley",AI,2023,"In recent years, there has been an increased focus on early detection,
prevention, and prediction of diseases. This, together with advances in sensor
technology and the Internet of Things, has led to accelerated efforts in the
development of personal health monitoring systems. Semantic technologies have
emerged as an effective way to not only deal with the issue of interoperability
associated with heterogeneous health sensor data, but also to represent expert
health knowledge to support complex reasoning required for decision-making.
This study evaluates the state of the art in the use of semantic technologies
in sensor-based personal health monitoring systems. Using a systematic
approach, a total of 40 systems representing the state of the art in the field
are analysed. Through this analysis, six key challenges that such systems must
overcome for optimal and effective health monitoring are identified:
interoperability, context awareness, situation detection, situation prediction,
decision support, and uncertainty handling. The study critically evaluates the
extent to which these systems incorporate semantic technologies to deal with
these challenges and identifies the prominent architectures, system development
and evaluation methodologies that are used. The study provides a comprehensive
mapping of the field, identifies inadequacies in the state of the art, and
provides recommendations for future research directions.",Semantic Technologies in Sensor-Based Personal Health Monitoring Systems: A Systematic Mapping Study,2023
"Tasmia Tahmida Jidney, Angona Biswas, MD Abdullah Al Nasim, Ismail Hossain, Md Jahangir Alam, Sajedul Talukder, Mofazzal Hossain, Dr. Md Azim Ullah",AI,2023,"The integration of machine learning in medical image analysis can greatly
enhance the quality of healthcare provided by physicians. The combination of
human expertise and computerized systems can result in improved diagnostic
accuracy. An automated machine learning approach simplifies the creation of
custom image recognition models by utilizing neural architecture search and
transfer learning techniques. Medical imaging techniques are used to
non-invasively create images of internal organs and body parts for diagnostic
and procedural purposes. This article aims to highlight the potential
applications, strategies, and techniques of AutoML in medical imaging through
theoretical and empirical evidence.",AutoML Systems For Medical Imaging,2023
"Xiang Li, Lu Zhang, Zihao Wu, Zhengliang Liu, Lin Zhao, Yixuan Yuan, Jun Liu, Gang Li, Dajiang Zhu, Pingkuan Yan, Quanzheng Li, Wei Liu, Tianming Liu, Dinggang Shen",AI,2023,"In this review, we explore the potential applications of Artificial General
Intelligence (AGI) models in healthcare, focusing on foundational Large
Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We
emphasize the importance of integrating clinical expertise, domain knowledge,
and multimodal capabilities into AGI models. In addition, we lay out key
roadmaps that guide the development and deployment of healthcare AGI models.
Throughout the review, we provide critical perspectives on the potential
challenges and pitfalls associated with deploying large-scale AGI models in the
medical field. This comprehensive review aims to offer insights into the future
implications of AGI in medical imaging, healthcare and beyond.",Artificial General Intelligence for Medical Imaging,2023
"Ismail Sahbane, Francis Rhys Ward, C Henrik Åslund",AI,2023,"How to detect and mitigate deceptive AI systems is an open problem for the
field of safe and trustworthy AI. We analyse two algorithms for mitigating
deception: The first is based on the path-specific objectives framework where
paths in the game that incentivise deception are removed. The second is based
on shielding, i.e., monitoring for unsafe policies and replacing them with a
safe reference policy. We construct two simple games and evaluate our
algorithms empirically. We find that both methods ensure that our agent is not
deceptive, however, shielding tends to achieve higher reward.",Experiments with Detecting and Mitigating AI Deception,2023
"Xingyue Wang, Hanrong Zhang, Ke Ma, Shuting Tao, Peng Peng, Hongwei Wang",AI,2023,"Fault diagnosis is essential in industrial processes for monitoring the
conditions of important machines. With the ever-increasing complexity of
working conditions and demand for safety during production and operation,
different diagnosis methods are required, and more importantly, an integrated
fault diagnosis system that can cope with multiple tasks is highly desired.
However, the diagnosis subtasks are often studied separately, and the currently
available methods still need improvement for such a generalized system. To
address this issue, we propose the Generalized Out-of-distribution Fault
Diagnosis (GOOFD) framework to integrate diagnosis subtasks, such as fault
detection, fault classification, and novel fault diagnosis. Additionally, a
unified fault diagnosis method based on internal contrastive learning is put
forward to underpin the proposed generalized framework. The method extracts
features utilizing the internal contrastive learning technique and then
recognizes the outliers based on the Mahalanobis distance. Experiments are
conducted on a simulated benchmark dataset as well as two practical process
datasets to evaluate the proposed framework. As demonstrated in the
experiments, the proposed method achieves better performance compared with
several existing techniques and thus verifies the effectiveness of the proposed
framework.",Internal Contrastive Learning for Generalized Out-of-distribution Fault Diagnosis (GOOFD) Framework,2023
"Andreia Martins, Eva Maia, Isabel Praça",AI,2023,"Complementary and alternative medicine are commonly used concomitantly with
conventional medications leading to adverse drug reactions and even fatality in
some cases. Furthermore, the vast possibility of herb-drug interactions
prevents health professionals from remembering or manually searching them in a
database. Decision support systems are a powerful tool that can be used to
assist clinicians in making diagnostic and therapeutic decisions in patient
care. Therefore, an original and hybrid decision support system was designed to
identify herb-drug interactions, applying artificial intelligence techniques to
identify new possible interactions. Different machine learning models will be
used to strengthen the typical rules engine used in these cases. Thus, using
the proposed system, the pharmacy community, people's first line of contact
within the Healthcare System, will be able to make better and more accurate
therapeutic decisions and mitigate possible adverse events.",Herb-Drug Interactions: A Holistic Decision Support System in Healthcare,2023
"Fahiem Bacchus, Joseph Y. Halpern, Hector J. Levesque",AI,1998,"Agents interacting with an incompletely known world need to be able to reason
about the effects of their actions, and to gain further information about that
world they need to use sensors of some sort. Unfortunately, both the effects of
actions and the information returned from sensors are subject to error. To cope
with such uncertainties, the agent can maintain probabilistic beliefs about the
state of the world. With probabilistic beliefs the agent will be able to
quantify the likelihood of the various outcomes of its actions and is better
able to utilize the information gathered from its error-prone actions and
sensors. In this paper, we present a model in which we can reason about an
agent's probabilistic degrees of belief and the manner in which these beliefs
change as various actions are executed. We build on a general logical theory of
action developed by Reiter and others, formalized in the situation calculus. We
propose a simple axiomatization that captures an agent's state of belief and
the manner in which these beliefs change when actions are executed. Our model
displays a number of intuitively reasonable properties.",Reasoning about Noisy Sensors and Effectors in the Situation Calculus,1998
Aspassia Daskalopulu,AI,2001,"This paper concentrates on the representation of the legal relations that
obtain between parties once they have entered a contractual agreement and their
evolution as the agreement progresses through time. Contracts are regarded as
process and they are analysed in terms of the obligations that are active at
various points during their life span. An informal notation is introduced that
summarizes conveniently the states of an agreement as it evolves over time.
Such a representation enables us to determine what the status of an agreement
is, given an event or a sequence of events that concern the performance of
actions by the agents involved. This is useful both in the context of contract
drafting (where parties might wish to preview how their agreement might evolve)
and in the context of contract performance monitoring (where parties might with
to establish what their legal positions are once their agreement is in force).
The discussion is based on an example that illustrates some typical patterns of
contractual obligations.",Modelling Legal Contracts as Processes,2001
"Javier Nicolas Sanchez, Adam Milstein, Evan Williamson",RO,2002,"Global mobile robot localization is the problem of determining a robot's pose
in an environment, using sensor data, when the starting position is unknown. A
family of probabilistic algorithms known as Monte Carlo Localization (MCL) is
currently among the most popular methods for solving this problem. MCL
algorithms represent a robot's belief by a set of weighted samples, which
approximate the posterior probability of where the robot is located by using a
Bayesian formulation of the localization problem. This article presents an
extension to the MCL algorithm, which addresses its problems when localizing in
highly symmetrical environments; a situation where MCL is often unable to
correctly track equally probable poses for the robot. The problem arises from
the fact that sample sets in MCL often become impoverished, when samples are
generated according to their posterior likelihood. Our approach incorporates
the idea of clusters of samples and modifies the proposal distribution
considering the probability mass of those clusters. Experimental results are
presented that show that this new extension to the MCL algorithm successfully
localizes in symmetric environments where ordinary MCL often fails.",Robust Global Localization Using Clustered Particle Filtering,2002
Victor Eliashberg,AI,2003,"This paper goes back to Turing (1936) and treats his machine as a cognitive
model (W,D,B), where W is an ""external world"" represented by memory device (the
tape divided into squares), and (D,B) is a simple robot that consists of the
sensory-motor devices, D, and the brain, B. The robot's sensory-motor devices
(the ""eye"", the ""hand"", and the ""organ of speech"") allow the robot to simulate
the work of any Turing machine. The robot simulates the internal states of a
Turing machine by ""talking to itself."" At the stage of training, the teacher
forces the robot (by acting directly on its motor centers) to perform several
examples of an algorithm with different input data presented on tape. Two
effects are achieved: 1) The robot learns to perform the shown algorithm with
any input data using the tape. 2) The robot learns to perform the algorithm
""mentally"" using an ""imaginary tape."" The model illustrates the simplest
concept of a universal learning neurocomputer, demonstrates universality of
associative learning as the mechanism of programming, and provides a
simplified, but nontrivial neurobiologically plausible explanation of the
phenomena of working memory and mental imagery. The model is implemented as a
user-friendly program for Windows called EROBOT. The program is available at
www.brain0.com/software.html.",What Is Working Memory and Mental Imagery? A Robot that Learns to Perform Mental Computations,2003
David N. Levin,CV,2004,"This paper shows how a machine, which observes stimuli through an
uncharacterized, uncalibrated channel and sensor, can glean machine-independent
information (i.e., channel- and sensor-independent information) about the
stimuli. First, we demonstrate that a machine defines a specific coordinate
system on the stimulus state space, with the nature of that coordinate system
depending on the device's channel and sensor. Thus, machines with different
channels and sensors ""see"" the same stimulus trajectory through state space,
but in different machine-specific coordinate systems. For a large variety of
physical stimuli, statistical properties of that trajectory endow the stimulus
configuration space with differential geometric structure (a metric and
parallel transfer procedure), which can then be used to represent relative
stimulus configurations in a coordinate-system-independent manner (and,
therefore, in a channel- and sensor-independent manner). The resulting
description is an ""inner"" property of the stimulus time series in the sense
that it does not depend on extrinsic factors like the observer's choice of a
coordinate system in which the stimulus is viewed (i.e., the observer's choice
of channel and sensor). This methodology is illustrated with analytic examples
and with a numerically simulated experiment. In an intelligent sensory device,
this kind of representation ""engine"" could function as a ""front-end"" that
passes channel/sensor-independent stimulus representations to a pattern
recognition module. After a pattern recognizer has been trained in one of these
devices, it could be used without change in other devices having different
channels and sensors.",Channel-Independent and Sensor-Independent Stimulus Representations,2005
"Pedro U. Lima, Luis M. M. Custodio",RO,2004,"This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams.",Artificial Intelligence and Systems Theory: Applied to Cooperative Robots,2004
"Vitaly Schetinin, Joachim Schult",AI,2005,"We describe a polynomial network technique developed for learning to classify
clinical electroencephalograms (EEGs) presented by noisy features. Using an
evolutionary strategy implemented within Group Method of Data Handling, we
learn classification models which are comprehensively described by sets of
short-term polynomials. The polynomial models were learnt to classify the EEGs
recorded from Alzheimer and healthy patients and recognize the EEG artifacts.
Comparing the performances of our technique and some machine learning methods
we conclude that our technique can learn well-suited polynomial models which
experts can find easy-to-understand.",Learning Polynomial Networks for Classification of Clinical Electroencephalograms,2005
"T. Kosel, I. Grabec",NE,2007,"The intelligent acoustic emission locator is described in Part I, while Part
II discusses blind source separation, time delay estimation and location of two
simultaneously active continuous acoustic emission sources.
  The location of acoustic emission on complicated aircraft frame structures is
a difficult problem of non-destructive testing. This article describes an
intelligent acoustic emission source locator. The intelligent locator comprises
a sensor antenna and a general regression neural network, which solves the
location problem based on learning from examples. Locator performance was
tested on different test specimens. Tests have shown that the accuracy of
location depends on sound velocity and attenuation in the specimen, the
dimensions of the tested area, and the properties of stored data. The location
accuracy achieved by the intelligent locator is comparable to that obtained by
the conventional triangulation method, while the applicability of the
intelligent locator is more general since analysis of sonic ray paths is
avoided. This is a promising method for non-destructive testing of aircraft
frame structures by the acoustic emission method.",Intelligent location of simultaneously active acoustic emission sources: Part I,2007
"Sizwe M. Dhlamini*, Fulufhelo V. Nelwamondo**, Tshilidzi Marwala**",NE,2007,"The work proposes the application of neural networks with particle swarm
optimisation (PSO) and genetic algorithms (GA) to compensate for missing data
in classifying high voltage bushings. The classification is done using DGA data
from 60966 bushings based on IEEEc57.104, IEC599 and IEEE production rates
methods for oil impregnated paper (OIP) bushings. PSO and GA were compared in
terms of accuracy and computational efficiency. Both GA and PSO simulations
were able to estimate missing data values to an average 95% accuracy when only
one variable was missing. However PSO rapidly deteriorated to 66% accuracy with
two variables missing simultaneously, compared to 84% for GA. The data
estimated using GA was found to classify the conditions of bushings than the
PSO.",Condition Monitoring of HV Bushings in the Presence of Missing Data Using Evolutionary Computing,2007
D. A. Sofge,AI,2007,"The SEMATECH sponsored J-88-E project teaming Texas Instruments with
NeuroDyne (et al.) focused on Fault Detection and Classification (FDC) on a Lam
9600 aluminum plasma etch reactor, used in the process of semiconductor
fabrication. Fault classification was accomplished by implementing a series of
virtual sensor models which used data from real sensors (Lam Station sensors,
Optical Emission Spectroscopy, and RF Monitoring) to predict recipe setpoints
and wafer state characteristics. Fault detection and classification were
performed by comparing predicted recipe and wafer state values with expected
values. Models utilized include linear PLS, Polynomial PLS, and Neural Network
PLS. Prediction of recipe setpoints based upon sensor data provides a
capability for cross-checking that the machine is maintaining the desired
setpoints. Wafer state characteristics such as Line Width Reduction and
Remaining Oxide were estimated on-line using these same process sensors (Lam,
OES, RFM). Wafer-to-wafer measurement of these characteristics in a production
setting (where typically this information may be only sparsely available, if at
all, after batch processing runs with numerous wafers have been completed)
would provide important information to the operator that the process is or is
not producing wafers within acceptable bounds of product quality. Production
yield is increased, and correspondingly per unit cost is reduced, by providing
the operator with the opportunity to adjust the process or machine before
etching more wafers.",Virtual Sensor Based Fault Detection and Classification on a Plasma Etch Reactor,2007
Juergen Schmidhuber,AI,2007,"I postulate that human or other intelligent agents function or should
function as follows. They store all sensory observations as they come - the
data is holy. At any time, given some agent's current coding capabilities, part
of the data is compressible by a short and hopefully fast program / description
/ explanation / world model. In the agent's subjective eyes, such data is more
regular and more ""beautiful"" than other data. It is well-known that knowledge
of regularity and repeatability may improve the agent's ability to plan actions
leading to external rewards. In absence of such rewards, however, known beauty
is boring. Then ""interestingness"" becomes the first derivative of subjective
beauty: as the learning agent improves its compression algorithm, formerly
apparently random data parts become subjectively more regular and beautiful.
Such progress in compressibility is measured and maximized by the curiosity
drive: create action sequences that extend the observation history and yield
previously unknown / unpredictable but quickly learnable algorithmic
regularity. We discuss how all of the above can be naturally implemented on
computers, through an extension of passive unsupervised learning to the case of
active data selection: we reward a general reinforcement learner (with access
to the adaptive compressor) for actions that improve the subjective
compressibility of the growing data. An unusually large breakthrough in
compressibility deserves the name ""discovery"". The ""creativity"" of artists,
dancers, musicians, pure mathematicians can be viewed as a by-product of this
principle. Several qualitative examples support this hypothesis.","Simple Algorithmic Principles of Discovery, Subjective Beauty, Selective Attention, Curiosity & Creativity",2007
"Cherif Smaili, Maan El Badaoui El Najjar, François Charpillet",AI,2007,"This paper presents a multi-sensor fusion strategy for a novel road-matching
method designed to support real-time navigational features within advanced
driving-assistance systems. Managing multihypotheses is a useful strategy for
the road-matching problem. The multi-sensor fusion and multi-modal estimation
are realized using Dynamical Bayesian Network. Experimental results, using data
from Antilock Braking System (ABS) sensors, a differential Global Positioning
System (GPS) receiver and an accurate digital roadmap, illustrate the
performances of this approach, especially in ambiguous situations.",Multi-Sensor Fusion Method using Dynamic Bayesian Network for Precise Vehicle Localization and Road Matching,2007
"Forrest Sheng Bao, Donald Yu-Chun Lie, Yuanlin Zhang",AI,2008,"Epilepsy is one of the most common neurological disorders that greatly impair
patient' daily lives. Traditional epileptic diagnosis relies on tedious visual
screening by neurologists from lengthy EEG recording that requires the presence
of seizure (ictal) activities. Nowadays, there are many systems helping the
neurologists to quickly find interesting segments of the lengthy signal by
automatic seizure detection. However, we notice that it is very difficult, if
not impossible, to obtain long-term EEG data with seizure activities for
epilepsy patients in areas lack of medical resources and trained neurologists.
Therefore, we propose to study automated epileptic diagnosis using interictal
EEG data that is much easier to collect than ictal data. The authors are not
aware of any report on automated EEG diagnostic system that can accurately
distinguish patients' interictal EEG from the EEG of normal people. The
research presented in this paper, therefore, aims to develop an automated
diagnostic system that can use interictal EEG data to diagnose whether the
person is epileptic. Such a system should also detect seizure activities for
further investigation by doctors and potential patient monitoring. To develop
such a system, we extract four classes of features from the EEG data and build
a Probabilistic Neural Network (PNN) fed with these features. Leave-one-out
cross-validation (LOO-CV) on a widely used epileptic-normal data set reflects
an impressive 99.5% accuracy of our system on distinguishing normal people's
EEG from patient's interictal EEG. We also find our system can be used in
patient monitoring (seizure detection) and seizure focus localization, with
96.7% and 77.5% accuracy respectively on the data set.",A New Approach to Automated Epileptic Diagnosis Using EEG and Probabilistic Neural Network,2008
J. R. Burger,AI,2008,"By way of explaining how a brain works logically, human associative memory is
modeled with logical and memory neurons, corresponding to standard digital
circuits. The resulting cognitive architecture incorporates basic psychological
elements such as short term and long term memory. Novel to the architecture are
memory searches using cues chosen pseudorandomly from short term memory.
Recalls alternated with sensory images, many tens per second, are analyzed
subliminally as an ongoing process, to determine a direction of attention in
short term memory.","Cognitive Architecture for Direction of Attention Founded on Subliminal Memory Searches, Pseudorandom and Nonstop",2008
"Carlos Miravet, Luis Pascual, Eloise Krouch, Juan Manuel del Cura",CV,2008,"In this paper are described the image processing algorithms developed by
SENER, Ingenieria y Sistemas to cope with the problem of image-based,
autonomous rendez-vous (RV) with an orbiting satellite. The methods developed
have a direct application in the OLEV (Orbital Life Extension Extension
Vehicle) mission. OLEV is a commercial mission under development by a
consortium formed by Swedish Space Corporation, Kayser-Threde and SENER, aimed
to extend the operational life of geostationary telecommunication satellites by
supplying them control, navigation and guidance services. OLEV is planned to
use a set of cameras to determine the angular position and distance to the
client satellite during the complete phases of rendez-vous and docking, thus
enabling the operation with satellites not equipped with any specific
navigational aid to provide support during the approach. The ability to operate
with un-equipped client satellites significantly expands the range of
applicability of the system under development, compared to other competing
video technologies already tested in previous spatial missions, such as the
ones described here below.",An Image-Based Sensor System for Autonomous Rendez-Vous with Uncooperative Satellites,2008
"Dasika Ratna Deepthi, K. Eswaran",AI,2008,"In this paper, we present a new kind of learning implementation to recognize
the patterns using the concept of Mirroring Neural Network (MNN) which can
extract information from distinct sensory input patterns and perform pattern
recognition tasks. It is also capable of being used as an advanced associative
memory wherein image data is associated with voice inputs in an unsupervised
manner. Since the architecture is hierarchical and modular it has the potential
of being used to devise learning engines of ever increasing complexity.",Pattern Recognition and Memory Mapping using Mirroring Neural Networks,2008
Victor Eliashberg,AI,2009,"The paper tackles four basic questions associated with human brain as a
learning system. How can the brain learn to (1) mentally simulate different
external memory aids, (2) perform, in principle, any mental computations using
imaginary memory aids, (3) recall the real sensory and motor events and
synthesize a combinatorial number of imaginary events, (4) dynamically change
its mental set to match a combinatorial number of contexts? We propose a
uniform answer to (1)-(4) based on the general postulate that the human
neocortex processes symbolic information in a ""nonclassical"" way. Instead of
manipulating symbols in a read/write memory, as the classical symbolic systems
do, it manipulates the states of dynamical memory representing different
temporary attributes of immovable symbolic structures stored in a long-term
memory. The approach is formalized as the concept of E-machine. Intuitively, an
E-machine is a system that deals mainly with characteristic functions
representing subsets of memory pointers rather than the pointers themselves.
This nonclassical symbolic paradigm is Turing universal, and, unlike the
classical one, is efficiently implementable in homogeneous neural networks with
temporal modulation topologically resembling that of the neocortex.","A nonclassical symbolic theory of working memory, mental computations, and mental set",2009
"Fahem Kebair, Frederic Serin",AI,2009,"Making a decision in a changeable and dynamic environment is an arduous task
owing to the lack of information, their uncertainties and the unawareness of
planners about the future evolution of incidents. The use of a decision support
system is an efficient solution of this issue. Such a system can help emergency
planners and responders to detect possible emergencies, as well as to suggest
and evaluate possible courses of action to deal with the emergency. We are
interested in our work to the modeling of a monitoring preventive and emergency
management system, wherein we stress the generic aspect. In this paper we
propose an agent-based architecture of this system and we describe a first step
of our approach which is the modeling of information and their representation
using a multiagent system.",Towards an Intelligent System for Risk Prevention and Management,2009
"Arash Shaban-Nejad, Olga Ormandjieva, Mohamad Kassab, Volker Haarslev",AI,2009,"Requirement volatility is an issue in software engineering in general, and in
Web-based clinical applications in particular, which often originates from an
incomplete knowledge of the domain of interest. With advances in the health
science, many features and functionalities need to be added to, or removed
from, existing software applications in the biomedical domain. At the same
time, the increasing complexity of biomedical systems makes them more difficult
to understand, and consequently it is more difficult to define their
requirements, which contributes considerably to their volatility. In this
paper, we present a novel agent-based approach for analyzing and managing
volatile and dynamic requirements in an ontology-driven laboratory information
management system (LIMS) designed for Web-based case reporting in medical
mycology. The proposed framework is empowered with ontologies and formalized
using category theory to provide a deep and common understanding of the
functional and nonfunctional requirement hierarchies and their interrelations,
and to trace the effects of a change on the conceptual framework.",Managing Requirement Volatility in an Ontology-Driven Clinical LIMS Using Category Theory. International Journal of Telemedicine and Applications,2009
"Naveen Ashish, Dmitri Kalashnikov, Sharad Mehrotra, Nalini Venkatasubramanian",DB,2009,"Many application domains require representing interrelated real-world
activities and/or evolving physical phenomena. In the crisis response domain,
for instance, one may be interested in representing the state of the unfolding
crisis (e.g., forest fire), the progress of the response activities such as
evacuation and traffic control, and the state of the crisis site(s). Such a
situation representation can then be used to support a multitude of
applications including situation monitoring, analysis, and planning. In this
paper, we make a case for an event based representation of situations where
events are defined to be domain-specific significant occurrences in space and
time. We argue that events offer a unifying and powerful abstraction to
building situational awareness applications. We identify challenges in building
an Event Management System (EMS) for which traditional data and knowledge
management systems prove to be limited and suggest possible directions and
technologies to address the challenges.",An Event Based Approach To Situational Representation,2009
Subhash Kak,NE,2009,"The term quantum neural computing indicates a unity in the functioning of the
brain. It assumes that the neural structures perform classical processing and
that the virtual particles associated with the dynamical states of the
structures define the underlying quantum state. We revisit the concept and also
summarize new arguments related to the learning modes of the brain in response
to sensory input that may be aggregated in three types: associative,
reorganizational, and quantum. The associative and reorganizational types are
quite apparent based on experimental findings; it is much harder to establish
that the brain as an entity exhibits quantum properties. We argue that the
reorganizational behavior of the brain may be viewed as inner adjustment
corresponding to its quantum behavior at the system level. Not only neural
structures but their higher abstractions also may be seen as whole entities. We
consider the dualities associated with the behavior of the brain and how these
dualities are bridged.",Another Look at Quantum Neural Computing,2013
Joshua Shinavier,AI,2010,"The Resource Description Framework (RDF) provides a common data model for the
integration of ""real-time"" social and sensor data streams with the Web and with
each other. While there exist numerous protocols and data formats for
exchanging dynamic RDF data, or RDF updates, these options should be examined
carefully in order to enable a Semantic Web equivalent of the high-throughput,
low-latency streams of typical Web 2.0, multimedia, and gaming applications.
This paper contains a brief survey of RDF update formats and a high-level
discussion of both TCP and UDP-based transport protocols for updates. Its main
contribution is the experimental evaluation of a UDP-based architecture which
serves as a real-world example of a high-performance RDF streaming application
in an Internet-scale distributed environment.",Optimizing real-time RDF data streams,2010
Kush Agrawal,AI,2010,"""Encoded in the large, highly evolved sensory and motor portions of the human
brain is a billion years of experience about the nature of the world and how to
survive in it. The deliberate process we call reasoning is, I believe, the
thinnest veneer of human thought, effective only because it is supported by
this much older and much powerful, though usually unconscious, sensor motor
knowledge. We are all prodigious Olympians in perceptual and motor areas, so
good that we make the difficult look easy. Abstract thought, though, is a new
trick, perhaps less than 100 thousand years old. We have not yet mastered it.
It is not all that intrinsically difficult; it just seems so when we do it.""-
Hans Moravec Moravec's paradox is involved with the fact that it is the
seemingly easier day to day problems that are harder to implement in a machine,
than the seemingly complicated logic based problems of today. The results prove
that most artificially intelligent machines are as adept if not more than us at
under-taking long calculations or even play chess, but their logic brings them
nowhere when it comes to carrying out everyday tasks like walking, facial
gesture recognition or speech recognition.",To study the phenomenon of the Moravec's Paradox,2010
"W. Burgard, D. Fox, S. Thrun",AI,2011,"Localization, that is the estimation of a robot's location from sensor data,
is a fundamental problem in mobile robotics. This papers presents a version of
Markov localization which provides accurate position estimates and which is
tailored towards dynamic environments. The key idea of Markov localization is
to maintain a probability density over the space of all locations of a robot in
its environment. Our approach represents this space metrically, using a
fine-grained grid to approximate densities. It is able to globally localize the
robot from scratch and to recover from localization failures. It is robust to
approximate models of the environment (such as occupancy grid maps) and noisy
sensors (such as ultrasound sensors). Our approach also includes a filtering
technique which allows a mobile robot to reliably estimate its position even in
densely populated environments in which crowds of people block the robot's
sensors for extended periods of time. The method described here has been
implemented and tested in several real-world applications of mobile robots,
including the deployments of two mobile robots as interactive museum
tour-guides.",Markov Localization for Mobile Robots in Dynamic Environments,2011
"G. A. Kaminka, M. Tambe",MA,2011,"Agents in dynamic multi-agent environments must monitor their peers to
execute individual and group plans. A key open question is how much monitoring
of other agents' states is required to be effective: The Monitoring Selectivity
Problem. We investigate this question in the context of detecting failures in
teams of cooperating agents, via Socially-Attentive Monitoring, which focuses
on monitoring for failures in the social relationships between the agents. We
empirically and analytically explore a family of socially-attentive teamwork
monitoring algorithms in two dynamic, complex, multi-agent domains, under
varying conditions of task distribution and uncertainty. We show that a
centralized scheme using a complex algorithm trades correctness for
completeness and requires monitoring all teammates. In contrast, a simple
distributed teamwork monitoring algorithm results in correct and complete
detection of teamwork failures, despite relying on limited, uncertain
knowledge, and monitoring only key agents in a team. In addition, we report on
the design of a socially-attentive monitoring system and demonstrate its
generality in monitoring several coordination relationships, diagnosing
detected failures, and both on-line and off-line applications.",Robust Agent Teams via Socially-Attentive Monitoring,2011
"P. Berry, T. J. Lee, D. E. Wilkins",MA,2011,"There is an increasing need for automated support for humans monitoring the
activity of distributed teams of cooperating agents, both human and machine. We
characterize the domain-independent challenges posed by this problem, and
describe how properties of domains influence the challenges and their
solutions. We will concentrate on dynamic, data-rich domains where humans are
ultimately responsible for team behavior. Thus, the automated aid should
interactively support effective and timely decision making by the human. We
present a domain-independent categorization of the types of alerts a plan-based
monitoring system might issue to a user, where each type generally requires
different monitoring techniques. We describe a monitoring framework for
integrating many domain-specific and task-specific monitoring techniques and
then using the concept of value of an alert to avoid operator overload. We use
this framework to describe an execution monitoring approach we have used to
implement Execution Assistants (EAs) in two different dynamic, data-rich,
real-world domains to assist a human in monitoring team behavior. One domain
(Army small unit operations) has hundreds of mobile, geographically distributed
agents, a combination of humans, robots, and vehicles. The other domain (teams
of unmanned ground and air vehicles) has a handful of cooperating robots. Both
domains involve unpredictable adversaries in the vicinity. Our approach
customizes monitoring behavior for each specific task, plan, and situation, as
well as for user preferences. Our EAs alert the human controller when reported
events threaten plan execution or physically threaten team members. Alerts were
generated in a timely manner without inundating the user with too many alerts
(less than 10 percent of alerts are unwanted, as judged by domain experts).",Interactive Execution Monitoring of Agent Teams,2011
"Nittaya Kerdprasop, Kittisak Kerdprasop",LO,2011,"Knowledge mining is the process of deriving new and useful knowledge from
vast volumes of data and background knowledge. Modern healthcare organizations
regularly generate huge amount of electronic data stored in the databases.
These data are a valuable resource for mining useful knowledge to help medical
practitioners making appropriate and accurate decision on the diagnosis and
treatment of diseases. In this paper, we propose the design of a novel medical
expert system based on a logic-programming framework. The proposed system
includes a knowledge-mining component as a repertoire of tools for discovering
useful knowledge. The implementation of classification and association mining
tools based on the higher order and meta-level programming schemes using Prolog
has been presented to express the power of logic-based language. Such language
also provides a pattern matching facility, which is an essential function for
the development of knowledge-intensive tasks. Besides the major goal of medical
decision support, the knowledge discovered by our logic-based knowledge-mining
component can also be deployed as background knowledge to pre-treatment data
from other sources as well as to guard the data repositories against constraint
violation. A framework for knowledge deployment is also presented.",Higher Order Programming to Mine Knowledge for a Modern Medical Expert System,2011
Michał B. Paradowski,AI,2011,"A few decades of work in the AI field have focused efforts on developing a
new generation of systems which can acquire knowledge via interaction with the
world. Yet, until very recently, most such attempts were underpinned by
research which predominantly regarded linguistic phenomena as separated from
the brain and body. This could lead one into believing that to emulate
linguistic behaviour, it suffices to develop 'software' operating on abstract
representations that will work on any computational machine. This picture is
inaccurate for several reasons, which are elucidated in this paper and extend
beyond sensorimotor and semantic resonance. Beginning with a review of
research, I list several heterogeneous arguments against disembodied language,
in an attempt to draw conclusions for developing embodied multisensory agents
which communicate verbally and non-verbally with their environment. Without
taking into account both the architecture of the human brain, and embodiment,
it is unrealistic to replicate accurately the processes which take place during
language acquisition, comprehension, production, or during non-linguistic
actions. While robots are far from isomorphic with humans, they could benefit
from strengthened associative connections in the optimization of their
processes and their reactivity and sensitivity to environmental stimuli, and in
situated human-machine interaction. The concept of multisensory integration
should be extended to cover linguistic input and the complementary information
combined from temporally coincident sensory impressions.",Developing Embodied Multisensory Dialogue Agents,2012
"Casey Bennett, Thomas Doub, April Bragg, Jason Luellen, Christina Van Regenmorter, Jennifer Lockman, Randall Reiserer",AI,2011,"The CDOI outcome measure - a patient-reported outcome (PRO) instrument
utilizing direct client feedback - was implemented in a large, real-world
behavioral healthcare setting in order to evaluate previous findings from
smaller controlled studies. PROs provide an alternative window into treatment
effectiveness based on client perception and facilitate detection of
problems/symptoms for which there is no discernible measure (e.g. pain). The
principal focus of the study was to evaluate the utility of the CDOI for
predictive modeling of outcomes in a live clinical setting. Implementation
factors were also addressed within the framework of the Theory of Planned
Behavior by linking adoption rates to implementation practices and clinician
perceptions. The results showed that the CDOI does contain significant capacity
to predict outcome delta over time based on baseline and early change scores in
a large, real-world clinical setting, as suggested in previous research. The
implementation analysis revealed a number of critical factors affecting
successful implementation and adoption of the CDOI outcome measure, though
there was a notable disconnect between clinician intentions and actual
behavior. Most importantly, the predictive capacity of the CDOI underscores the
utility of direct client feedback measures such as PROs and their potential use
as the basis for next generation clinical decision support tools and
personalized treatment approaches.",Data Mining Session-Based Patient Reported Outcomes (PROs) in a Mental Health Setting: Toward Data-Driven Clinical Decision Support and Personalized Treatment,2011
"Tobias Jung, Daniel Polani, Peter Stone",AI,2012,"This paper develops generalizations of empowerment to continuous states.
Empowerment is a recently introduced information-theoretic quantity motivated
by hypotheses about the efficiency of the sensorimotor loop in biological
organisms, but also from considerations stemming from curiosity-driven
learning. Empowemerment measures, for agent-environment systems with stochastic
transitions, how much influence an agent has on its environment, but only that
influence that can be sensed by the agent sensors. It is an
information-theoretic generalization of joint controllability (influence on
environment) and observability (measurement by sensors) of the environment by
the agent, both controllability and observability being usually defined in
control theory as the dimensionality of the control/observation spaces. Earlier
work has shown that empowerment has various interesting and relevant
properties, e.g., it allows us to identify salient states using only the
dynamics, and it can act as intrinsic reward without requiring an external
reward. However, in this previous work empowerment was limited to the case of
small-scale and discrete domains and furthermore state transition probabilities
were assumed to be known. The goal of this paper is to extend empowerment to
the significantly more important and relevant case of continuous vector-valued
state spaces and initially unknown state transition probabilities. The
continuous state space is addressed by Monte-Carlo approximation; the unknown
transitions are addressed by model learning and prediction for which we apply
Gaussian processes regression with iterated forecasting. In a number of
well-known continuous control tasks we examine the dynamics induced by
empowerment and include an application to exploration and online model
learning.",Empowerment for Continuous Agent-Environment Systems,2012
"Jason Yosinski, Cooper Bills",RO,2012,"In many situations, Miniature Aerial Vehicles (MAVs) are limited to using
only on-board sensors for navigation. This limits the data available to
algorithms used for stabilization and localization, and current control methods
are often insufficient to allow reliable hovering in place or trajectory
following. In this research, we explore using machine learning to predict the
drift (flight path errors) of an MAV while executing a desired flight path.
This predicted drift will allow the MAV to adjust it's flightpath to maintain a
desired course.",MAV Stabilization using Machine Learning and Onboard Sensors,2012
"Rina Panigrahy, Li Zhang",AI,2012,"There is a vast supply of prior art that study models for mental processes.
Some studies in psychology and philosophy approach it from an inner perspective
in terms of experiences and percepts. Others such as neurobiology or
connectionist-machines approach it externally by viewing the mind as complex
circuit of neurons where each neuron is a primitive binary circuit. In this
paper, we also model the mind as a place where a circuit grows, starting as a
collection of primitive components at birth and then builds up incrementally in
a bottom up fashion. A new node is formed by a simple composition of prior
nodes when we undergo a repeated experience that can be described by that
composition. Unlike neural networks, however, these circuits take ""concepts"" or
""percepts"" as inputs and outputs. Thus the growing circuits can be likened to a
growing collection of lambda expressions that are built on top of one another
in an attempt to compress the sensory input as a heuristic to bound its
Kolmogorov Complexity.",The Mind Grows Circuits,2012
Yoshua Bengio,LG,2012,"We propose a theory that relates difficulty of learning in deep architectures
to culture and language. It is articulated around the following hypotheses: (1)
learning in an individual human brain is hampered by the presence of effective
local minima; (2) this optimization difficulty is particularly important when
it comes to learning higher-level abstractions, i.e., concepts that cover a
vast and highly-nonlinear span of sensory configurations; (3) such high-level
abstractions are best represented in brains by the composition of many levels
of representation, i.e., by deep architectures; (4) a human brain can learn
such high-level abstractions if guided by the signals produced by other humans,
which act as hints or indirect supervision for these high-level abstractions;
and (5), language and the recombination and optimization of mental concepts
provide an efficient evolutionary recombination operator, and this gives rise
to rapid search in the space of communicable ideas that help humans build up
better high-level internal representations of their world. These hypotheses put
together imply that human culture and the evolution of ideas have been crucial
to counter an optimization difficulty: this optimization difficulty would
otherwise make it very difficult for human brains to capture high-level
knowledge of the world. The theory is grounded in experimental observations of
the difficulties of training deep artificial neural networks. Plausible
consequences of this theory for the efficiency of cultural evolutions are
sketched.",Evolving Culture vs Local Minima,2012
"Naveen Ashish, Antarip Biswas, Sumit Das, Saurav Nag, Rajiv Pratap",IR,2012,"This paper describes a technology to connect patients to information in the
experiences of other patients by using the power of structured big data. The
approach, implemented in the Abzooba Smart Health Informatics Platform
(SHIP),is to distill concepts of facts and expressions from conversations and
discussions in health social media forums, and use those distilled concepts in
connecting patients to experiences and insights that are highly relevant to
them in particular. We envision our work, in progress, to provide new and
effective tools to exploit the richness of content in social media in health
for outcomes research.",The Abzooba Smart Health Informatics Platform (SHIP) TM - From Patient Experiences to Big Data to Insights,2012
"Jun-Ming Xu, Aniruddha Bhargava, Robert Nowak, Xiaojin Zhu",AI,2012,"Many real-world phenomena can be represented by a spatio-temporal signal:
where, when, and how much. Social media is a tantalizing data source for those
who wish to monitor such signals. Unlike most prior work, we assume that the
target phenomenon is known and we are given a method to count its occurrences
in social media. However, counting is plagued by sample bias, incomplete data,
and, paradoxically, data scarcity -- issues inadequately addressed by prior
work. We formulate signal recovery as a Poisson point process estimation
problem. We explicitly incorporate human population bias, time delays and
spatial distortions, and spatio-temporal regularization into the model to
address the noisy count issues. We present an efficient optimization algorithm
and discuss its theoretical properties. We show that our model is more accurate
than commonly-used baselines. Finally, we present a case study on wildlife
roadkill monitoring, where our model produces qualitatively convincing results.",Robust Spatio-Temporal Signal Recovery from Noisy Counts in Social Media,2012
"Andino Maseleno, Md. Mahmud Hasan",AI,2012,"Based on Cumulative Number of Confirmed Human Cases of Avian Influenza (H5N1)
Reported to World Health Organization (WHO) in the 2011 from 15 countries,
Indonesia has the largest number death because Avian Influenza which 146
deaths. In this research, the researcher built a Web Mapping and
Dempster-Shafer theory as early warning system of avian influenza. Early
warning is the provision of timely and effective information, through
identified institutions, that allows individuals exposed to a hazard to take
action to avoid or reduce their risk and prepare for effective response. In
this paper as example we use five symptoms as major symptoms which include
depression, combs, wattle, bluish face region, swollen face region, narrowness
of eyes, and balance disorders. Research location is in the Lampung Province,
South Sumatera. The researcher reason to choose Lampung Province in South
Sumatera on the basis that has a high poultry population. Geographically,
Lampung province is located at 103040' to 105050' East Longitude and 6045' -
3045' South latitude, confined with: South Sumatera and Bengkulu on North Side,
Sunda Strait on the Side, Java Sea on the East Side, Indonesia Ocean on the
West Side. Our approach uses Dempster Shafer theory to combine beliefs in
certain hypotheses under conditions of uncertainty and ignorance, and allows
quantitative measurement of the belief and plausibility in our identification
result. Web Mapping is also used for displaying maps on a screen to visualize
the result of the identification process. The result reveal that avian
influenza warning system has successfully identified the existence of avian
influenza and the maps can be displayed as the visualization.",Avian Influenza (H5N1) Warning System using Dempster-Shafer Theory and Web Mapping,2012
"Andino Maseleno, Md. Mahmud Hasan",AI,2012,"Based on Cumulative Number of Confirmed Human Cases of Avian Influenza (H5N1)
Reported to World Health Organization (WHO) in the 2011 from 15 countries,
Indonesia has the largest number death because Avian Influenza which 146
deaths. In this research, the researcher built an Avian Influenza (H5N1) Expert
System for identifying avian influenza disease and displaying the result of
identification process. In this paper, we describe five symptoms as major
symptoms which include depression, combs, wattle, bluish face region, swollen
face region, narrowness of eyes, and balance disorders. We use chicken as
research object. Research location is in the Lampung Province, South Sumatera.
The researcher reason to choose Lampung Province in South Sumatera on the basis
that has a high poultry population. Dempster-Shafer theory to quantify the
degree of belief as inference engine in expert system, our approach uses
Dempster-Shafer theory to combine beliefs under conditions of uncertainty and
ignorance, and allows quantitative measurement of the belief and plausibility
in our identification result. The result reveal that Avian Influenza (H5N1)
Expert System has successfully identified the existence of avian influenza and
displaying the result of identification process.",Avian Influenza (H5N1) Expert System using Dempster-Shafer Theory,2012
